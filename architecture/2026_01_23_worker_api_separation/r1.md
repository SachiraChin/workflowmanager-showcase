# Worker API Separation

## Summary

Decouple server and worker by having worker expose an HTTP API for task operations. Server will call this API instead of importing TaskQueue directly. Worker will own its own database (task queue collection).

## Current State

### Problem 1: Server imports from Worker
```python
# backend/server/api/routes/tasks.py
from backend.worker.queue import TaskQueue  # TODO comment exists

# backend/server/api/routes/streaming.py
from backend.worker.queue import TaskQueue  # TODO comment exists
```

This creates wrong-direction dependency: server → worker.

### Problem 2: Shared Database Access
Both server and worker access the same MongoDB database directly:
- Server uses `Database` class for workflows, events, content, etc.
- Worker uses `Database` class for content repository (storing generated media)
- Worker uses `TaskQueue` which accesses `tasks` collection directly

### Current TaskQueue Location
```
backend/worker/queue.py - TaskQueue class
```

### Current TaskQueue Methods
```python
class TaskQueue:
    def __init__(self, connection_string=None, database_name=None)
    def enqueue(self, actor: str, payload: dict, priority: int = 0) -> str
    def claim_task(self, worker_id: str, actor: str) -> Optional[dict]
    def complete_task(self, task_id: str, result: dict) -> bool
    def fail_task(self, task_id: str, error: dict) -> bool
    def update_heartbeat(self, task_id: str, progress: dict) -> bool
    def get_task(self, task_id: str) -> Optional[dict]
    def get_tasks_for_workflow(self, workflow_run_id: str, limit: int = 100) -> list
    def get_queued_tasks_by_concurrency(self, provider_id: str, limit: int) -> list
    def requeue_stale_tasks(self, stale_threshold_seconds: int = 30) -> int
    def peek_next_task(self, actor: str = None) -> Optional[dict]
```

### Server's Usage of TaskQueue
1. **tasks.py** - Task creation and status:
   - `POST /api/task` → `queue.enqueue()`
   - `GET /api/task/{task_id}` → `queue.get_task()`
   - `GET /api/task/{task_id}/stream` → `queue.get_task()` (polling)
   - `GET /api/task/workflow/{workflow_run_id}` → `queue.get_tasks_for_workflow()`

2. **streaming.py** - Sub-action task creation:
   - `POST /workflow/{id}/sub-action` → `queue.enqueue()`

## Target State

### Architecture
```
┌─────────────────┐         HTTP API          ┌─────────────────┐
│     Server      │ ──────────────────────► │     Worker      │
│                 │                           │                 │
│ - Workflows     │  POST /task               │ - TaskQueue     │
│ - Events        │  GET /task/{id}           │ - Task DB       │
│ - Content meta  │  GET /task/{id}/stream    │ - Media actors  │
│                 │  GET /tasks/workflow/{id} │                 │
└────────┬────────┘                           └────────┬────────┘
         │                                             │
         │ MongoDB                                     │ MongoDB
         ▼                                             ▼
┌─────────────────┐                           ┌─────────────────┐
│  workflow_db    │                           │  worker_db      │
│                 │                           │  (or same db,   │
│ - workflows     │                           │   tasks coll)   │
│ - events        │                           │                 │
│ - content_meta  │                           │ - tasks         │
│ - users         │                           │                 │
└─────────────────┘                           └─────────────────┘
```

### Worker API Endpoints
Worker will run a FastAPI server (separate port, e.g., 9001) with these endpoints:

```
POST   /task                    - Create task (enqueue)
GET    /task/{task_id}          - Get task status
GET    /task/{task_id}/stream   - SSE stream for task progress
GET    /tasks/workflow/{id}     - Get tasks for workflow
DELETE /task/{task_id}          - Cancel task (future)
```

### Request/Response Models

```python
# Create task
POST /task
{
    "actor": "media",
    "payload": { ... },
    "priority": 0
}
Response: { "task_id": "task_xxx" }

# Get task
GET /task/{task_id}
Response: {
    "task_id": "task_xxx",
    "actor": "media",
    "status": "pending|running|completed|failed",
    "payload": { ... },
    "result": { ... },
    "error": { ... },
    "progress": { "message": "...", "elapsed_ms": 123 },
    "created_at": "...",
    "started_at": "...",
    "completed_at": "..."
}

# Stream task (SSE)
GET /task/{task_id}/stream
Events: progress, complete, error

# Get workflow tasks
GET /tasks/workflow/{workflow_run_id}?limit=100
Response: { "tasks": [...] }
```

## Implementation Plan

### Phase 1: Add API to Worker

1. **Create worker API module** (`backend/worker/api/`)
   ```
   backend/worker/api/
   ├── __init__.py
   ├── app.py          # FastAPI app
   ├── routes/
   │   └── tasks.py    # Task endpoints
   └── models.py       # Request/Response models
   ```

2. **Update worker entry point** (`backend/worker/__main__.py`)
   - Start both: task processing loop AND API server
   - Use asyncio to run both concurrently
   - Or run API in separate thread

3. **Add worker port configuration**
   - Environment variable: `WORKER_API_PORT` (default: 9001)
   - Update `start_worker.sh` with `--api-port` option

### Phase 2: Update Server to Use Worker API

1. **Create worker client** (`backend/server/worker_client.py`)
   ```python
   class WorkerClient:
       def __init__(self, base_url: str = "http://localhost:9001"):
           self.base_url = base_url

       async def create_task(self, actor: str, payload: dict, priority: int = 0) -> str:
           # POST /task

       async def get_task(self, task_id: str) -> dict:
           # GET /task/{task_id}

       async def get_tasks_for_workflow(self, workflow_run_id: str) -> list:
           # GET /tasks/workflow/{workflow_run_id}

       def stream_task(self, task_id: str) -> AsyncGenerator:
           # GET /task/{task_id}/stream (SSE)
   ```

2. **Update server routes**
   - `backend/server/api/routes/tasks.py` → Use `WorkerClient` instead of `TaskQueue`
   - `backend/server/api/routes/streaming.py` → Use `WorkerClient` for sub-actions

3. **Remove TaskQueue import from server**
   - Delete the TODO comments
   - Server no longer imports from worker

### Phase 3: Move Content Repository to Worker (Optional)

Currently worker uses `Database` from server to store generated content metadata. Options:

**Option A: Keep content in server DB**
- Worker calls server API to store content metadata
- Adds complexity but cleaner separation

**Option B: Worker stores content, server queries**
- Worker owns content_generations collection
- Server queries worker API for content
- More work but true separation

**Option C: Shared DB, separate concerns (Recommended for now)**
- Both access same MongoDB
- Server owns: workflows, events, users, versions
- Worker owns: tasks collection
- Content metadata stays in server (workflow context)

### Phase 4: Configuration & Deployment

1. **Environment variables**
   ```bash
   # Worker
   WORKER_API_PORT=9001
   WORKER_API_HOST=0.0.0.0

   # Server
   WORKER_API_URL=http://localhost:9001
   ```

2. **Update start scripts**
   ```bash
   # start_worker.sh
   --api-port 9001

   # start_server.sh
   --worker-url http://localhost:9001
   ```

3. **Health checks**
   - Worker API: `GET /health`
   - Server checks worker health on startup

## Files to Modify

### New Files
- `backend/worker/api/__init__.py`
- `backend/worker/api/app.py`
- `backend/worker/api/routes/__init__.py`
- `backend/worker/api/routes/tasks.py`
- `backend/worker/api/models.py`
- `backend/server/worker_client.py`

### Modified Files
- `backend/worker/__main__.py` - Start API server
- `backend/worker/__init__.py` - Export API app
- `backend/server/api/routes/tasks.py` - Use WorkerClient
- `backend/server/api/routes/streaming.py` - Use WorkerClient
- `backend/server/api/app.py` - Initialize WorkerClient
- `backend/server/api/dependencies.py` - Add get_worker_client
- `start_worker.sh` - Add API port option
- `start_server.sh` - Add worker URL option

### Removed
- Server's direct import of `TaskQueue`

## Migration Strategy

1. **Backward compatible first**: Worker API runs alongside existing direct access
2. **Feature flag**: `USE_WORKER_API=true` to switch server to use API
3. **Test thoroughly**: Both paths work
4. **Remove direct access**: Once API is stable

## Testing Checklist

- [ ] Worker API starts on configured port
- [ ] `POST /task` creates task and returns task_id
- [ ] `GET /task/{id}` returns task status
- [ ] `GET /task/{id}/stream` streams progress events
- [ ] `GET /tasks/workflow/{id}` returns workflow tasks
- [ ] Server can create tasks via WorkerClient
- [ ] Server can poll task status via WorkerClient
- [ ] Server can stream task progress via WorkerClient
- [ ] SSE streaming works end-to-end (webui → server → worker)
- [ ] Error handling: worker down, task not found, etc.

## Questions for Review

1. Should worker API require authentication? (Probably not for internal service)
2. Should we use gRPC instead of HTTP? (HTTP simpler, SSE already works)
3. Separate database for worker or same DB different collections?
4. Should content metadata move to worker or stay in server?
