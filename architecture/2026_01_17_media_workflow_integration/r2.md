# Media Generation Workflow Integration Architecture - R2

## Summary

Design for integrating media generation (images/videos) into workflows. This document analyzes the current webui/server architecture and proposes how to enable in-place, iterative media generation within interactions.

---

## Current Architecture Analysis

### Server: Interaction Flow

```
Module.get_interaction_request()
    → Store INTERACTION_REQUESTED event
    → Return InteractionRequest to client

[User responds]

POST /workflow/respond
    → Store INTERACTION_RESPONSE event
    → Module.execute_with_response()
    → Continue to next module
```

**Key constraint:** Every response triggers `execute_with_response()` and advances the workflow. There's no mechanism for "do something without advancing."

**Existing "partial" mechanisms:**
- `retry_requested` → Re-executes module with feedback (branches execution)
- `jump_back_requested` → Jumps to earlier step (branches execution)

Neither allows in-place updates to the current interaction.

### WebUI: Interaction Rendering

```
InteractionHost (routes by interaction_type)
    → TextInputEnhanced | StructuredSelect | ReviewGrouped | etc.
    → InteractionFooter (Continue/Retry buttons)

[User clicks button]
    → getResponse() builds InteractionResponseData
    → POST /workflow/respond
    → Wait for next interaction or completion
```

**Key constraint:** Once an interaction displays, the only action is to respond (which advances workflow). Components don't have a mechanism to trigger server actions and update their own content.

**Progress events:** Only update sidebar (`elapsedMs`, `lastMessage`), not interaction content.

### What's Missing

| Need | Current State |
|------|---------------|
| Button triggers server action | Buttons only submit response (advances workflow) |
| Server returns data without advancing | Every response advances workflow |
| UI updates in-place | No mechanism for partial content updates |
| Accumulating results | Results only appear after workflow advances |

---

## The Use Case Revisited

### What User Wants

**Image Generation Module:**
1. Display prompts from previous step
2. User clicks "Generate Images" → 4 images appear below prompt
3. User adjusts params, clicks again → 4 more images added to grid
4. User selects preferred images
5. User clicks "Continue" → workflow advances with selections

**This requires:**
- Server action that doesn't advance workflow (generate images)
- Data returned to client (image URLs)
- Client updates display in-place (adds to grid)
- Final response includes selections (advances workflow)

### Proposed Mental Model

```
Interaction with Sub-Actions:

┌─────────────────────────────────────────────────────────┐
│ InteractionRequest                                       │
│ ├── interaction_type: "media_generation"                │
│ ├── display_data: { prompts, generated_images: [] }     │
│ ├── sub_actions: [                                      │
│ │     { id: "generate", handler: "media.txt2img", ... } │
│ │   ]                                                   │
│ └── response_schema: { selected_images: [...] }         │
└─────────────────────────────────────────────────────────┘

User clicks "Generate":
    → POST /workflow/{id}/sub-action
    → Server generates images (calls MidAPI)
    → Returns { action_id, result: { urls: [...] } }
    → Client adds images to display_data.generated_images
    → NO workflow state change

User clicks "Continue":
    → POST /workflow/{id}/respond
    → response: { selected_images: [...] }
    → Workflow advances normally
```

---

## Implementation Options

### Option A: New Sub-Action Endpoint

**Server changes:**

1. **New endpoint:** `POST /workflow/{id}/sub-action`
   ```python
   class SubActionRequest(BaseModel):
       workflow_run_id: str
       interaction_id: str
       action_id: str
       params: Dict[str, Any]
   ```

2. **Handler registry:** Map action handlers
   ```python
   SUB_ACTION_HANDLERS = {
       "media.txt2img": handle_media_txt2img,
       "media.img2vid": handle_media_img2vid,
       # ...
   }
   ```

3. **Handler implementation:**
   ```python
   async def handle_media_txt2img(
       workflow_run_id: str,
       params: dict,
       context: WorkflowExecutionContext
   ) -> dict:
       client = MidAPIClient()
       urls, raw = client.txt2img(params)

       # Store in DB (associated with workflow run)
       metadata_id = context.db.content_repo.store_generation(...)

       return {
           "urls": urls,
           "metadata_id": metadata_id
       }
   ```

4. **Response:** Returns result directly, no workflow state change
   ```json
   {
       "action_id": "generate",
       "result": {
           "urls": ["...", "...", "...", "..."],
           "metadata_id": "cgm_xxx"
       }
   }
   ```

<!--above feels like lots of hardcoded logic, can we do something like below, 

workflow module config, workflow know this at that point of time, module is working on what media call:

    {
        "module.id": "media.generation",
        "inputs": {
            "title": "Generate images",
            "type": "txt2img",
            "data": "{{ state.generated_prompts }}",
            "schema": "{{ state.dynamic_image_display_schema }}"
        }
    }
-->

**WebUI changes:**

1. **New interaction type:** `media_generation`
   ```typescript
   // InteractionHost routes to:
   case "media_generation":
       return <MediaGenerationInteraction request={request} mode={mode} />
   ```

2. **Component structure:**
   ```
   MediaGenerationInteraction
   ├── PromptDisplay (from display_data.prompts)
   ├── GenerationControls (provider select, params)
   ├── GenerateButton
   │   └── onClick: callSubAction("generate", params)
   ├── ImageGrid (accumulated from sub-action results)
   │   └── SelectableImage (click to select)
   └── [Continue button via InteractionFooter]
   ```

3. **Sub-action call:**
   ```typescript
   async function callSubAction(actionId: string, params: object) {
       <!--i feel like following should be a stream-->
       const response = await fetch(`/workflow/${workflowRunId}/sub-action`, {
           method: 'POST',
           body: JSON.stringify({
               workflow_run_id: workflowRunId,
               interaction_id: interactionId,
               <!--media.{module.input.type}-->
               action_id: actionId,
               <!--how does component know how to generate this list. I think this is where input
                    where schema (display schema) from module comes in. we can already create forms
                    based on module data, if we can leverage that, module will simply create form
                    based on that. we probably need adjustments made to render form in specific 
                    format, but most should be covered there. looking at current form. it always 
                    renders a table, we need to make it render other structures.-->
               params
           })
       });
       const result = await response.json();

        <!--all logic so far for this proposal make sense until below. this simply assume that
            result always will be list of images/video, but this method imply that this is 
            generic kind of action, which can handle different kind of sub-actions, which 
            it deviate here, or did you mean to add this as part of MediaGenerationInteraction
            component?-->
       // Update local state (add to grid)
       setGeneratedImages(prev => [...prev, ...result.result.urls]);
   }
   ```

4. **Response building:**
   ```typescript
   getResponse() {
       return {
           selected_images: selectedImages,  // From local selection state
           all_generated: generatedImages    // All accumulated URLs
       };
   }
   ```

**Pros:**
- Clean separation: sub-actions vs responses
- Reusable pattern for other interactive features
- Server controls what actions are available

**Cons:**
- New endpoint + handler system
- Need to define sub-action state storage
- More complex than current interaction model

---

### Option B: Module Replaces Display

Instead of modifying interactions, create a new module type that:
1. Receives prompts as input
2. Shows prompts + media generation UI
3. Handles all generation internally
4. Only responds when user is done

**Server: New module type**
```python
class MediaGenerationModule(InteractiveModule):
    """
    Interactive module that handles media generation internally.
    Uses multiple interaction cycles for generate/select flow.
    """

    def get_interaction_request(self, inputs, context):
        # First interaction: show prompts + generate button
        prompts = inputs["prompts"]
        return InteractionRequest(
            interaction_type=InteractionType.MEDIA_GENERATION,
            display_data={
                "prompts": prompts,
                "generated_images": [],
                "mode": "generate"  # or "select"
            }
        )

    def execute_with_response(self, inputs, context, response):
        if response.get("action") == "generate":
            # Generate images
            urls, raw = self._generate(response["params"])
            # Store to DB
            self._store_generation(context, urls, raw)
            # Return ANOTHER interaction (same module, updated state)
            return {
                "_next_interaction": InteractionRequest(
                    display_data={
                        "prompts": inputs["prompts"],
                        "generated_images": self._get_all_generated(context),
                        "mode": "generate"
                    }
                )
            }

        elif response.get("action") == "continue":
            # User is done, return selections
            return {
                "selected_images": response["selected_images"]
            }
```

**Key insight:** The module can return `_next_interaction` to stay in the same step but update the display.

**WebUI: Handle `_next_interaction`**

In `useWorkflowExecution.ts`, when response contains `_next_interaction`:
```typescript
// Instead of moving to next module
if (result._next_interaction) {
    setCurrentInteraction(result._next_interaction);
    // Don't advance workflow position
} else {
    // Normal flow: advance workflow
}
```

**Pros:**
- Uses existing module pattern (somewhat)
- Server controls flow
- Generated images persist in DB

**Cons:**
- `_next_interaction` is a new concept
- Module becomes complex (multiple interaction modes)
- Still requires webui changes for the new pattern

---

### Option C: Client-Side Generation (Minimal Server Changes)

Client handles generation directly, server only validates/stores on final submit.

**Server:**
- No sub-action endpoint
- Module provides config for what client can do
- Final response includes all generated content for server to store

**WebUI:**
- MediaGenerationInteraction calls media APIs directly
- Uses API keys from... where? (security concern)
- Submits final selections + all generated URLs to server

**Pros:**
- Minimal server changes
- Client has full control

**Cons:**
- API keys exposed to client (security issue)
- Generated content not tracked until final submit
- Can't enforce server-side rate limits or quotas
- Refresh loses all progress

---

### Option D: Streaming Sub-Actions

Sub-actions delivered via SSE within the interaction stream.

**Server:**
- Same endpoint for respond, but with `sub_action` flag
- If `sub_action=true`, don't advance workflow, return result via SSE
- New SSE event type: `sub_action_result`

**WebUI:**
- Keep SSE connection open during interaction
- Send sub-actions via POST
- Receive results via SSE stream

**Pros:**
- Leverages existing streaming infrastructure
- Real-time progress for long generations

**Cons:**
- SSE connection must stay open (currently disconnects on interaction)
- More complex state management
- Mixing concerns in stream

---

## Recommendation

**Option A (Sub-Action Endpoint)** is the cleanest approach:

1. **Clear separation:** Sub-actions are distinct from responses
2. **Reusable:** Pattern works for future interactive features
3. **Server control:** Server defines available actions, validates params
4. **State management:** Sub-action results stored immediately
5. **Refresh resilience:** Can reload generated content from DB

**Implementation order:**

1. **Server:**
   - Add `POST /workflow/{id}/sub-action` endpoint
   - Add `sub_actions` field to `InteractionRequest`
   - Implement media generation handlers
   - Add content repository for storage

2. **WebUI:**
   - Add `MediaGenerationInteraction` component
   - Add `callSubAction()` utility
   - Handle sub-action results in component state
   - Build image grid with selection

3. **Module:**
   - Create `MediaGenerationModule` that returns interaction with sub-actions
   - Define what params are adjustable
   - Handle final response with selections

---

## Storage Considerations

With sub-actions, generated content is stored immediately:

```
content_generation_metadata
├── content_generation_metadata_id: str
├── workflow_run_id: str
├── interaction_id: str        ← NEW: Associate with interaction
├── provider: enum
├── operation: enum
├── request_params: jsonb
├── response_data: jsonb
├── ...

generated_content
├── generated_content_id: str
├── workflow_run_id: str
├── content_generation_metadata_id: str (FK)
├── prompt_id: str (nullable)  ← NEW: Which prompt generated this
├── ...
```

On refresh, client can query:
```
GET /workflow/{id}/sub-action-state?interaction_id=xxx
→ Returns all generated content for this interaction
```

---

## Questions for Review

1. **Option A vs B:** Is the sub-action endpoint approach preferred, or should we explore the `_next_interaction` pattern?
<!--A is better just because it follows current architecture, its just a different kind of module. _next_interaction can cause
more issues that solutions give we are replacing same module always.-->

2. **Streaming for generation:** Media generation takes 30s-2min. Should sub-actions support SSE for progress, or is polling acceptable?
<!--we need make media generation SSE, whole reason we lean towards SSE just because of the fact that we knew we need
media generation in the future and we need SSE for that.-->

3. **Prompt association:** How do we track which prompt generated which images? Explicit `prompt_id` in request, or derived from context?
<!--this is a good point. We need to store raw data we got for given request, we cant rely on any data provided by provider about 
our prompt. so we need to keep track of each and every data sent to provider. i think we can store this in content_generation_metadata 
table we discused in 2026_01_17_media_generation_providers-->

4. **Selection carry-forward:** When user selects images and continues, how should that data flow to the next step?
<!--we can pass on generated_content.generated_content_id to state, and next module can use that to get image info-->
