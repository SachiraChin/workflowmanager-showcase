# Provider Data Resolution

## Summary

Step 3 of the CC workflow currently uses an `api.fetch` module to retrieve
Stable Diffusion model categories from a hardcoded internal API endpoint:

```json
{
  "module_id": "api.fetch",
  "inputs": {
    "url": "/wm-api/models/by_category",
    "base_url": "http://192.168.1.181:7860",
    ...
  }
}
```

This approach has several problems:

1. **Security Risk**: Workflow JSON can be modified to call arbitrary endpoints
2. **Infrastructure Coupling**: Workflow definitions contain hardcoded internal
   URLs
3. **Inconsistency**: Other providers (Leonardo, Midjourney, OpenAI) have their
   model data embedded in the display schema, while SD fetches dynamically
4. **No Abstraction**: The workflow knows too much about backend implementation

This document proposes a universal mechanism for providers to expose their
available models (and other metadata) through a standardized interface that
display schemas can reference without knowing implementation details.

## Current State Analysis

### How Model Data is Currently Used

**Stable Diffusion** (dynamic fetch):
```json
// step.json - Module 2
{
  "module_id": "api.fetch",
  "inputs": { "url": "/wm-api/models/by_category", ... },
  "outputs_to_state": { "response": "sd_model_categories" }
}

// display_schema.json
"model_category": {
  "enum": "{{ state.sd_model_categories }}",
  ...
}
```

**Leonardo** (static in schema):
```json
// display_schema.json - model options hardcoded
"model_id": {
  "enum": [
    { "key": "de7d3faf-...", "label": "Leonardo Phoenix 1.0", "styles": [...] },
    { "key": "6b645e3a-...", "label": "Leonardo Phoenix 0.9", "styles": [...] },
    ...
  ]
}
```

**OpenAI** (static in schema):
```json
"model": {
  "enum": ["gpt-image-1.5", "gpt-image-1", "gpt-image-1-mini"],
  ...
}
```

**Midjourney** (static in schema):
```json
"version": {
  "enum": ["7", "6.1", "6", "5.2", "niji6"],
  ...
}
```

### Problems with Current Approach

| Provider | Data Source | Problems |
|----------|-------------|----------|
| SD | `api.fetch` to internal URL | Security risk, hardcoded endpoint |
| Leonardo | Hardcoded in schema | Stale data, large schema size |
| OpenAI | Hardcoded in schema | Must update schema for new models |
| Midjourney | Hardcoded in schema | Same as above |

## Proposed Solution

### Core Concept

Introduce a **Provider Data Directive** (`$provider`) that display schemas can
use to reference provider-supplied data. The server resolves these directives
during schema processing, calling the appropriate provider methods.

### Provider Interface

Each provider implements a standardized interface:

```python
# backend/providers/base.py
class BaseProvider(ABC):
    @abstractmethod
    def get_id(self) -> str:
        """Provider identifier (e.g., 'stable_diffusion', 'leonardo')"""
        pass

    @abstractmethod
    def models(self) -> list[dict]:
        """
        Return available models for this provider.

        Returns:
            List of model definitions with at minimum:
            - key: Unique identifier
            - label: Human-readable name

            May also include provider-specific fields like:
            - styles, presets, capabilities, etc.
        """
        pass

    def styles(self, model_id: str = None) -> list[dict]:
        """Optional: Return available styles for a model."""
        return []

    def presets(self, model_id: str = None) -> list[dict]:
        """Optional: Return available presets for a model."""
        return []
```
<!--

I read your proposals below, I dont feel them works simply because of the fact
that they require new kind of directive and then resolving data around it into
tooling. I dont want to do this unless its absolutely necessary, simply for the
complixity it can add to already very complex workflow logic. 

I feel like we can take advantage of backend/server/modules/media/generate.py
which is purely there for media generation, and it is safe to assume that it
can provide models in interaction request. but this begs the question, how does
this module know what providers are used in previous llm request, because thats
where prompts are generated, and in a way start point of this flow. given
current limitations we have, only safe bet is to add "providers" input to the
generate module, and user will define what providers to use. the next input we
need for this module is action type which can be txt2img, img2vid and
txt2audio, this is also fine because when user define the module, they know
what they are going to do with the module.

now with generate module having these 2 params, module can fetch all model info
from the interface you mentioned above. i think we have few paths from here for
the place we are going to store this data, i think the best way is to store it
in data. we will need slight changes here for
backend/server/modules/media/generate.py 

1. #129 will resolve data = {prompts: [prompts]}
2. resolve models for all input providers, and add it inside data["provider_metadata"]
3. remove genrations from #143 and set data["generations"] = []. this is for 
    future

now when module loads in client side, we have everything inside "data" node
which is actually used to render things in ui. from here, we will render each
ui input with data we have. 

-->

### Schema Directive Syntax

```json
{
  "enum": {
    "$provider": "stable_diffusion",
    "method": "models",
    "params": {},
    "path": "categories"
  }
}
```

**Fields:**
- `$provider`: Provider identifier (required)
- `method`: Provider method to call (required) - `models`, `styles`, `presets`
- `params`: Parameters to pass to the method (optional)
- `path`: JSONPath to extract from result (optional)

### Resolution Flow

```
┌─────────────────────────────────────────────────────────────────────┐
│                        Schema Processing                             │
├─────────────────────────────────────────────────────────────────────┤
│                                                                      │
│  1. Server loads display_schema.json                                 │
│                        │                                             │
│                        ▼                                             │
│  2. Schema resolver walks the schema tree                            │
│                        │                                             │
│                        ▼                                             │
│  3. Encounters { "$provider": "stable_diffusion", ... }              │
│                        │                                             │
│                        ▼                                             │
│  4. Looks up provider in ProviderRegistry                            │
│                        │                                             │
│                        ▼                                             │
│  5. Calls provider.models() (or specified method)                    │
│                        │                                             │
│                        ▼                                             │
│  6. Applies optional path extraction                                 │
│                        │                                             │
│                        ▼                                             │
│  7. Replaces directive with resolved data                            │
│                        │                                             │
│                        ▼                                             │
│  8. Returns fully resolved schema to client                          │
│                                                                      │
└─────────────────────────────────────────────────────────────────────┘
```

### Example: Stable Diffusion Migration

**Before (step.json):**
```json
{
  "module_id": "api.fetch",
  "inputs": {
    "url": "/wm-api/models/by_category",
    "base_url": "http://192.168.1.181:7860",
    ...
  },
  "outputs_to_state": { "response": "sd_model_categories" }
}
```

**After (step.json):**
```json
// Module removed entirely - no fetch needed
```

**Before (display_schema.json):**
```json
"model_category": {
  "enum": "{{ state.sd_model_categories }}",
  ...
}
```

**After (display_schema.json):**
```json
"model_category": {
  "enum": {
    "$provider": "stable_diffusion",
    "method": "models"
  },
  "value_key": "id",
  "label_key": "name",
  ...
}
```

### Example: Leonardo Migration

**Before:**
```json
"model_id": {
  "enum": [
    { "key": "de7d3faf-...", "label": "Leonardo Phoenix 1.0", "styles": [...] },
    { "key": "6b645e3a-...", "label": "Leonardo Phoenix 0.9", "styles": [...] },
    // ... 400+ lines of hardcoded model data
  ]
}
```

**After:**
```json
"model_id": {
  "enum": {
    "$provider": "leonardo",
    "method": "models"
  },
  "value_key": "key",
  "label_key": "label"
}
```

The Leonardo provider's `models()` method returns the same structure, but it's
now centralized in the provider code rather than duplicated in every workflow.

## Technical Specification

### Provider Registry

```python
# backend/providers/registry.py
class ProviderRegistry:
    _providers: dict[str, BaseProvider] = {}

    @classmethod
    def register(cls, provider: BaseProvider) -> None:
        cls._providers[provider.get_id()] = provider

    @classmethod
    def get(cls, provider_id: str) -> BaseProvider | None:
        return cls._providers.get(provider_id)

    @classmethod
    def resolve_directive(
        cls,
        directive: dict
    ) -> Any:
        """
        Resolve a $provider directive.

        Args:
            directive: Dict with $provider, method, params, path

        Returns:
            Resolved data from the provider
        """
        provider_id = directive.get("$provider")
        method_name = directive.get("method", "models")
        params = directive.get("params", {})
        path = directive.get("path")

        provider = cls.get(provider_id)
        if not provider:
            raise ValueError(f"Unknown provider: {provider_id}")

        method = getattr(provider, method_name, None)
        if not method or not callable(method):
            raise ValueError(
                f"Provider {provider_id} has no method: {method_name}"
            )

        result = method(**params)

        if path:
            result = extract_path(result, path)

        return result
```

### Schema Resolver Integration

The existing schema resolver needs to handle `$provider` directives:

```python
# In schema resolution code
def resolve_schema_value(value: Any, context: dict) -> Any:
    if isinstance(value, dict):
        if "$provider" in value:
            return ProviderRegistry.resolve_directive(value)
        elif "$ref" in value:
            return resolve_ref(value, context)
        # ... other directive handling
    elif isinstance(value, str) and "{{" in value:
        return render_jinja(value, context)
    return value
```

### Provider Implementation Example

```python
# backend/providers/stable_diffusion/provider.py
class StableDiffusionProvider(BaseProvider):
    def __init__(self, api_base_url: str):
        self._api_base = api_base_url

    def get_id(self) -> str:
        return "stable_diffusion"

    def models(self) -> list[dict]:
        """Fetch models from the SD API."""
        response = httpx.get(f"{self._api_base}/wm-api/models/by_category")
        response.raise_for_status()
        return response.json()["categories"]
```

```python
# backend/providers/leonardo/provider.py
class LeonardoProvider(BaseProvider):
    def get_id(self) -> str:
        return "leonardo"

    def models(self) -> list[dict]:
        """Return Leonardo model definitions."""
        return [
            {
                "key": "de7d3faf-762f-48e0-b3b7-9d0ac3a3fcf3",
                "label": "Leonardo Phoenix 1.0",
                "styles": [
                    {"key": "556c1ee5-...", "label": "None"},
                    {"key": "a5632c7c-...", "label": "Cinematic"},
                    # ...
                ]
            },
            # ... other models
        ]
```

### Caching Strategy

Provider data can be cached to avoid repeated API calls:

```python
class ProviderRegistry:
    _cache: dict[str, tuple[Any, float]] = {}
    _cache_ttl: float = 300  # 5 minutes default

    @classmethod
    def resolve_directive(cls, directive: dict) -> Any:
        cache_key = cls._make_cache_key(directive)

        if cache_key in cls._cache:
            data, timestamp = cls._cache[cache_key]
            if time.time() - timestamp < cls._cache_ttl:
                return data

        result = cls._fetch_from_provider(directive)
        cls._cache[cache_key] = (result, time.time())
        return result
```

## Design Decisions

### Why `$provider` Directive Over Module-Based Approach?

**Option considered**: Keep using a module like `provider.data` that populates
state, then reference state in schema.

**Why rejected**:
1. Requires explicit module in every workflow that uses provider data
2. Data flows through state unnecessarily
3. Module execution order matters
4. More verbose workflow definitions

**Chosen approach** (`$provider` directive):
1. Schema declares what it needs directly
2. Server resolves at schema processing time
3. No workflow module needed
4. Cleaner separation of concerns

### Why Not Implicit Provider Data Injection?

**Option considered**: Automatically inject provider data when schema declares
`"provider": "stable_diffusion"` in `_ux`.

**Why rejected**:
1. Too implicit/"magic" - harder to understand data flow
2. What if schema references provider but doesn't need models?
3. Less control over what data is loaded

**Chosen approach**: Explicit `$provider` directive makes intent clear.

### Allowed Methods (Security)

To prevent abuse, only whitelisted methods can be called:

```python
ALLOWED_PROVIDER_METHODS = {"models", "styles", "presets", "capabilities"}

def resolve_directive(cls, directive: dict) -> Any:
    method_name = directive.get("method", "models")
    if method_name not in ALLOWED_PROVIDER_METHODS:
        raise ValueError(f"Method not allowed: {method_name}")
    # ...
```

## Migration Plan

### Phase 1: Infrastructure

1. Create `BaseProvider` abstract class
2. Implement `ProviderRegistry`
3. Add `$provider` directive handling to schema resolver

### Phase 2: Stable Diffusion

1. Implement `StableDiffusionProvider.models()`
2. Update CC step 3 display schema to use `$provider`
3. Remove `api.fetch` module from step.json
4. Test thoroughly

### Phase 3: Leonardo

1. Implement `LeonardoProvider.models()`
2. Extract hardcoded model data from display schemas
3. Update display schemas to use `$provider`
4. Reduces schema size significantly

### Phase 4: Other Providers

1. OpenAI models (if dynamic model list needed)
2. Midjourney versions
3. Any future providers

## Files Affected

### New Files
- `backend/providers/base.py` - BaseProvider ABC
- `backend/providers/registry.py` - ProviderRegistry
- `backend/providers/stable_diffusion/provider.py` - SD provider
- `backend/providers/leonardo/provider.py` - Leonardo provider

### Modified Files
- `backend/server/engine/schema_resolver.py` - Add `$provider` handling
- `workflows/cc/steps/3_image_prompts/step.json` - Remove fetch module
- `workflows/cc/steps/3_image_prompts/schemas/cc_image_prompts_display_schema.json`
  - Update SD enum to use `$provider`

## Open Questions

1. **Resolution Timing**: Should provider data be resolved:
   - At schema compilation time (before sending to client)?
   - At interaction setup time (when step starts)?
   - On-demand / lazy (when user opens a specific tab)?

   Current proposal assumes schema compilation time. Is this acceptable?

2. **Caching Granularity**:
   - Per-session cache? (refresh on new workflow run)
   - Time-based TTL? (e.g., 5 minutes)
   - Manual invalidation? (e.g., when user installs new SD models)

3. **Error Handling**: If a provider's `models()` call fails:
   - Return empty list and continue?
   - Fail the entire schema resolution?
   - Show error state in UI?

4. **Scope Expansion**: Should this pattern also handle:
   - Sampler lists?
   - VAE lists?
   - LoRA lists?
   - Or keep those as part of the cascading controls system?

5. **State Access**: Some providers might need workflow state to determine
   available models (e.g., user preferences). Should `$provider` support
   passing state context?

   ```json
   {
     "$provider": "stable_diffusion",
     "method": "models",
     "params": {
       "category": "{{ state.preferred_category }}"
     }
   }
   ```

