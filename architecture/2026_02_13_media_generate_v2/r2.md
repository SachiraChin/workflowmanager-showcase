# media.generateV2

## Summary

Current prompt-media flow is split across two modules:

1. `api.llm` builds provider prompt payloads.
2. `media.generate` renders those prompts and executes generation sub-actions.

This split causes contract drift:

- schema rules are enforced in one module but consumed in another
- display schema depends on prompt schema shape across module boundaries
- users must configure and keep two modules synchronized

`media.generateV2` combines both phases into one interactive module. Prompt
generation, prompt schema, display schema, provider constraints, and media
selection are managed in one place.

## Design Decisions

### 1) Additive module id

Introduce `media.generateV2` as a new module id, without changing existing
`media.generate` behavior.

Reasons:

- backward compatibility for current workflows
- safer rollout and side-by-side validation

### 2) Reuse `api.llm` execution rules directly

V2 must follow the same LLM behavior as `api.llm`.

Decision:

- V2 invokes `LLMCallModule` with constructed inputs and existing execution
  context, instead of re-implementing provider/model/message logic.

This preserves:

- provider/model precedence rules
- `ai_config` behavior
- mock mode behavior
- output-schema structured generation behavior

### 3) Prompt generation is mandatory inside V2

V2 is tightly coupled to internal LLM prompt generation for now.

- no manual/non-LLM prompt source mode in this revision

### 4) Server owns prompt output schema shape

User cannot directly define prompt output schema structure. V2 synthesizes it
from selected `action_type` and providers.

### 5) Provider set is unique and authoritative

Users select providers in V2 input. Constraints:

- only providers valid for chosen `action_type`
- no duplicate provider instances

Prompt schema and display schema are regenerated from this provider set.

### 6) Display schema is user-editable, but user-responsible

V2 provides default display schema on module creation. Users may edit it.

If user edits break runtime rendering/behavior, execution fails. Server does
not silently fall back to generated defaults in this revision.

### 7) Always expose generated prompts output

`generated_prompts` is always available as module output for downstream state
mapping and reuse.

## Technical Specification

### Backend module

Add:

- `backend/workflow_engine/modules/media/generate_v2.py`

Type:

- `InteractiveModule`

Execution flow:

1. Resolve V2 inputs (`action_type`, `providers`, `prompt_config`, etc.).
2. Validate provider set against action type and uniqueness.
3. Build canonical prompt output schema from selected providers.
4. Build `api.llm` inputs from `prompt_config` and canonical schema.
5. Invoke `LLMCallModule` to generate structured prompts.
6. Enrich prompts with `_provider_metadata` and `_generations` using existing
   media-generation enrichment logic.
7. Resolve display schema (default generated + optional user-edited value).
8. Return media-generation interaction request with same payload shape as
   current `media.generate` renderer expectations.
9. On response, process selected content and generations with parity to current
   `media.generate` outputs.

Sub-actions:

- reuse current task queue media execution path
- reuse current mock sub-action placeholder behavior

### V2 input contract

Required:

- `action_type`: `txt2img | img2vid | txt2audio`
- `providers`: array of unique provider ids
- `prompt_config`:
  - `provider`
  - `model` (optional)
  - `system` (standard structure)
  - `user` (shared prompt template)
  - `providers` (optional per-provider prompt templates/overrides)
  - `ai_config` (optional)

Optional:

- `title`
- `source_image` (for `img2vid`)
- `display_schema` (user-edited display schema)
- `sub_actions`
- `retryable`

### V2 output contract

- `selected_content_id`
- `selected_content`
- `generations`
- `generated_prompts`

### Prompt schema synthesis

V2 synthesizes strict schema by action type + selected providers.

Example shape for `txt2img`:

- top-level context fields as configured
- `prompts.<provider>.<required_fields>` for each selected provider
- required providers must match selected provider set

### Display schema synthesis

V2 generates default display schema from selected providers and action type,
following current `tab.media[...]` and `input_schema` patterns.

Editor uses this default on module creation. User edits are stored back in
`inputs.display_schema`.

### Editor changes

Add new node implementation:

- `ui/editor/src/modules/media/generateV2/*`

Editor behavior:

- one module editor for action type, providers, prompt config, display schema
  <!--just to clarify, i assume when you said module, you are talking about one
  in editor grid. when it comes to editing prompts and display schemas, we
  should keep it separate. we already have a good prompt editor, we may need to
  add some extensions to support different "types" of propts for providers, etc
  -->
- provider selection drives both prompt and display schema shape
- prevent duplicate provider selection

## Database Schema

No DB migration required for initial V2 rollout.

V2 reuses existing content metadata and task queue persistence pathways.

## API Contracts

### Workflow module JSON

New module id:

- `module_id: "media.generateV2"`

### Interaction payload shape

V2 keeps current media interaction payload conventions:

- `display_data.data`
- `display_data.schema`
- `display_data.sub_actions`
- `display_data.generations`
- `display_data.retryable`

This allows reuse of current media interaction renderer.

### Mock mode

V2 uses existing execution context mock flag and preserves full mock behavior
across both prompt generation and media sub-actions.

## Cross-Module Impact

### Backend

- new `media.generateV2` module
- reuse `LLMCallModule` for prompt generation stage
- shared helper extraction from `media.generate` for metadata enrichment and
  selection extraction where useful

### Editor

- new V2 module registration and node editor
- keep `media.generate` editor for backward compatibility

### Workflows

- new workflows use V2
- existing workflows continue unchanged

## Rollout Plan

1. Implement backend `media.generateV2` with `api.llm` invocation integration.
2. Implement editor node for V2 with provider uniqueness enforcement.
3. Add one workflow pilot in CC prompt/media step.
4. Validate mock mode, retry/regenerate behavior, and media sub-actions.
5. Keep legacy pair (`api.llm` + `media.generate`) supported.

## Questions for Review

1. For per-provider prompt overrides, should V2 use full template replacement,
   or merge strategy (shared base + per-provider suffix/prefix)?

   <!--since all these happens in one llm call, and all requests or target
   prompts based on what user wants to do in workflow, i'd say lets make it
   required to add llm prompts for each provider. we can add baseline text for
   each provider when user create a new module in the editor, but it still
   require some contexual details and example added by user.-->

2. Should V2 persist generated prompt output in interaction payload for user
   inspection, or only in module outputs/state?

   <!--we need it to interaction to show it in editor right? idea is, when it
   comes to ux side of feeling, final result would be 1:1 match of current
   ux-->
