# media.generateV2

## Summary

Current prompt-media flow is split across two modules:

1. `api.llm` builds provider prompt payloads.
2. `media.generate` renders prompts and executes generation sub-actions.

This split causes contract drift across module boundaries and forces users to
maintain two configurations for one feature.

`media.generateV2` combines prompt generation and media generation into one
editor-grid module with one contract for providers, prompt schema, display
schema, and selection outputs.

## Design Decisions

### 1) Additive module id

Introduce `media.generateV2` without changing existing `media.generate`.

- existing workflows remain stable
- V2 can roll out incrementally

### 2) Reuse `api.llm` logic directly

V2 invokes `LLMCallModule` with constructed inputs/context. V2 does not
re-implement provider/model/message precedence behavior.

### 3) Prompt generation is mandatory in V2

V2 is tightly coupled to internal LLM prompt generation in this revision.
No manual/non-LLM prompt mode.

### 4) Server controls prompt output schema

Users cannot redefine prompt output schema structure. V2 synthesizes strict
schema from `action_type` + selected providers.

### 5) Provider selection rules

- providers must be valid for `action_type`
- providers are unique (no duplicate instances)
- provider set drives prompt and display schema generation

### 6) Display schema ownership

Editor provides default display schema at module creation.
Users may edit it and are responsible for resulting validity.

### 7) Generated prompts exposure

`generated_prompts` is always produced for downstream state mapping and reuse.

### 8) Shared prompt reference semantics

Provider prompts do not text-replace shared prompt content.

Shared instructions are emitted before provider-specific prompts. Provider
prompts reference shared instructions via explicit reference tokens.

## Technical Specification

### Backend module

Add:

- `backend/workflow_engine/modules/media/generate_v2.py`

Type:

- `InteractiveModule`

Execution flow:

1. Resolve inputs (`action_type`, providers, prompt config).
2. Validate provider validity + uniqueness.
3. Build canonical prompt output schema from providers.
4. Build `api.llm` inputs from prompt config + canonical schema.
5. Invoke `LLMCallModule` for structured prompt generation.
6. Enrich prompt payload with `_provider_metadata` and `_generations`.
7. Resolve display schema (generated default + user-edited schema).
8. Build media interaction payload for existing renderer shape.
9. Process response into `selected_content*` and `generations` outputs.

Sub-actions:

- reuse current task queue execution in real mode
- reuse current placeholder behavior in mock mode

### V2 input contract

Required:

- `action_type`: `txt2img | img2vid | txt2audio`
- `providers`: array of unique provider ids
- `prompt_config`:
  - `provider`
  - `model` (optional)
  - `system`
  - `shared_user`
  - `provider_prompts` (optional map, per-provider)
  - `ai_config` (optional)

Optional:

- `title`
- `source_image` (for `img2vid`)
- `display_schema`
- `sub_actions`
- `retryable`

### Prompt reference contract

`shared_user` defines common instructions.

Each `provider_prompts.<provider>` is a provider-specific prompt block.

Reference token:

- `{{shared_prompt_ref}}`

Prompt construction order:

1. shared instructions block
2. server-owned provider header envelope
3. provider-specific body block(s) containing reference token(s)

### Provider header ownership

Provider header is server-owned and deterministic. It is not editable in
provider prompt text. Header includes provider identity and strict output
requirements for schema compliance.

User prompt text is body-only and appended after header.

### Provider prompt fallback behavior

Editor behavior per provider:

- when prompt content is cleared, editor auto-inserts default reference text
  (containing `{{shared_prompt_ref}}`)
- if user removes that default reference text as well, provider prompt block is
  omitted from `provider_prompts`

Runtime behavior:

- missing provider prompt block is allowed
- shared prompt remains authoritative context for that provider
- final provider prompt still includes server header envelope
- when provider body is missing, runtime uses minimal body:
  `Use {{shared_prompt_ref}}.`

### V2 output contract

- `selected_content_id`
- `selected_content`
- `generations`
- `generated_prompts`

### Interaction data contract

`generated_prompts` must be present in interaction data for UI rendering with
display schema and also present in module outputs for state mapping.

## Editor Plan

Add node implementation:

- `ui/editor/src/modules/media/generateV2/*`

One module in grid, two editing surfaces:

- prompt editor surface (shared + provider prompt blocks)
- display schema editor surface

Editor enforces unique provider selection and action-type compatibility.

## Database Schema

No DB migration required for initial V2 rollout.
Existing task queue/content metadata persistence are reused.

## API Contracts

### Workflow module JSON

- `module_id: "media.generateV2"`

### Interaction payload shape

V2 keeps current media interaction payload conventions:

- `display_data.data`
- `display_data.schema`
- `display_data.sub_actions`
- `display_data.generations`
- `display_data.retryable`

## Rollout Plan

1. Implement backend `media.generateV2` with `LLMCallModule` invocation.
2. Implement editor node for V2 with provider uniqueness checks.
3. Extend prompt editor for shared + provider prompt blocks.
4. Pilot one CC step with V2.
5. Validate mock mode, retry loop, sub-actions, and UX parity.

## Questions for Review

1. Should omitted provider prompt blocks be materialized during save/export, or
   preserved as omitted and handled purely at runtime?

   <!--idea would be, if user remove all text for provide prompt, that count as
   no prompt. if user leave ref, we leave ref. on server, only valiudation is
   if provider prompt is empty, if empty, no provider prompt, if exists, server
       adds header and provided provider prompt -->
