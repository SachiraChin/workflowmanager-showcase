# Task Queue Architecture - Revision 2

## Summary

This document describes the architecture for a persistent, database-backed task queue system. The primary motivation is to fix a flaw in media generation SSE handling where closing the SSE stream (e.g., page refresh) causes generation results to be lost because DB write logic executes outside the asyncio thread that continues running.

## Changes from R1

Based on feedback:
- **File structure**: Option D selected - providers extracted to root-level `providers/` package
- **Concurrency model**: Actor pattern introduced - worker has ActorRegistry, actors interface with providers
- **Task retention**: Keep forever (audit trail)
- **Progress**: Keep as-is (elapsed_ms + message)
- **Errors**: Full stack trace + structured error
- **WebUI**: Breaking changes acceptable

---

## High-Level Architecture

```
┌─────────────────────────────────────────────────────────────┐
│                        FastAPI Server                        │
│  ┌─────────────┐  ┌─────────────┐  ┌─────────────────────┐  │
│  │ POST /task  │  │ GET /task   │  │ SSE /task/stream    │  │
│  │ (enqueue)   │  │ (status)    │  │ (poll progress)     │  │
│  └──────┬──────┘  └──────┬──────┘  └──────────┬──────────┘  │
│         │                │                     │             │
│         └────────────────┼─────────────────────┘             │
│                          │                                   │
│                    ┌─────▼─────┐                             │
│                    │  MongoDB  │                             │
│                    │task_queue │                             │
│                    └─────┬─────┘                             │
└──────────────────────────┼───────────────────────────────────┘
                           │
              ┌────────────▼─────────────────────────────────┐
              │            Worker Process                     │
              │  ┌───────────────────────────────────────┐   │
              │  │           Worker Loop                  │   │
              │  │  - Poll for tasks                      │   │
              │  │  - Check concurrency via actor         │   │
              │  │  - Claim & dispatch to actor           │   │
              │  │  - Heartbeat                           │   │
              │  └───────────────────┬───────────────────┘   │
              │                      │                        │
              │  ┌───────────────────▼───────────────────┐   │
              │  │          Actor Registry                │   │
              │  │  - MediaActor                          │   │
              │  │  - (future actors...)                  │   │
              │  └───────────────────┬───────────────────┘   │
              └──────────────────────┼────────────────────────┘
                                     │ imports
              ┌──────────────────────▼────────────────────────┐
              │              providers/                        │
              │  - media/ (ProviderRegistry + providers)      │
              │  - (future provider types...)                 │
              └───────────────────────────────────────────────┘
```

### Dependency Direction

```
worker/
  └── actors/
        └── media.py ────imports────► providers/media/
                                            │
server/                                     │
  └── api/ ──────────imports────────────────┘
```

**Rules**:
- `worker` imports from `providers` (via actors)
- `server` imports from `providers` (for preview info, etc.)
- `providers` has NO knowledge of `worker` or `server`

---

## File Structure (Option D)

```
providers/                      # Shared provider implementations (NEW)
  __init__.py
  media/
    __init__.py
    base.py                     # Provider base class, exceptions
    registry.py                 # ProviderRegistry (with concurrency config)
    download.py                 # Media download utility
    providers/
      __init__.py
      leonardo/
        __init__.py
        provider.py
      midjourney/
        __init__.py
        provider.py

worker/                         # Worker process (NEW)
  __init__.py
  __main__.py                   # Entry: python -m worker
  queue.py                      # TaskQueue class (DB operations)
  loop.py                       # Main worker loop
  actors/
    __init__.py                 # ActorRegistry
    base.py                     # ActorBase ABC
    media.py                    # MediaActor implementation

server/                         # FastAPI server (existing, modified)
  __init__.py
  api/
    routes/
      streaming.py              # Modified: enqueue task instead of direct execution
      tasks.py                  # NEW: task status endpoints
  modules/
    media/
      __init__.py               # Simplified: re-exports from providers.media
      sub_action.py             # REMOVED or simplified to just enqueue
  engine/
  ...
```

### Migration Notes

1. Move `server/modules/media/providers/` → `providers/media/providers/`
2. Move `server/modules/media/base.py` → `providers/media/base.py`
3. Move `server/modules/media/registry.py` → `providers/media/registry.py`
4. Move `server/modules/media/download.py` → `providers/media/download.py`
5. Update all imports in `server/` to use `providers.media`
6. `server/modules/media/` becomes thin re-export layer or removed

---

## Database Schema

### Collection: `task_queue`

```python
{
    # Identity
    "task_id": "tq_01abc...",           # Unique task identifier
    "actor": "media",                    # Routes to appropriate actor

    # Concurrency control
    "concurrency_identifier": null,      # Set by actor (e.g., "midjourney")
    "concurrency_limit": null,           # Set by actor (e.g., 1)

    # Status
    "status": "queued",                  # queued | processing | completed | failed
    "priority": 0,                       # For future priority support

    # Task data
    "payload": {                         # Actor-specific input
        # For media actor:
        "workflow_run_id": "...",
        "interaction_id": "...",
        "provider": "midjourney",
        "action_type": "txt2img",
        "prompt_id": "...",
        "params": { ... },
        "source_data": { ... }
    },
    "result": {                          # Actor-specific output (set on completion)
        # For media actor:
        "metadata_id": "...",
        "content_ids": ["gc_...", ...],
        "urls": ["...", ...]
    },
    "error": {                           # Structured error (if failed)
        "type": "GenerationError",
        "message": "Provider returned error",
        "details": { ... },
        "stack_trace": "Traceback (most recent call last):\n..."
    },

    # Progress tracking
    "progress": {
        "elapsed_ms": 0,
        "message": "Queued",
        "updated_at": "2026-01-20T..."
    },

    # Timestamps
    "created_at": "2026-01-20T...",
    "started_at": null,                  # When worker claimed it
    "completed_at": null,                # When finished (success or fail)

    # Worker tracking
    "worker_id": null,                   # Which worker is processing
    "heartbeat_at": null,                # Last heartbeat from worker

    # Retry handling
    "retry_count": 0,
    "max_retries": 3,
}
```

### Indexes

```python
# For polling queued tasks (with concurrency check)
{"status": 1, "concurrency_identifier": 1, "priority": -1, "created_at": 1}

# For stale task detection
{"status": 1, "heartbeat_at": 1}

# For task lookup
{"task_id": 1}  # unique

# For counting concurrent tasks
{"status": 1, "concurrency_identifier": 1}

# For listing tasks by workflow
{"payload.workflow_run_id": 1, "created_at": -1}
```

---

## Actor System

### Actor Base Class

```python
# worker/actors/base.py

from abc import ABC, abstractmethod
from typing import Dict, Any, Tuple, Callable

ProgressCallback = Callable[[int, str], None]  # (elapsed_ms, message)


class ActorBase(ABC):
    """
    Base class for task actors.

    Actors are the interface between the worker and domain-specific logic.
    They handle concurrency decisions and task execution.
    """

    @property
    @abstractmethod
    def name(self) -> str:
        """
        Actor name for registration and task routing.
        Must match the 'actor' field in task_queue documents.
        """
        pass

    @abstractmethod
    def get_concurrency_info(self, payload: Dict[str, Any]) -> Tuple[str, int]:
        """
        Determine concurrency constraints for a task.

        Called by worker before claiming a task to check if it can run.

        Args:
            payload: Task payload from task_queue document

        Returns:
            Tuple of (concurrency_identifier, max_concurrent)
            - concurrency_identifier: Group tasks by this (e.g., provider name)
            - max_concurrent: Max tasks with this identifier that can run simultaneously

        Example:
            payload = {"provider": "midjourney", ...}
            return ("midjourney", 1)  # Only 1 midjourney task at a time
        """
        pass

    @abstractmethod
    def execute(
        self,
        payload: Dict[str, Any],
        progress_callback: ProgressCallback
    ) -> Dict[str, Any]:
        """
        Execute the task.

        Args:
            payload: Task-specific input data
            progress_callback: Call with (elapsed_ms, message) to update progress

        Returns:
            Task-specific result dict (stored in task.result)

        Raises:
            Exception: On failure (caught by worker, stored in task.error)
        """
        pass
```

### Actor Registry

```python
# worker/actors/__init__.py

from typing import Dict, Optional
from .base import ActorBase


class ActorRegistry:
    """Registry of available actors."""

    _actors: Dict[str, ActorBase] = {}

    @classmethod
    def register(cls, actor: ActorBase) -> None:
        """Register an actor instance."""
        cls._actors[actor.name] = actor

    @classmethod
    def get(cls, name: str) -> Optional[ActorBase]:
        """Get actor by name."""
        return cls._actors.get(name)

    @classmethod
    def names(cls) -> list[str]:
        """List registered actor names."""
        return list(cls._actors.keys())


# Auto-register actors on import
from .media import MediaActor

ActorRegistry.register(MediaActor())
```

### Media Actor

```python
# worker/actors/media.py

from typing import Dict, Any, Tuple
from .base import ActorBase, ProgressCallback

# Import from providers package (NOT from server)
from providers.media import ProviderRegistry
from providers.media.download import download_media, DownloadError

# Need DB access for content storage
from db import Database
from api.dependencies import get_media_images_path, get_media_videos_path, get_server_base_url
from utils import uuid7_str


class MediaActor(ActorBase):
    """Actor for media generation tasks."""

    name = "media"

    def __init__(self):
        # Initialize DB connection for this actor
        # TODO: Get connection string from config
        self.db = Database()

    def get_concurrency_info(self, payload: Dict[str, Any]) -> Tuple[str, int]:
        """Get concurrency based on provider."""
        provider_name = payload.get("provider", "unknown")
        concurrency = ProviderRegistry.get_concurrency(provider_name)
        return (provider_name, concurrency)

    def execute(
        self,
        payload: Dict[str, Any],
        progress_callback: ProgressCallback
    ) -> Dict[str, Any]:
        """Execute media generation."""

        # Extract payload
        workflow_run_id = payload["workflow_run_id"]
        interaction_id = payload["interaction_id"]
        provider_name = payload["provider"]
        action_type = payload["action_type"]
        prompt_id = payload["prompt_id"]
        params = payload["params"]
        source_data = payload["source_data"]

        # Get provider
        provider = ProviderRegistry.get(provider_name)
        method = getattr(provider, action_type)

        # Create metadata record
        metadata_id = self.db.content_repo.store_generation(
            workflow_run_id=workflow_run_id,
            interaction_id=interaction_id,
            provider=provider_name,
            prompt_id=prompt_id,
            operation=action_type,
            request_params=params,
            source_data=source_data,
        )

        try:
            # Prepare arguments
            prompt = params.get("prompt", "")
            method_params = {k: v for k, v in params.items() if k != "prompt"}
            method_params["prompt_id"] = prompt_id

            # Call provider (synchronous)
            if action_type == "txt2img":
                result = method(prompt, method_params, progress_callback=progress_callback)
            elif action_type in ("img2img", "img2vid"):
                source_image = params.get("source_image", "")
                result = method(source_image, prompt, method_params, progress_callback=progress_callback)
            else:
                raise ValueError(f"Unknown action type: {action_type}")

            # Update metadata with completion
            self.db.content_repo.update_generation_status(
                metadata_id=metadata_id,
                status="completed",
                response_data=result.raw_response,
                provider_task_id=result.provider_task_id,
            )

            # Download and store content
            content_ids = []
            server_urls = []
            content_type = "video" if action_type == "img2vid" else "image"

            images_path = get_media_images_path()
            videos_path = get_media_videos_path()
            server_base_url = get_server_base_url() or ""

            for index, provider_url in enumerate(result.urls):
                content_id = f"gc_{uuid7_str()}"

                download_result = download_media(
                    url=provider_url,
                    metadata_id=metadata_id,
                    content_id=content_id,
                    index=index,
                    content_type=content_type,
                    images_path=images_path,
                    videos_path=videos_path,
                )

                self.db.content_repo.store_content_with_download(
                    content_id=content_id,
                    metadata_id=metadata_id,
                    workflow_run_id=workflow_run_id,
                    index=index,
                    provider_url=provider_url,
                    content_type=content_type,
                    extension=download_result.extension,
                    local_path=download_result.local_path,
                )

                content_ids.append(content_id)
                server_url = f"{server_base_url}/workflow/{workflow_run_id}/media/{content_id}.{download_result.extension}"
                server_urls.append(server_url)

            return {
                "metadata_id": metadata_id,
                "content_ids": content_ids,
                "urls": server_urls,
            }

        except Exception as e:
            self.db.content_repo.update_generation_status(
                metadata_id=metadata_id,
                status="failed",
                error_message=str(e),
            )
            raise
```

---

## Provider Registry Updates

```python
# providers/media/registry.py

from typing import Dict, Type, Optional
from .base import MediaProvider


class ProviderConfig:
    """Configuration for a provider."""
    def __init__(
        self,
        provider_class: Type[MediaProvider],
        concurrency: int = 1,
    ):
        self.provider_class = provider_class
        self.concurrency = concurrency
        self._instance: Optional[MediaProvider] = None

    @property
    def instance(self) -> MediaProvider:
        if self._instance is None:
            self._instance = self.provider_class()
        return self._instance


class MediaProviderRegistry:
    """Registry for media providers with configuration."""

    _providers: Dict[str, ProviderConfig] = {}

    @classmethod
    def register(
        cls,
        name: str,
        provider_class: Type[MediaProvider],
        concurrency: int = 1,
    ) -> None:
        """
        Register a provider with its configuration.

        Args:
            name: Provider identifier (e.g., "midjourney", "leonardo")
            provider_class: Provider implementation class
            concurrency: Max concurrent tasks for this provider (default: 1)
        """
        cls._providers[name] = ProviderConfig(
            provider_class=provider_class,
            concurrency=concurrency,
        )

    @classmethod
    def get(cls, name: str) -> MediaProvider:
        """Get provider instance by name."""
        config = cls._providers.get(name)
        if not config:
            raise ValueError(f"Unknown provider: {name}")
        return config.instance

    @classmethod
    def get_concurrency(cls, name: str) -> int:
        """Get max concurrency for a provider."""
        config = cls._providers.get(name)
        if not config:
            return 1  # Default to 1 for unknown providers
        return config.concurrency

    @classmethod
    def list_providers(cls) -> list[str]:
        """List registered provider names."""
        return list(cls._providers.keys())


# Register providers with concurrency
from .providers.leonardo import LeonardoProvider
from .providers.midjourney import MidJourneyProvider

MediaProviderRegistry.register("leonardo", LeonardoProvider, concurrency=3)
MediaProviderRegistry.register("midjourney", MidJourneyProvider, concurrency=1)
```

---

## Worker Process

### Main Entry Point

```python
# worker/__main__.py

import asyncio
import logging
import signal
import sys

from .loop import WorkerLoop

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(name)s: %(message)s"
)
logger = logging.getLogger("worker")


def main():
    logger.info("Starting worker process...")

    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)

    worker = WorkerLoop()

    # Handle shutdown signals
    def shutdown(signum, frame):
        logger.info(f"Received signal {signum}, shutting down...")
        worker.stop()

    signal.signal(signal.SIGINT, shutdown)
    signal.signal(signal.SIGTERM, shutdown)

    try:
        loop.run_until_complete(worker.run())
    except KeyboardInterrupt:
        logger.info("Interrupted, shutting down...")
    finally:
        loop.close()
        logger.info("Worker stopped")


if __name__ == "__main__":
    main()
```

### Worker Loop

```python
# worker/loop.py

import asyncio
import logging
import traceback
from datetime import datetime, timedelta
from typing import Optional

from .queue import TaskQueue, Task
from .actors import ActorRegistry

logger = logging.getLogger("worker.loop")

HEARTBEAT_INTERVAL = 5  # seconds
STALE_THRESHOLD = 30    # seconds
POLL_INTERVAL = 1       # seconds


class WorkerLoop:
    """Main worker loop."""

    def __init__(self):
        self.queue = TaskQueue()
        self.worker_id = self._generate_worker_id()
        self._running = True
        self._current_task: Optional[Task] = None
        self._heartbeat_task: Optional[asyncio.Task] = None

    def _generate_worker_id(self) -> str:
        """Generate unique worker identifier."""
        import socket
        import os
        hostname = socket.gethostname()
        pid = os.getpid()
        return f"{hostname}-{pid}"

    def stop(self):
        """Signal worker to stop."""
        self._running = False
        if self._heartbeat_task:
            self._heartbeat_task.cancel()

    async def run(self):
        """Main worker loop."""
        logger.info(f"Worker {self.worker_id} starting")

        # Recover stale tasks on startup
        await self._recover_stale_tasks()

        while self._running:
            try:
                # Try to get and process a task
                task = await self._try_claim_task()

                if task:
                    await self._process_task(task)
                else:
                    # No task available, wait before polling again
                    await asyncio.sleep(POLL_INTERVAL)

            except Exception as e:
                logger.exception(f"Error in worker loop: {e}")
                await asyncio.sleep(POLL_INTERVAL)

        logger.info(f"Worker {self.worker_id} stopped")

    async def _recover_stale_tasks(self):
        """Reset stale tasks for retry."""
        stale_cutoff = datetime.utcnow() - timedelta(seconds=STALE_THRESHOLD)
        recovered = self.queue.recover_stale_tasks(stale_cutoff)
        if recovered:
            logger.info(f"Recovered {recovered} stale tasks")

    async def _try_claim_task(self) -> Optional[Task]:
        """Try to claim the next available task, respecting concurrency."""

        # Get next queued task (peek, don't claim yet)
        task = self.queue.peek_next_task()
        if not task:
            return None

        # Get actor for this task
        actor = ActorRegistry.get(task.actor)
        if not actor:
            logger.error(f"No actor registered for: {task.actor}")
            self.queue.fail_task(
                task.task_id,
                error_type="ConfigurationError",
                message=f"No actor registered for: {task.actor}",
                details={},
                stack_trace=""
            )
            return None

        # Get concurrency info from actor
        concurrency_id, max_concurrent = actor.get_concurrency_info(task.payload)

        # Check current concurrency
        current_count = self.queue.count_processing(concurrency_id)
        if current_count >= max_concurrent:
            # At concurrency limit, can't process this task yet
            return None

        # Claim the task
        claimed = self.queue.claim_task(
            task.task_id,
            self.worker_id,
            concurrency_id,
            max_concurrent
        )

        return claimed

    async def _process_task(self, task: Task):
        """Process a claimed task."""
        logger.info(f"Processing task {task.task_id} (actor={task.actor})")

        self._current_task = task

        # Start heartbeat
        self._heartbeat_task = asyncio.create_task(
            self._heartbeat_loop(task.task_id)
        )

        try:
            # Get actor
            actor = ActorRegistry.get(task.actor)

            # Progress callback updates DB
            def progress_callback(elapsed_ms: int, message: str):
                self.queue.update_progress(task.task_id, elapsed_ms, message)

            # Execute in thread pool (providers are synchronous)
            loop = asyncio.get_event_loop()
            result = await loop.run_in_executor(
                None,
                lambda: actor.execute(task.payload, progress_callback)
            )

            # Mark completed
            self.queue.complete_task(task.task_id, result)
            logger.info(f"Task {task.task_id} completed")

        except Exception as e:
            # Mark failed with full error info
            self.queue.fail_task(
                task.task_id,
                error_type=type(e).__name__,
                message=str(e),
                details=getattr(e, '__dict__', {}),
                stack_trace=traceback.format_exc()
            )
            logger.error(f"Task {task.task_id} failed: {e}")

        finally:
            self._current_task = None
            if self._heartbeat_task:
                self._heartbeat_task.cancel()
                try:
                    await self._heartbeat_task
                except asyncio.CancelledError:
                    pass
                self._heartbeat_task = None

    async def _heartbeat_loop(self, task_id: str):
        """Update heartbeat while processing."""
        while True:
            await asyncio.sleep(HEARTBEAT_INTERVAL)
            self.queue.update_heartbeat(task_id)
```

### Task Queue Class

```python
# worker/queue.py

from dataclasses import dataclass
from datetime import datetime
from typing import Dict, Any, Optional, List

from db import Database
from utils import uuid7_str


@dataclass
class Task:
    """Task data from queue."""
    task_id: str
    actor: str
    payload: Dict[str, Any]
    status: str
    priority: int
    concurrency_identifier: Optional[str]
    concurrency_limit: Optional[int]
    retry_count: int
    max_retries: int


class TaskQueue:
    """Database-backed task queue operations."""

    def __init__(self):
        # TODO: Get connection from config
        self.db = Database()
        self.collection = self.db.db.task_queue
        self._ensure_indexes()

    def _ensure_indexes(self):
        """Create required indexes."""
        self.collection.create_index([
            ("status", 1),
            ("concurrency_identifier", 1),
            ("priority", -1),
            ("created_at", 1)
        ])
        self.collection.create_index([("status", 1), ("heartbeat_at", 1)])
        self.collection.create_index("task_id", unique=True)
        self.collection.create_index([
            ("payload.workflow_run_id", 1),
            ("created_at", -1)
        ])

    def enqueue(
        self,
        actor: str,
        payload: Dict[str, Any],
        priority: int = 0,
        max_retries: int = 3,
    ) -> str:
        """Add a task to the queue."""
        task_id = f"tq_{uuid7_str()}"

        self.collection.insert_one({
            "task_id": task_id,
            "actor": actor,
            "status": "queued",
            "priority": priority,
            "concurrency_identifier": None,
            "concurrency_limit": None,
            "payload": payload,
            "result": None,
            "error": None,
            "progress": {
                "elapsed_ms": 0,
                "message": "Queued",
                "updated_at": datetime.utcnow(),
            },
            "created_at": datetime.utcnow(),
            "started_at": None,
            "completed_at": None,
            "worker_id": None,
            "heartbeat_at": None,
            "retry_count": 0,
            "max_retries": max_retries,
        })

        return task_id

    def peek_next_task(self) -> Optional[Task]:
        """Get next queued task without claiming it."""
        doc = self.collection.find_one(
            {"status": "queued"},
            sort=[("priority", -1), ("created_at", 1)]
        )

        if not doc:
            return None

        return Task(
            task_id=doc["task_id"],
            actor=doc["actor"],
            payload=doc["payload"],
            status=doc["status"],
            priority=doc["priority"],
            concurrency_identifier=doc.get("concurrency_identifier"),
            concurrency_limit=doc.get("concurrency_limit"),
            retry_count=doc["retry_count"],
            max_retries=doc["max_retries"],
        )

    def count_processing(self, concurrency_identifier: str) -> int:
        """Count tasks currently processing with given identifier."""
        return self.collection.count_documents({
            "status": "processing",
            "concurrency_identifier": concurrency_identifier,
        })

    def claim_task(
        self,
        task_id: str,
        worker_id: str,
        concurrency_identifier: str,
        concurrency_limit: int,
    ) -> Optional[Task]:
        """Atomically claim a task."""
        from pymongo import ReturnDocument

        doc = self.collection.find_one_and_update(
            {
                "task_id": task_id,
                "status": "queued",
            },
            {
                "$set": {
                    "status": "processing",
                    "worker_id": worker_id,
                    "concurrency_identifier": concurrency_identifier,
                    "concurrency_limit": concurrency_limit,
                    "started_at": datetime.utcnow(),
                    "heartbeat_at": datetime.utcnow(),
                    "progress.message": "Processing",
                    "progress.updated_at": datetime.utcnow(),
                }
            },
            return_document=ReturnDocument.AFTER
        )

        if not doc:
            return None

        return Task(
            task_id=doc["task_id"],
            actor=doc["actor"],
            payload=doc["payload"],
            status=doc["status"],
            priority=doc["priority"],
            concurrency_identifier=doc.get("concurrency_identifier"),
            concurrency_limit=doc.get("concurrency_limit"),
            retry_count=doc["retry_count"],
            max_retries=doc["max_retries"],
        )

    def update_progress(self, task_id: str, elapsed_ms: int, message: str):
        """Update task progress."""
        self.collection.update_one(
            {"task_id": task_id},
            {
                "$set": {
                    "progress.elapsed_ms": elapsed_ms,
                    "progress.message": message,
                    "progress.updated_at": datetime.utcnow(),
                }
            }
        )

    def update_heartbeat(self, task_id: str):
        """Update task heartbeat."""
        self.collection.update_one(
            {"task_id": task_id},
            {"$set": {"heartbeat_at": datetime.utcnow()}}
        )

    def complete_task(self, task_id: str, result: Dict[str, Any]):
        """Mark task as completed."""
        self.collection.update_one(
            {"task_id": task_id},
            {
                "$set": {
                    "status": "completed",
                    "result": result,
                    "completed_at": datetime.utcnow(),
                    "progress.message": "Completed",
                    "progress.updated_at": datetime.utcnow(),
                }
            }
        )

    def fail_task(
        self,
        task_id: str,
        error_type: str,
        message: str,
        details: Dict[str, Any],
        stack_trace: str,
    ):
        """Mark task as failed."""
        self.collection.update_one(
            {"task_id": task_id},
            {
                "$set": {
                    "status": "failed",
                    "error": {
                        "type": error_type,
                        "message": message,
                        "details": details,
                        "stack_trace": stack_trace,
                    },
                    "completed_at": datetime.utcnow(),
                    "progress.message": f"Failed: {message}",
                    "progress.updated_at": datetime.utcnow(),
                }
            }
        )

    def recover_stale_tasks(self, stale_cutoff: datetime) -> int:
        """Reset stale tasks for retry. Returns count of recovered tasks."""
        recovered = 0

        stale_tasks = list(self.collection.find({
            "status": "processing",
            "heartbeat_at": {"$lt": stale_cutoff}
        }))

        for task in stale_tasks:
            if task["retry_count"] < task["max_retries"]:
                self.collection.update_one(
                    {"task_id": task["task_id"]},
                    {
                        "$set": {
                            "status": "queued",
                            "worker_id": None,
                            "heartbeat_at": None,
                            "concurrency_identifier": None,
                            "progress.message": f"Retrying (attempt {task['retry_count'] + 1})",
                            "progress.updated_at": datetime.utcnow(),
                        },
                        "$inc": {"retry_count": 1}
                    }
                )
                recovered += 1
            else:
                self.collection.update_one(
                    {"task_id": task["task_id"]},
                    {
                        "$set": {
                            "status": "failed",
                            "error": {
                                "type": "MaxRetriesExceeded",
                                "message": f"Task failed after {task['max_retries']} retries",
                                "details": {},
                                "stack_trace": "",
                            },
                            "completed_at": datetime.utcnow(),
                            "progress.message": "Failed: max retries exceeded",
                            "progress.updated_at": datetime.utcnow(),
                        }
                    }
                )

        return recovered

    def get_task(self, task_id: str) -> Optional[Dict[str, Any]]:
        """Get task by ID."""
        return self.collection.find_one(
            {"task_id": task_id},
            {"_id": 0}
        )

    def get_tasks_for_workflow(
        self,
        workflow_run_id: str,
        limit: int = 100
    ) -> List[Dict[str, Any]]:
        """Get tasks for a workflow."""
        return list(self.collection.find(
            {"payload.workflow_run_id": workflow_run_id},
            {"_id": 0}
        ).sort("created_at", -1).limit(limit))
```

---

## API Changes

### New Task Endpoints

```python
# server/api/routes/tasks.py

from fastapi import APIRouter, HTTPException, Depends
from pydantic import BaseModel
from typing import Dict, Any, Optional

from worker.queue import TaskQueue

router = APIRouter(prefix="/api/task", tags=["tasks"])


class CreateTaskRequest(BaseModel):
    actor: str
    payload: Dict[str, Any]
    priority: int = 0


class CreateTaskResponse(BaseModel):
    task_id: str


@router.post("", response_model=CreateTaskResponse)
async def create_task(request: CreateTaskRequest):
    """Create a new task in the queue."""
    queue = TaskQueue()
    task_id = queue.enqueue(
        actor=request.actor,
        payload=request.payload,
        priority=request.priority,
    )
    return CreateTaskResponse(task_id=task_id)


@router.get("/{task_id}")
async def get_task(task_id: str):
    """Get task status and details."""
    queue = TaskQueue()
    task = queue.get_task(task_id)
    if not task:
        raise HTTPException(status_code=404, detail="Task not found")
    return task
```

### SSE Task Stream

```python
# server/api/routes/tasks.py (continued)

import asyncio
import json
from sse_starlette.sse import EventSourceResponse


@router.get("/{task_id}/stream")
async def stream_task(task_id: str):
    """Stream task progress via SSE."""
    queue = TaskQueue()

    async def event_generator():
        last_progress = None

        while True:
            task = queue.get_task(task_id)

            if not task:
                yield {"event": "error", "data": json.dumps({"message": "Task not found"})}
                return

            # Emit progress if changed
            current_progress = task.get("progress", {})
            if current_progress != last_progress:
                yield {
                    "event": "progress",
                    "data": json.dumps({
                        "status": task["status"],
                        "progress": current_progress,
                    })
                }
                last_progress = current_progress

            # Check for completion
            if task["status"] == "completed":
                yield {
                    "event": "complete",
                    "data": json.dumps({
                        "result": task["result"]
                    })
                }
                return

            if task["status"] == "failed":
                yield {
                    "event": "error",
                    "data": json.dumps({
                        "error": task["error"]
                    })
                }
                return

            # Poll interval
            await asyncio.sleep(1)

    return EventSourceResponse(event_generator())
```

### Updated Media Sub-Action

```python
# server/api/routes/streaming.py

# Replace execute_media_sub_action call with task queue

@router.post("/{workflow_run_id}/interaction/{interaction_id}/sub-action")
async def create_sub_action(
    workflow_run_id: str,
    interaction_id: str,
    request: SubActionRequest,
    db = Depends(get_db),
):
    """Create a media generation sub-action task."""

    # Validate interaction exists
    interaction_event = db.event_repo.get_event({
        "workflow_run_id": workflow_run_id,
        "event_type": DbEventType.INTERACTION_REQUESTED.value,
        "data.interaction_id": interaction_id
    })

    if not interaction_event:
        raise HTTPException(status_code=404, detail="Interaction not found")

    # Enqueue task
    queue = TaskQueue()
    task_id = queue.enqueue(
        actor="media",
        payload={
            "workflow_run_id": workflow_run_id,
            "interaction_id": interaction_id,
            "provider": request.provider,
            "action_type": request.action_type,
            "prompt_id": request.prompt_id,
            "params": request.params,
            "source_data": request.source_data,
        }
    )

    return {"task_id": task_id}
```

---

## WebUI Changes

The WebUI needs to be updated to use the new task-based flow:

### Old Flow
```typescript
// Start generation → open SSE → receive progress/complete events
const eventSource = new EventSource(`/workflow/${id}/interaction/${iid}/sub-action/stream?...`);
```

### New Flow
```typescript
// 1. Create task
const response = await fetch(`/api/task`, {
  method: 'POST',
  body: JSON.stringify({
    actor: 'media',
    payload: { workflow_run_id, interaction_id, provider, action_type, ... }
  })
});
const { task_id } = await response.json();

// 2. Stream task progress
const eventSource = new EventSource(`/api/task/${task_id}/stream`);
eventSource.addEventListener('progress', (e) => { ... });
eventSource.addEventListener('complete', (e) => { ... });
eventSource.addEventListener('error', (e) => { ... });
```

---

## Implementation Plan

### Phase 1: Infrastructure (Extract Providers Package)

#### 1.1 Create `providers/` package structure
- Create `providers/__init__.py`
- Create `providers/media/__init__.py`
- Ensure Python path considerations: both `server/` and `worker/` need to import from `providers/`
- May need to adjust `PYTHONPATH` or use relative imports carefully
- **Commit**: "chore: Create providers package structure"

#### 1.2 Move provider base classes and exceptions
- Move `server/modules/media/base.py` → `providers/media/base.py`
- Contains: `MediaProvider` ABC, `GenerationResult`, all exception classes
- No import changes needed in base.py itself (it's self-contained)
- **Commit**: "refactor: Move media provider base classes to providers package"

#### 1.3 Move download utility
- Move `server/modules/media/download.py` → `providers/media/download.py`
- Check imports: uses `requests`, standard lib only - should be clean
- **Commit**: "refactor: Move media download utility to providers package"

#### 1.4 Move provider implementations
- Move `server/modules/media/leonardo/` → `providers/media/leonardo/`
- Move `server/modules/media/midapi/` → `providers/media/midapi/`
- Update internal imports within each provider to use `providers.media.base`
- Check for any imports from `server.*` that need adjustment
- **Commit**: "refactor: Move Leonardo provider to providers package"
- **Commit**: "refactor: Move MidJourney provider to providers package"

#### 1.5 Move and update registry
- Move `server/modules/media/registry.py` → `providers/media/registry.py`
- Add `concurrency` parameter to registration
- Update provider imports to use new paths
- Set default concurrency values (leonardo=3, midjourney=1)
- **Commit**: "refactor: Move provider registry to providers package with concurrency support"

#### 1.6 Update `providers/media/__init__.py` exports
- Export `MediaProviderRegistry`, `MediaProvider`, exceptions, `GenerationResult`
- Export `download_media`, `DownloadError`
- This becomes the public API for the providers package
- **Commit**: "refactor: Add providers.media public exports"

#### 1.7 Update server imports
- Update `server/modules/media/__init__.py` to re-export from `providers.media`
- Update `server/modules/media/sub_action.py` imports
- Update `server/api/routes/streaming.py` imports
- Update any other files in `server/` that import from `server.modules.media`
- Search for all `from server.modules.media` and `from .` imports in media module
- **Commit**: "refactor: Update server to import from providers package"

#### 1.8 Verify server still works
- Run server, test media generation still functions
- Check all provider methods accessible
- Verify no import errors on startup
- **Commit**: None (verification step)

---

### Phase 2: Worker Package

#### 2.1 Create `worker/` package structure
- Create `worker/__init__.py`
- Create `worker/__main__.py` (entry point)
- Create `worker/actors/__init__.py`
- Test: `python -m worker` should at least import without errors
- **Commit**: "feat(worker): Create worker package structure"

#### 2.2 Implement TaskQueue class
- Create `worker/queue.py`
- Implement all queue operations: `enqueue`, `peek_next_task`, `claim_task`, `count_processing`, `update_progress`, `update_heartbeat`, `complete_task`, `fail_task`, `recover_stale_tasks`, `get_task`, `get_tasks_for_workflow`
- Create indexes on first init
- Handle DB connection (reuse existing `Database` class pattern)
- Consider: TaskQueue needs DB connection - should it create its own or receive one?
- Decision: Create own connection (worker is separate process)
- **Commit**: "feat(worker): Implement TaskQueue class"

#### 2.3 Implement ActorBase ABC
- Create `worker/actors/base.py`
- Define abstract interface: `name`, `get_concurrency_info()`, `execute()`
- Define `ProgressCallback` type alias
- Keep it minimal - just the interface
- **Commit**: "feat(worker): Implement ActorBase abstract class"

#### 2.4 Implement ActorRegistry
- Update `worker/actors/__init__.py`
- Simple dict-based registry with `register()`, `get()`, `names()`
- Do NOT auto-register actors yet (circular import risk)
- **Commit**: "feat(worker): Implement ActorRegistry"

#### 2.5 Implement MediaActor
- Create `worker/actors/media.py`
- Import from `providers.media` (NOT from `server.modules.media`)
- Implement `get_concurrency_info()` - delegates to `ProviderRegistry.get_concurrency()`
- Implement `execute()` - full generation flow (provider call, download, DB updates)
- Handle all the logic currently in `execute_media_sub_action`
- Consider: MediaActor needs DB access for `content_repo` - create own connection
- Consider: MediaActor needs `get_media_images_path()` etc. - these are in `server/api/dependencies.py`
  - Option A: Move these utils to shared location
  - Option B: Duplicate in worker
  - Option C: Import from server (creates dependency)
  - Decision: These are config-based paths - move to shared config or duplicate
- **Commit**: "feat(worker): Implement MediaActor"

#### 2.6 Implement worker main loop
- Create `worker/loop.py`
- Implement `WorkerLoop` class with main loop, task claiming, heartbeat, stale recovery
- Handle graceful shutdown (SIGINT, SIGTERM)
- Run executor in thread pool (providers are sync)
- Update `worker/__main__.py` to use WorkerLoop
- **Commit**: "feat(worker): Implement worker main loop"

#### 2.7 Register MediaActor
- Update `worker/actors/__init__.py` to import and register MediaActor
- Handle import order carefully to avoid circular imports
- **Commit**: "feat(worker): Register MediaActor in ActorRegistry"

#### 2.8 Manual testing of worker
- Start worker: `python -m worker`
- Manually insert a task into `task_queue` collection via mongosh
- Verify worker picks up task, processes it, updates status
- Verify heartbeat updates during processing
- Verify completion/failure handling
- Test stale task recovery by killing worker mid-task
- **Commit**: None (verification step)

---

### Phase 3: API Integration

#### 3.1 Create task API routes
- Create `server/api/routes/tasks.py`
- Implement `POST /api/task` - create task (enqueue)
- Implement `GET /api/task/{task_id}` - get task status
- Add pydantic models for request/response
- Consider: Should server import from `worker.queue`?
  - This creates server → worker dependency
  - Alternative: Extract queue to shared location
  - Decision: For now, server can import `worker.queue.TaskQueue` (it's just DB operations)
- **Commit**: "feat(api): Add task creation and status endpoints"

#### 3.2 Add SSE task stream endpoint
- Add `GET /api/task/{task_id}/stream` to `tasks.py`
- Poll DB for task status changes
- Emit progress, complete, error events
- Handle client disconnect gracefully
- Consider poll interval (1 second as designed)
- **Commit**: "feat(api): Add SSE task progress streaming endpoint"

#### 3.3 Register task routes in app
- Update `server/api/routes/__init__.py` to include tasks router
- Update `server/api/app.py` if needed
- Verify endpoints accessible
- **Commit**: "feat(api): Register task routes in application"

#### 3.4 Update sub-action endpoint
- Modify `server/api/routes/streaming.py`
- Change from direct execution to enqueue + return task_id
- Keep validation logic (check interaction exists)
- Return `{"task_id": "..."}` instead of SSE stream
- **Commit**: "refactor(api): Update sub-action endpoint to use task queue"

#### 3.5 API integration testing
- Test full flow: POST sub-action → get task_id → GET task status
- Test SSE stream receives progress from worker
- Verify old SSE endpoint no longer used (or returns task_id)
- **Commit**: None (verification step)

---

### Phase 4: WebUI Updates

#### 4.1 Add task API client functions
- Update `webui/src/lib/api.ts`
- Add `createTask(actor, payload)` function
- Add `getTask(taskId)` function
- Add `streamTask(taskId, callbacks)` function for SSE
- **Commit**: "feat(webui): Add task API client functions"

#### 4.2 Update MediaGeneration to use task flow
- Update `webui/src/components/workflow/interactions/media-generation/`
- Change generation trigger to:
  1. Call `createTask()` (or updated sub-action endpoint)
  2. Get `task_id` from response
  3. Open SSE stream to `/api/task/{task_id}/stream`
- Update progress/completion handlers for new event format
- Consider: Store task_id in component state for reconnection support
- **Commit**: "refactor(webui): Update MediaGeneration to use task-based flow"

#### 4.3 Update SSE event handling
- Update event listeners for new event names/formats:
  - `progress` → `{ status, progress: { elapsed_ms, message } }`
  - `complete` → `{ result: { metadata_id, content_ids, urls } }`
  - `error` → `{ error: { type, message, details, stack_trace } }`
- Map to existing UI state structure
- **Commit**: "refactor(webui): Update SSE event handling for task stream format"

#### 4.4 Handle reconnection (optional enhancement)
- If page refreshes mid-generation, task continues in worker
- On component mount, check for in-progress tasks for this interaction
- Reconnect to SSE stream if task is still processing
- This provides the "refresh doesn't lose work" benefit
- **Commit**: "feat(webui): Add task reconnection on page refresh"

#### 4.5 WebUI testing
- Test full generation flow through UI
- Test page refresh during generation (should reconnect)
- Test error handling display
- Verify progress updates display correctly
- **Commit**: None (verification step)

---

### Phase 5: Cleanup

#### 5.1 Remove old execute_media_sub_action
- Delete or deprecate `server/modules/media/sub_action.py`
- This file contained the old SSE-based execution logic
- Verify no other code imports from it
- **Commit**: "refactor: Remove old execute_media_sub_action module"

#### 5.2 Clean up old streaming routes
- Remove old SSE sub-action streaming endpoint from `streaming.py`
- Keep other streaming endpoints (workflow stream, state stream)
- Update route registrations if needed
- **Commit**: "refactor: Remove old sub-action SSE streaming endpoint"

#### 5.3 Clean up server/modules/media
- `server/modules/media/` should now be minimal (just re-exports)
- Consider: Remove entirely and update all imports to use `providers.media`?
- Decision: Keep thin re-export layer for backwards compatibility during transition
- Remove any dead code
- **Commit**: "refactor: Clean up server/modules/media after migration"

#### 5.4 Update documentation
- Update any README or docs that reference old flow
- Add worker startup instructions
- Document new API endpoints
- Add architecture diagram if helpful
- **Commit**: "docs: Update documentation for task queue architecture"

#### 5.5 Final verification
- Full end-to-end test of generation flow
- Test worker restart recovery
- Test multiple concurrent generations (respecting concurrency limits)
- Test error scenarios
- **Commit**: None (verification step)

---

### Rollback Plan

If issues are discovered after deployment:
1. Worker can be stopped (tasks stay queued, no data loss)
2. Revert API to use old `execute_media_sub_action` directly
3. Revert WebUI to old SSE flow
4. Queued tasks can be manually cleared or left for later

### Open Considerations

1. **Shared config for media paths**: `get_media_images_path()` etc. are in `server/api/dependencies.py`. Worker needs these. Options:
   - Move to shared config module (cleanest)
   - Environment variables (already used, just need to read in worker)
   - Import from server (creates dependency)

2. **Database connection management**: Both server and worker create their own `Database()` instances. This is fine for MongoDB (connection pooling handled by driver).

3. **Logging configuration**: Worker needs its own logging setup. Should be consistent with server format.

4. **Error notification**: Currently errors are just stored in DB. Consider adding alerting for repeated failures (future enhancement).

---

## Questions Resolved

1. **File structure**: Option D - providers at root ✓
2. **Concurrency**: Actor pattern with registry ✓
3. **Task retention**: Keep forever ✓
4. **Progress**: Keep as-is (elapsed_ms + message) ✓
5. **Errors**: Full stack trace + structured ✓
6. **WebUI**: Breaking changes acceptable ✓
