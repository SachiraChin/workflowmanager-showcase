# Task Queue Architecture

## Summary

This document describes the architecture for a persistent, database-backed task queue system. The primary motivation is to fix a flaw in media generation SSE handling where closing the SSE stream (e.g., page refresh) causes generation results to be lost because DB write logic executes outside the asyncio thread that continues running.

The solution decouples task execution from SSE streaming by:
1. Storing tasks in a DB queue
2. Running a separate worker process that processes tasks independently
3. SSE endpoints poll task status from DB

## Problem Statement

Current flow in `server/modules/media/sub_action.py`:
```
[SSE Request]
  → Create metadata (DB write)
  → Run provider in thread pool executor
  → await future (this is where SSE can disconnect)
  → Download content (never reached if disconnected)
  → Update DB with results (never reached if disconnected)
```

When SSE disconnects:
- Thread pool executor continues running provider
- Generation completes successfully in background
- But DB writes (completion status, content download) never happen
- Result: Lost generation, wasted API credits

## Proposed Solution

### High-Level Architecture

```
┌─────────────────────────────────────────────────────────────┐
│                        FastAPI Server                        │
│  ┌─────────────┐  ┌─────────────┐  ┌─────────────────────┐  │
│  │ POST /task  │  │ GET /task   │  │ SSE /task/stream    │  │
│  │ (enqueue)   │  │ (status)    │  │ (poll progress)     │  │
│  └──────┬──────┘  └──────┬──────┘  └──────────┬──────────┘  │
│         │                │                     │             │
│         └────────────────┼─────────────────────┘             │
│                          │                                   │
│                    ┌─────▼─────┐                             │
│                    │  MongoDB  │                             │
│                    │task_queue │                             │
│                    └─────┬─────┘                             │
└──────────────────────────┼───────────────────────────────────┘
                           │
              ┌────────────▼────────────┐
              │   Worker Process        │
              │  (separate process)     │
              │                         │
              │  ┌───────────────────┐  │
              │  │ Poll for tasks    │  │
              │  │ Claim & process   │  │
              │  │ Update progress   │  │
              │  │ Heartbeat         │  │
              │  │ Mark complete     │  │
              │  └───────────────────┘  │
              │                         │
              │  Task Handlers:         │
              │  - media_generation     │
              │  - (future types...)    │
              └─────────────────────────┘
```

### New Flow

```
[API Request to generate media]
  → Insert task into task_queue (status: "queued")
  → Return task_id to client

[Worker Process - runs independently]
  → Poll DB for queued tasks
  → Claim task (atomic: status → "processing", set worker_id, heartbeat_at)
  → Execute handler (provider call + download + content_repo updates)
  → Update progress in DB periodically
  → Update heartbeat every 5 seconds
  → Mark task completed/failed with result

[SSE Endpoint - client subscribes]
  → Poll task_queue for status/progress by task_id
  → Yield progress events to client
  → Client disconnect is harmless - task continues in worker
  → Client can reconnect and resume polling same task_id
```

---

## Database Schema

### Collection: `task_queue`

```python
{
    # Identity
    "task_id": "tq_01abc...",           # Unique task identifier
    "task_type": "media_generation",     # Routes to appropriate handler
    "queue_name": "default",             # For partitioning (future: per-provider queues)

    # Status
    "status": "queued",                  # queued | processing | completed | failed
    "priority": 0,                       # For future priority support (higher = sooner)

    # Task data
    "payload": {                         # Task-specific input (varies by task_type)
        # For media_generation:
        "workflow_run_id": "...",
        "interaction_id": "...",
        "provider": "midjourney",
        "action_type": "txt2img",
        "prompt_id": "...",
        "params": { ... },
        "source_data": { ... }
    },
    "result": {                          # Task-specific output (set on completion)
        # For media_generation:
        "metadata_id": "...",
        "content_ids": ["gc_...", ...],
        "urls": ["...", ...]
    },
    "error": null,                       # Error message if status=failed

    # Progress tracking
    "progress": {
        "percent": 0,                    # 0-100
        "message": "Queued",
        "elapsed_ms": 0,
        "updated_at": "2026-01-20T..."
    },

    # Timestamps
    "created_at": "2026-01-20T...",
    "started_at": null,                  # When worker claimed it
    "completed_at": null,                # When finished (success or fail)

    # Worker tracking
    "worker_id": null,                   # Which worker is processing
    "heartbeat_at": null,                # Last heartbeat from worker

    # Retry handling
    "retry_count": 0,
    "max_retries": 3,
}
```

### Indexes

```python
# For polling queued tasks
{"status": 1, "priority": -1, "created_at": 1}

# For stale task detection
{"status": 1, "heartbeat_at": 1}

# For task lookup
{"task_id": 1}  # unique

# For listing tasks by workflow
{"payload.workflow_run_id": 1, "created_at": -1}
```

---

## Worker Process Design

### Main Loop

```python
async def worker_loop():
    worker_id = generate_worker_id()

    while True:
        # 1. Check for stale tasks (on startup and periodically)
        await recover_stale_tasks()

        # 2. Try to claim a task
        task = await claim_next_task(worker_id)

        if task:
            # 3. Process the task
            await process_task(task, worker_id)
        else:
            # 4. No tasks available, wait before polling again
            await asyncio.sleep(1)
```

### Task Claiming (Atomic)

```python
async def claim_next_task(worker_id: str) -> Optional[dict]:
    """Atomically claim the next available task."""
    result = db.task_queue.find_one_and_update(
        {
            "status": "queued",
            # Future: add queue_name filter for concurrency control
        },
        {
            "$set": {
                "status": "processing",
                "worker_id": worker_id,
                "started_at": datetime.utcnow(),
                "heartbeat_at": datetime.utcnow(),
            }
        },
        sort=[("priority", -1), ("created_at", 1)],  # Highest priority, oldest first
        return_document=ReturnDocument.AFTER
    )
    return result
```

### Heartbeat

```python
HEARTBEAT_INTERVAL = 5  # seconds

async def heartbeat_loop(task_id: str, stop_event: asyncio.Event):
    """Update heartbeat while task is processing."""
    while not stop_event.is_set():
        db.task_queue.update_one(
            {"task_id": task_id},
            {"$set": {"heartbeat_at": datetime.utcnow()}}
        )
        await asyncio.sleep(HEARTBEAT_INTERVAL)
```

### Stale Task Recovery

```python
STALE_THRESHOLD = 30  # seconds

async def recover_stale_tasks():
    """Reset stale tasks for retry."""
    stale_cutoff = datetime.utcnow() - timedelta(seconds=STALE_THRESHOLD)

    stale_tasks = db.task_queue.find({
        "status": "processing",
        "heartbeat_at": {"$lt": stale_cutoff}
    })

    for task in stale_tasks:
        if task["retry_count"] < task["max_retries"]:
            db.task_queue.update_one(
                {"task_id": task["task_id"]},
                {
                    "$set": {
                        "status": "queued",
                        "worker_id": None,
                        "heartbeat_at": None,
                        "progress.message": f"Retrying (attempt {task['retry_count'] + 1})"
                    },
                    "$inc": {"retry_count": 1}
                }
            )
            logger.warning(f"Reset stale task {task['task_id']} for retry")
        else:
            db.task_queue.update_one(
                {"task_id": task["task_id"]},
                {
                    "$set": {
                        "status": "failed",
                        "error": "Exceeded max retries after worker failures",
                        "completed_at": datetime.utcnow()
                    }
                }
            )
            logger.error(f"Task {task['task_id']} failed after {task['max_retries']} retries")
```

### Task Handler Interface

```python
from abc import ABC, abstractmethod
from typing import Callable, Dict, Any

class TaskHandler(ABC):
    """Base class for task handlers."""

    @property
    @abstractmethod
    def task_type(self) -> str:
        """Task type this handler processes."""
        pass

    @abstractmethod
    def execute(
        self,
        payload: Dict[str, Any],
        progress_callback: Callable[[int, str], None]
    ) -> Dict[str, Any]:
        """
        Execute the task.

        Args:
            payload: Task-specific input data
            progress_callback: Call with (percent, message) to update progress

        Returns:
            Task-specific result dict

        Raises:
            Exception: On failure (will be caught and stored as error)
        """
        pass
```

### Media Generation Handler

```python
class MediaGenerationHandler(TaskHandler):
    task_type = "media_generation"

    def execute(self, payload, progress_callback):
        # Extract payload
        workflow_run_id = payload["workflow_run_id"]
        provider_name = payload["provider"]
        action_type = payload["action_type"]
        # ... etc

        # Get provider (imported from providers package)
        provider = MediaProviderRegistry.get(provider_name)
        method = getattr(provider, action_type)

        # Create metadata record
        metadata_id = db.content_repo.store_generation(...)

        try:
            # Call provider (synchronous)
            result = method(prompt, params, progress_callback=progress_callback)

            # Download content
            content_ids = []
            urls = []
            for index, provider_url in enumerate(result.urls):
                content_id = f"gc_{uuid7_str()}"
                download_result = download_media(...)
                db.content_repo.store_content_with_download(...)
                content_ids.append(content_id)
                urls.append(server_url)

            # Update metadata
            db.content_repo.update_generation_status(
                metadata_id=metadata_id,
                status="completed",
                response_data=result.raw_response
            )

            return {
                "metadata_id": metadata_id,
                "content_ids": content_ids,
                "urls": urls
            }

        except Exception as e:
            db.content_repo.update_generation_status(
                metadata_id=metadata_id,
                status="failed",
                error_message=str(e)
            )
            raise
```

---

## Concurrency Design (Future)

Design with concurrency in mind, implement single-task for now.

### Configuration Structure

```python
QUEUE_CONFIG = {
    "default": {
        "max_concurrent": 1,
    },
    "media:midjourney": {
        "max_concurrent": 1,  # MidJourney is rate-limited
    },
    "media:leonardo": {
        "max_concurrent": 3,  # Leonardo allows more
    },
}
```

### Concurrency Check (Future Implementation)

```python
async def can_claim_from_queue(queue_name: str) -> bool:
    """Check if we can claim another task from this queue."""
    config = QUEUE_CONFIG.get(queue_name, QUEUE_CONFIG["default"])
    max_concurrent = config["max_concurrent"]

    current_count = db.task_queue.count_documents({
        "queue_name": queue_name,
        "status": "processing"
    })

    return current_count < max_concurrent
```

---

## File Structure Options

The worker process needs access to provider logic (currently in `server/modules/media/`). Here are the options:

### Option A: Worker Inside Server

```
server/
  worker/
    __init__.py
    __main__.py           # Entry: python -m server.worker
    queue.py              # TaskQueue class
    loop.py               # Worker main loop
    handlers/
      __init__.py
      base.py             # TaskHandler ABC
      media.py            # MediaGenerationHandler
  modules/
    media/
      providers/
      base.py
      registry.py
  api/
  engine/
```

**Imports**: `from server.modules.media import MediaProviderRegistry`

**Pros**:
- Simple imports
- No refactoring of existing code

**Cons**:
- Conceptually muddy - worker is "inside" server but runs as separate process
- `server/` package becomes overloaded

---

### Option B: Worker at Root, Imports from Server

```
worker/
  __init__.py
  __main__.py             # Entry: python -m worker
  queue.py
  loop.py
  handlers/
    __init__.py
    base.py
    media.py              # imports from server.modules.media
server/
  modules/
    media/
      providers/
      ...
  api/
  engine/
```

**Imports**: `from server.modules.media import MediaProviderRegistry`

**Pros**:
- Clear separation - worker is visibly a separate process
- Minimal refactoring

**Cons**:
- Cross-package dependency (worker → server)
- Unusual pattern

---

### Option C: Extract Shared Modules to Root

```
modules/                  # Shared business logic
  media/
    providers/
      leonardo/
      midjourney/
    base.py
    registry.py
    download.py
server/                   # FastAPI application
  api/
  engine/
  # imports from modules.media
worker/                   # Worker process
  __init__.py
  __main__.py
  queue.py
  loop.py
  handlers/
    base.py
    media.py              # imports from modules.media
```

**Imports**: `from modules.media import MediaProviderRegistry`

**Pros**:
- Clean separation of concerns
- Both server and worker import from shared `modules/`
- Scales well as more shared code is added

**Cons**:
- Requires moving `server/modules/` → `modules/`
- Need to update all imports in server
- `server/engine/` also uses modules - need to verify compatibility

---

### Option D: Extract Only Provider Logic to Root

```
providers/                # Provider implementations only
  media/
    leonardo/
    midjourney/
    base.py
    registry.py
    download.py
server/
  modules/                # Server-specific module logic
    media/
      sub_action.py       # Will be simplified/removed
      __init__.py         # Re-exports from providers.media
  api/
  engine/
worker/
  __init__.py
  __main__.py
  queue.py
  loop.py
  handlers/
    base.py
    media.py              # imports from providers.media
```

**Imports**: `from providers.media import MediaProviderRegistry`

**Pros**:
- Providers are clearly shared infrastructure
- Server modules can still have server-specific logic
- Clean conceptual model: providers are external service integrations

**Cons**:
- Need to move provider code
- `server/modules/media/` becomes thin wrapper or removed
- Similar refactoring effort to Option C

---

### Comparison Matrix

| Aspect | A (inside server) | B (root, imports server) | C (shared modules) | D (providers at root) |
|--------|-------------------|--------------------------|--------------------|-----------------------|
| Refactoring effort | None | Minimal | Medium | Medium |
| Conceptual clarity | Low | Medium | High | High |
| Import pattern | Natural | Cross-package | Clean | Clean |
| Future scalability | Low | Medium | High | High |
| Separation of concerns | Low | Medium | High | High |

---

## Worker Management

### Manual Start (Development)

```bash
# Start worker
python -m worker

# Or if inside server/
python -m server.worker

# Stop: Ctrl+C or kill <pid>
```

### Systemd Service (Production - Future)

```ini
# /etc/systemd/system/wm-worker.service
[Unit]
Description=WM Task Queue Worker
After=network.target mongodb.service

[Service]
Type=simple
User=wm
WorkingDirectory=/path/to/wm-dev
ExecStart=/path/to/venv/bin/python -m worker
Restart=always
RestartSec=5

[Install]
WantedBy=multi-user.target
```

---

## API Changes

### New Endpoints

```python
# Create task (replaces direct SSE generation)
POST /api/task
{
    "task_type": "media_generation",
    "payload": { ... }
}
→ {"task_id": "tq_..."}

# Get task status
GET /api/task/{task_id}
→ {"task_id": "...", "status": "...", "progress": {...}, "result": {...}}

# Stream task progress (SSE)
GET /api/task/{task_id}/stream
→ SSE events polling task status
```

### Migration Path

1. Add new task queue infrastructure
2. Add new API endpoints
3. Update `execute_media_sub_action` to enqueue task and return task_id
4. Update SSE endpoint to poll task status
5. Update WebUI to use new flow

---

## Questions for Review

1. **File structure preference**: Which option (A, B, C, or D) do you prefer? This affects the refactoring scope.
<!--i'm leaning towards D, as it feels clear. and server can see providers and access methods like get_preview_info directly, while worker interacts with other ones. -->

2. **Queue naming strategy**: For future concurrency, should queue_name be:
   - Provider-based: `media:midjourney`, `media:leonardo`
   - Task-type based: `media_generation`, `video_processing`
   - Both: `media_generation:midjourney`
<!--i am leaning towards using existing registry, and with registery allowing to add max concurrency. but we have to make sure that existing prop based logic still works as well. i dont want to hardcode this inside worker, we need to defer it to something inside media folder, but again now we have the question of worker seeing registry of the media folder, and in roudabout way, it directly see providers. may be be need that class mentioned above provide the concurrency for the task. for example, lets say worker has its own registry for differnt "actors", there's class inside worker for each actor, for example for media, there's workers/actors/media.py. media.py works as interface between worker and provider. media.py register itself as an actor for the worker. (the task db entry will have actor field). when there's a task, worker will look for the actor and it show the payload and will ask what concurrency would supported for the given payload, media actor will return something like concurrency_identifier and concurrency fields, task will update concurrency_identifier (if its already there in the task), check for running tasks with identifier, if nothing, start immediately, if tasks count == concurrency, leave it in queue.

just to note media actor will use registry inside media providers to retrieve each provider, get concurrency (we can update existing registry to registter concurrency as well), and run methods of each provider.

i'm not sure how complex above implementation is, but it feels cleaner.
-->

3. **Task retention**: How long should completed/failed tasks remain in DB?
   - Delete after N days
   - Keep forever (audit trail)
   - Move to archive collection
<!-- #2 -->

4. **Progress granularity**: Current providers report progress as elapsed_ms + message. Should we:
   - Keep as-is (simple)
   - Add percent estimation based on typical duration
   - Let each provider report its own percent
<!-- #1 -->

5. **Error detail level**: How much error detail should be stored in task.error?
   - Simple message only
   - Full stack trace
   - Structured error (type, message, details)
<!-- #2+#3 -->

6. **WebUI changes**: The WebUI currently expects SSE events from generation. New flow will be:
   - POST to create task → get task_id
   - SSE to poll task status
   - Should the API maintain backward compatibility, or is breaking change acceptable?
<!-- #2, breaking changes are okay -->
