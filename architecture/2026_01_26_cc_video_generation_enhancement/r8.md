# CC Workflow Step 4: Video Generation Enhancement

## Summary

Enhance CC workflow by adding image analysis as a new step after image
generation, and enhancing video generation with motion element selection and
actual video generation.

## Step Restructure

```
1. user_input
2. scene_generation
3. image_prompts
4. image_analysis      ← NEW STEP
5. video_generation    ← was video_prompts (renamed + enhanced)
6. text_overlays
7. titles_social
8. music
9. workflow_summary
```

---

## Decisions Summary

| Item | Decision |
|------|----------|
| Image Analysis | New step 4 (no review UI) |
| Video Providers | Leonardo + Sora 2 |
| Source image access | Server reads from stored `_resolved_inputs` |
| Client display | Source image + video preview below prompts |
| Sora 2 | Needs implementation |

---

## Source Image for img2vid - Implementation

### Approach: Server-Side Access + Client-Side Display

For img2vid sub-actions:
- **Server**: Reads `source_image` from stored `_resolved_inputs` (no round-trip)
- **Client**: Receives `source_image` in `display_data` for preview display

### Data Flow

```
Workflow step.json
├── "source_image": "{{ state.selected_image_data }}"
│
↓ Resolver resolves Jinja2

executor.py (execute_step_modules)
├── resolved_inputs = { ..., source_image: {...} }
├── Stores interaction event with _resolved_inputs
│
↓

media.generate module (get_interaction_request)
├── source_image = inputs.get('source_image')
├── Returns InteractionRequest with display_data.source_image
│
↓ SSE stream to client

WebUI MediaGeneration component
├── Receives display_data.source_image
├── Displays source image in preview area (below prompts)
├── On "Generate" click, calls POST /workflow/{id}/sub-action
│   (does NOT send source_image back - server has it)
│
↓ API endpoint

streaming.py (execute_sub_action)
├── Looks up interaction_event (already does this)
├── Reads source_image from interaction_event.data._resolved_inputs
├── Includes in task payload
│
↓ Task queue

sub_action.py (execute_media_sub_action)
├── source_image = request.params.get("source_image", "")
└── provider.img2vid(source_image, prompt, params, ...)
```

### Step 5 step.json (video_generation)

```json
{
  "module_id": "media.generate",
  "inputs": {
    "resolver_schema": {
      "type": "object",
      "properties": {
        "prompts": { "resolver": "server" },
        "source_image": { "resolver": "server" }
      }
    },
    "prompts": "{{ state.video_prompts.prompts }}",
    "schema": {
      "$ref": "schemas/cc_video_generation_display_schema.json",
      "type": "json"
    },
    "title": "Generate and Select Videos",
    "source_image": "{{ state.selected_image_data }}"
  },
  ...
}
```

### Backend Changes

#### streaming.py (execute_sub_action)

Read `source_image` from stored `_resolved_inputs` for img2vid actions:

```python
@router.post("/{workflow_run_id}/sub-action")
async def execute_sub_action(...):
    ...
    interaction_event = db.events.find_one({...})

    # For img2vid, get source_image from stored resolved inputs
    resolved_inputs = interaction_event.get("data", {}).get("_resolved_inputs", {})

    params = request.params.copy()
    if request.action_type == "img2vid":
        source_image = resolved_inputs.get("source_image")
        if source_image:
            params["source_image"] = source_image

    task_id = queue.enqueue(
        actor="media",
        payload={
            ...
            "params": params,  # Now includes source_image from server
            ...
        }
    )
```

#### media/generate.py (get_interaction_request)

Pass `source_image` to WebUI for display:

```python
def get_interaction_request(self, inputs, context):
    prompts = inputs.get('prompts', {})
    schema = inputs.get('schema', {})
    title = self.get_input_value(inputs, 'title')

    # Source image for img2vid - for client display
    source_image = inputs.get('source_image')

    sub_actions = getattr(context, 'sub_actions', None)
    retryable = getattr(context, 'retryable', None)

    return InteractionRequest(
        ...
        display_data={
            "data": prompts,
            "schema": schema,
            "sub_actions": sub_actions,
            "source_image": source_image,  # For preview display
            "generations": {},
            "retryable": retryable
        },
        ...
    )
```

### WebUI Layout

Display source image and generated video below prompt controls:

```
┌─────────────────────────────────────────────────────────┐
│  Generate and Select Videos                             │
├─────────────────────────────────────────────────────────┤
│                                                         │
│  [Leonardo] [Sora 2]                                    │
│  ┌─────────────────────────────────────────────────┐   │
│  │ Prompt: Gentle shimmer on sunlit curtains...    │   │
│  │ Negative: morphing, deformation, extra limbs... │   │
│  │ Motion Strength: [=====>----] 5                 │   │
│  │                                                 │   │
│  │ [Generate Videos]                               │   │
│  └─────────────────────────────────────────────────┘   │
│                                                         │
│  Preview                                                │
│  ┌──────────────────┐    ┌──────────────────┐          │
│  │                  │    │                  │          │
│  │  Source Image    │    │  Generated Video │          │
│  │                  │    │  (after generate)│          │
│  │                  │    │                  │          │
│  └──────────────────┘    └──────────────────┘          │
│                                                         │
└─────────────────────────────────────────────────────────┘
```

### Why This Approach

1. **No unnecessary round-trip** - Server already has source_image stored
2. **Client displays source for context** - User sees what image video is based on
3. **Clean separation** - Display data for UI, _resolved_inputs for processing
4. **Leverages existing infrastructure** - _resolved_inputs already stored

---

## Step 4: Image Analysis (NEW)

**Folder:** `workflows/cc/steps/4_image_analysis/`

### step.json

```json
{
  "step_id": "image_analysis",
  "name": "Step {step_number}: Image Analysis",
  "description": "Analyze selected image for motion elements and audio atmosphere",
  "modules": [
    {
      "module_id": "api.llm",
      "inputs": {
        "resolver_schema": {
          "type": "object",
          "properties": {
            "input": { "resolver": "server" },
            "system": { "resolver": "server" },
            "metadata": { "resolver": "server" }
          }
        },
        "input": [
          {
            "content": "{{ state.selected_image_data.local_path }}",
            "type": "image"
          },
          { "$ref": "prompts/cc_image_analysis_user.txt", "type": "text" }
        ],
        "output_schema": {
          "$ref": "schemas/cc_image_analysis_schema.json",
          "type": "json"
        },
        "metadata": {
          "step_id": "{{ step.step_id }}"
        },
        "system": [
          {
            "$ref": "prompts/cc_role_visual_analyst.txt",
            "type": "text",
            "cache_ttl": 10800
          },
          {
            "$ref": "prompts/cc_image_analysis_system.txt",
            "type": "text",
            "cache_ttl": 10800
          }
        ],
        "provider": "openai"
      },
      "outputs_to_state": {
        "response": "image_analysis"
      },
      "name": "analyze_image"
    }
  ]
}
```

### Prompts

#### cc_role_visual_analyst.txt

```
You are a visual analyst specializing in image-to-video production. Your role
is to analyze static images and identify elements suitable for subtle animation
in cute pet videos.

You have expertise in:
- Identifying animatable environmental elements (light, particles, fabric)
- Understanding motion physics for realistic subtle animation
- Recognizing atmospheric elements that enhance cozy aesthetics
- Analyzing audio characteristics implied by visual scenes

Your analysis directly informs video generation prompts and music selection,
so accuracy and specificity are critical.
```

#### cc_image_analysis_system.txt

```
# IMAGE ANALYSIS FOR PET VIDEO PRODUCTION

Analyze the provided image to extract environment details, motion elements, and
audio atmosphere for a cute pet video production.

## PART 1: ENVIRONMENT ANALYSIS

Extract scene details for narrative and caption generation.

**Identify:**
- Setting: Brief, specific description of the scene
- Time of day: dawn, morning, noon, afternoon, dusk, night
- Weather: If visible or implied (sunny, cloudy, rain, etc.)
- Location type: indoor, outdoor, specific venue type
- Key subjects: 3-6 main elements visible (include the pet!)
- Dominant colors: 3-5 colors defining the palette
- Atmosphere: The mood conveyed by the visuals
- Lighting: How light behaves in the scene

**Rules:**
- Only describe what is VISIBLE
- Be specific (e.g., "sunny living room with large windows" not "indoor")
- For key_subjects, prioritize the pet and their immediate environment

## PART 2: MOTION ELEMENT ANALYSIS

Identify elements for subtle animation suitable for short pet videos.

**Look for:**
- Light effects (sunbeams, lamp glow, reflections)
- Particles (dust motes, floating fur/hair)
- Fabric (pet bed, blankets, curtains)
- Environmental (plants swaying, water in bowl)
- Pet-adjacent elements (toys, food bowl steam)

**Constraints:**
- The PET must remain STATIC - do not suggest animating the pet itself
- Camera must remain STATIC - no zoom, pan, or movement
- Focus on SUBTLE, LOOPING motion suitable for 5-10 second clips
- Only identify elements CLEARLY VISIBLE in the image

**detected_elements (3-8 items):**
Best candidates for animation. For each provide:
- element: What could be animated
- motion_type: Type of motion (e.g., "gentle sway", "shimmer")
- location: Where in the frame

**all_potential_elements (5-20 items):**
Complete list of everything that COULD move (for negative prompts).
Include detected elements plus any others that might drift unintentionally.

## PART 3: AUDIO ATMOSPHERE ANALYSIS

Analyze for music generation context. This data will be used by the music
generation step to create appropriate background music.

**Identify:**
- implied_sounds: 2-5 sounds the scene suggests
- silence_level: very quiet / quiet / moderate / busy / loud
- emotional_intensity: serene / calm / neutral / tense / dramatic / intense
- temporal_feel: frozen/still / slow/meditative / gentle/flowing / moderate
- warmth: cold/clinical / cool / neutral / warm / very warm/intimate
- era_feel: Time period evoked (e.g., "cozy modern home")
- cultural_context: Geographic/cultural context if apparent
- suggested_music_genres: 2-3 genres that would complement
- musical_texture: Suggested texture (e.g., "warm and gentle")
```

#### cc_image_analysis_user.txt

```
Analyze this image of a pet scene and extract:
1. Environment details (setting, time, weather, location, subjects, colors,
   atmosphere, lighting)
2. Motion elements for animation (detected candidates and all potential
   elements)
3. Audio atmosphere (implied sounds, emotional qualities, music suggestions)

Focus on elements that would enhance a cute, heartwarming pet video.
```

### Schemas

#### cc_image_analysis_schema.json

```json
{
  "type": "object",
  "properties": {
    "environment": {
      "type": "object",
      "properties": {
        "setting": { "type": "string" },
        "time_of_day": { "type": "string" },
        "weather": { "type": "string" },
        "location_type": { "type": "string" },
        "key_subjects": {
          "type": "array",
          "items": { "type": "string" },
          "minItems": 3,
          "maxItems": 6
        },
        "dominant_colors": {
          "type": "array",
          "items": { "type": "string" },
          "minItems": 3,
          "maxItems": 5
        },
        "atmosphere": { "type": "string" },
        "lighting": { "type": "string" }
      },
      "required": ["setting", "time_of_day", "weather", "location_type",
                   "key_subjects", "dominant_colors", "atmosphere", "lighting"],
      "additionalProperties": false
    },
    "motion_elements": {
      "type": "object",
      "properties": {
        "detected_elements": {
          "type": "array",
          "items": {
            "type": "object",
            "properties": {
              "element": { "type": "string" },
              "motion_type": { "type": "string" },
              "location": { "type": "string" }
            },
            "required": ["element", "motion_type", "location"],
            "additionalProperties": false
          },
          "minItems": 3,
          "maxItems": 8
        },
        "all_potential_elements": {
          "type": "array",
          "items": { "type": "string" },
          "minItems": 5,
          "maxItems": 20
        }
      },
      "required": ["detected_elements", "all_potential_elements"],
      "additionalProperties": false
    },
    "audio_atmosphere": {
      "type": "object",
      "properties": {
        "implied_sounds": {
          "type": "array",
          "items": { "type": "string" },
          "minItems": 2,
          "maxItems": 5
        },
        "silence_level": {
          "type": "string",
          "enum": ["very quiet", "quiet", "moderate", "busy", "loud"]
        },
        "emotional_intensity": {
          "type": "string",
          "enum": ["serene", "calm", "neutral", "tense", "dramatic", "intense"]
        },
        "temporal_feel": {
          "type": "string",
          "enum": ["frozen/still", "slow/meditative", "gentle/flowing",
                   "moderate/steady", "dynamic/energetic", "urgent/fast"]
        },
        "warmth": {
          "type": "string",
          "enum": ["cold/clinical", "cool", "neutral", "warm",
                   "very warm/intimate"]
        },
        "era_feel": { "type": "string" },
        "cultural_context": { "type": "string" },
        "suggested_music_genres": {
          "type": "array",
          "items": { "type": "string" },
          "minItems": 2,
          "maxItems": 3
        },
        "musical_texture": { "type": "string" }
      },
      "required": ["implied_sounds", "silence_level", "emotional_intensity",
                   "temporal_feel", "warmth", "era_feel", "cultural_context",
                   "suggested_music_genres", "musical_texture"],
      "additionalProperties": false
    }
  },
  "required": ["environment", "motion_elements", "audio_atmosphere"],
  "additionalProperties": false
}
```

---

## Step 5: Video Generation (ENHANCED)

**Folder:** `workflows/cc/steps/5_video_generation/`

### step.json

```json
{
  "step_id": "video_generation",
  "name": "Step {step_number}: Video Generation",
  "description": "Select motion elements, generate video prompts, and create videos",
  "modules": [
    {
      "module_id": "user.select",
      "inputs": {
        "resolver_schema": {
          "type": "object",
          "properties": {
            "data": { "resolver": "server" }
          }
        },
        "data": {
          "detected": "{{ state.image_analysis.motion_elements.detected_elements }}"
        },
        "schema": {
          "$ref": "schemas/cc_motion_elements_display_schema.json",
          "type": "json"
        },
        "prompt": "Select motion elements to animate in your video",
        "multi_select": true,
        "mode": "select"
      },
      "outputs_to_state": {
        "selected_indices": "selected_motion_element_indices",
        "selected_data": "selected_motion_elements"
      },
      "name": "select_motion_elements"
    },
    {
      "module_id": "api.llm",
      "inputs": {
        "resolver_schema": {
          "type": "object",
          "properties": {
            "input": { "resolver": "server" },
            "system": { "resolver": "server" },
            "metadata": { "resolver": "server" }
          }
        },
        "input": {
          "$ref": "prompts/cc_video_prompts_user.txt",
          "type": "jinja2"
        },
        "output_schema": {
          "$ref": "schemas/cc_video_prompts_schema.json",
          "type": "json"
        },
        "metadata": {
          "step_id": "{{ step.step_id }}"
        },
        "system": [
          {
            "$ref": "prompts/cc_role_motion_engineer.txt",
            "type": "text",
            "cache_ttl": 10800
          },
          {
            "$ref": "prompts/cc_video_prompts_system.txt",
            "type": "text",
            "cache_ttl": 10800
          }
        ],
        "provider": "openai"
      },
      "outputs_to_state": {
        "response": "video_prompts"
      },
      "name": "generate_video_prompts"
    },
    {
      "module_id": "media.generate",
      "inputs": {
        "resolver_schema": {
          "type": "object",
          "properties": {
            "prompts": { "resolver": "server" },
            "source_image": { "resolver": "server" }
          }
        },
        "prompts": "{{ state.video_prompts.prompts }}",
        "schema": {
          "$ref": "schemas/cc_video_generation_display_schema.json",
          "type": "json"
        },
        "title": "Generate and Select Videos",
        "source_image": "{{ state.selected_image_data }}"
      },
      "outputs_to_state": {
        "selected_content_id": "selected_video_id",
        "selected_content": "selected_video_data",
        "generations": "video_generations"
      },
      "sub_actions": [
        {
          "id": "generate",
          "label": "Generate Videos",
          "action_type": "img2vid",
          "loading_label": "Generating video...",
          "result_key": "generations"
        }
      ],
      "retryable": {
        "default_option": "continue",
        "options": [
          {
            "id": "continue",
            "mode": "continue",
            "label": "Accept and continue"
          },
          {
            "id": "retry_prompts",
            "mode": "retry",
            "label": "Regenerate video prompts",
            "shortcut": "r",
            "target_module": "generate_video_prompts",
            "feedback": {
              "enabled": true,
              "prompt": "What should be different?",
              "default_message": "Please generate different video prompts."
            }
          },
          {
            "id": "change_motion_elements",
            "mode": "jump",
            "label": "Change motion elements",
            "target_step": "video_generation",
            "target_module": "select_motion_elements"
          },
          {
            "id": "change_image",
            "mode": "jump",
            "label": "Go back to image selection",
            "target_step": "image_prompts",
            "target_module": "generate_and_select_images"
          }
        ]
      },
      "name": "generate_and_select_videos"
    }
  ]
}
```

### Prompts

#### cc_video_prompts_system.txt

```
Generate video/motion prompts for the selected scene. Create prompts optimized
for Leonardo Motion and Sora 2.

## Context
You have:
- A scene concept with narrative and visual moments
- Image analysis with detected motion elements
- User-selected elements to animate
- Elements to block in negative prompts

## CRITICAL: Use Selected Motion Elements

The user has specifically selected which elements to animate. Your prompts
must prioritize including animation of these selected elements. All other
potentially movable elements should be included in the negative prompt to
keep them static.

## Critical: Negative Prompts
Negative prompts are ESSENTIAL for quality video generation. They prevent:
- Morphing/deformation artifacts
- Unnatural movement
- Flickering and inconsistency
- Anatomical errors during motion

### Standard Negative Elements
Always include in negative prompts:
- morphing, deformation, distortion
- extra limbs, missing limbs, fused limbs
- blurry, out of focus, low quality
- flickering, strobing, jittering
- unnatural movement, robotic motion
- face distortion, eye glitches
- sudden scene changes, jump cuts

### Pet-Specific Negatives
- extra legs, missing legs, leg fusion
- tail splitting, multiple tails
- ear morphing, floating ears
- eye color change, pupil distortion
- fur texture changing, bald patches appearing
- body stretching, size fluctuation

### Block Unselected Motion Elements
Add ALL elements from `all_potential_elements` that are NOT in the user's
selected list to the negative prompt.

## Model-Specific Guidelines

### Leonardo Motion
**Docs:** https://docs.leonardo.ai/reference/createimagetovideogeneration

**Positive Prompt:** Focus on selected motion elements, describe motion type
**Negative Prompt:** Unselected elements + standard artifacts

### Sora 2
**Docs:** https://platform.openai.com/docs/guides/video-generation

**Prompt:** Natural language description with motion focus
**Style Notes:** Visual consistency, motion intensity, lighting preservation
```

#### cc_video_prompts_user.txt

```
Generate video/motion prompts for this pet scene:

## Scene Details

**Title:** {{ state.selected_scene.title }}
**Narrative:** {{ state.selected_scene.narrative }}
**Pet Behavior Focus:** {{ state.selected_scene.pet_behavior }}
**Setting:** {{ state.selected_scene.setting }}
**Emotional Arc:** {{ state.selected_scene.emotional_arc }}

## Context

**Pet Type:** {{ state.pet_type_selection.label }}
**Visual Tone:** {{ state.aesthetic_selection.visual_tone.mood }}
**Duration Target:** {{ state.duration_selection.video_duration }} seconds

## Image Analysis

**Environment:**
- Setting: {{ state.image_analysis.environment.setting }}
- Atmosphere: {{ state.image_analysis.environment.atmosphere }}
- Lighting: {{ state.image_analysis.environment.lighting }}

## IMPORTANT: Selected Motion Elements

The user has specifically chosen these elements to animate. Your prompts must
prioritize including animation of following elements:

{% for element in state.selected_motion_elements %}
- **{{ element.element }}**: {{ element.motion_type }} ({{ element.location }})
{% endfor %}

## Elements to Block in Negative Prompts

All of these elements should be kept STATIC:
{{ state.image_analysis.motion_elements.all_potential_elements | join(', ') }}

Generate Leonardo Motion and Sora 2 video prompts with natural, subtle motion.
The pet and all unselected elements must remain static.
```

### Schemas

#### cc_motion_elements_display_schema.json

```json
{
  "type": "object",
  "_ux.display": "passthrough",
  "properties": {
    "detected": {
      "type": "array",
      "_ux": {
        "display": "visible",
        "display_label": "Detected Motion Elements",
        "render_as": "content-panel"
      },
      "items": {
        "type": "object",
        "_ux": {
          "display": "visible",
          "render_as": "card",
          "nudges": ["index-badge"],
          "selectable": true
        },
        "properties": {
          "element": {
            "type": "string",
            "_ux": {
              "display": true,
              "display_label": "Element",
              "render_as": "card-title",
              "highlight": true,
              "highlight_color": "#4ECDC4"
            }
          },
          "motion_type": {
            "type": "string",
            "_ux.display": true,
            "_ux.display_label": "Motion Type"
          },
          "location": {
            "type": "string",
            "_ux.display": true,
            "_ux.display_label": "Location"
          }
        }
      }
    }
  }
}
```

#### cc_video_prompts_schema.json

```json
{
  "type": "object",
  "properties": {
    "scene_title": { "type": "string" },
    "motion_summary": { "type": "string" },
    "prompts": {
      "type": "object",
      "properties": {
        "leonardo": {
          "type": "object",
          "properties": {
            "positive_prompt": { "type": "string" },
            "negative_prompt": { "type": "string" },
            "motion_strength": {
              "type": "string",
              "enum": ["subtle", "moderate", "dynamic"]
            },
            "motion_notes": { "type": "string" }
          },
          "required": ["positive_prompt", "negative_prompt", "motion_strength",
                       "motion_notes"],
          "additionalProperties": false
        },
        "sora": {
          "type": "object",
          "properties": {
            "prompt": { "type": "string" },
            "style_notes": { "type": "string" }
          },
          "required": ["prompt", "style_notes"],
          "additionalProperties": false
        }
      },
      "required": ["leonardo", "sora"],
      "additionalProperties": false
    }
  },
  "required": ["scene_title", "motion_summary", "prompts"],
  "additionalProperties": false
}
```

#### cc_video_generation_display_schema.json

```json
{
  "type": "object",
  "_ux.display": "passthrough",
  "properties": {
    "leonardo": {
      "type": "object",
      "_ux": {
        "display": "visible",
        "tab_label": "Leonardo",
        "render_as": "tab.media",
        "provider": "leonardo",
        "input_schema": {
          "type": "object",
          "_ux": {
            "layout": "grid",
            "layout_columns": 3,
            "layout_columns_sm": 2
          },
          "properties": {
            "_text": {
              "type": "string",
              "title": "Prompt",
              "destination_field": "prompt",
              "_ux": {
                "input_type": "textarea",
                "col_span": "full",
                "rows": 4,
                "source_field": "positive_prompt"
              }
            },
            "negative_prompt": {
              "type": "string",
              "title": "Negative Prompt",
              "_ux": {
                "input_type": "textarea",
                "col_span": "full",
                "rows": 2,
                "source_field": "negative_prompt"
              }
            },
            "motion_strength": {
              "type": "integer",
              "title": "Motion Strength",
              "minimum": 1,
              "maximum": 10,
              "default": 5,
              "step": 1,
              "_ux": { "input_type": "slider" }
            }
          }
        }
      },
      "properties": {
        "positive_prompt": { "type": "string", "_ux.display": false },
        "negative_prompt": { "type": "string", "_ux.display": false },
        "motion_strength": {
          "type": "string",
          "_ux.display": true,
          "_ux.display_label": "Motion"
        },
        "motion_notes": {
          "type": "string",
          "_ux.display": true,
          "_ux.display_label": "Notes",
          "_ux.highlight": true,
          "_ux.highlight_color": "#7B68EE"
        }
      }
    },
    "sora": {
      "type": "object",
      "_ux": {
        "display": "visible",
        "tab_label": "Sora 2",
        "render_as": "tab.media",
        "provider": "openai",
        "input_schema": {
          "type": "object",
          "_ux": {
            "layout": "grid",
            "layout_columns": 3,
            "layout_columns_sm": 2
          },
          "properties": {
            "_text": {
              "type": "string",
              "title": "Prompt",
              "destination_field": "prompt",
              "_ux": {
                "input_type": "textarea",
                "col_span": "full",
                "rows": 4,
                "source_field": "prompt"
              }
            },
            "duration": {
              "type": "string",
              "title": "Duration",
              "enum": ["5", "10", "15", "20"],
              "default": "5",
              "enum_labels": {
                "5": "5 seconds",
                "10": "10 seconds",
                "15": "15 seconds",
                "20": "20 seconds"
              },
              "_ux": { "input_type": "select" }
            },
            "aspect_ratio": {
              "type": "string",
              "title": "Aspect Ratio",
              "enum": ["16:9", "9:16", "1:1"],
              "default": "9:16",
              "_ux": { "input_type": "select" }
            },
            "quality": {
              "type": "string",
              "title": "Quality",
              "enum": ["720p", "1080p"],
              "default": "720p",
              "_ux": { "input_type": "select" }
            }
          }
        }
      },
      "properties": {
        "prompt": { "type": "string", "_ux.display": false },
        "style_notes": {
          "type": "string",
          "_ux.display": true,
          "_ux.display_label": "Style",
          "_ux.highlight": true,
          "_ux.highlight_color": "#4ECDC4"
        }
      }
    }
  }
}
```

---

## Files to Create/Modify

### New Step: `4_image_analysis/`

| File | Purpose |
|------|---------|
| `step.json` | Step definition |
| `prompts/cc_role_visual_analyst.txt` | Role prompt |
| `prompts/cc_image_analysis_system.txt` | System prompt |
| `prompts/cc_image_analysis_user.txt` | User prompt |
| `schemas/cc_image_analysis_schema.json` | LLM output schema |

### Modified: `5_video_generation/`

| File | Change |
|------|--------|
| `step.json` | Rename, add modules, add source_image input |
| `prompts/cc_video_prompts_system.txt` | Leonardo + Sora |
| `prompts/cc_video_prompts_user.txt` | Motion elements |
| `schemas/cc_video_prompts_schema.json` | New structure |
| `schemas/cc_motion_elements_display_schema.json` | NEW |
| `schemas/cc_video_generation_display_schema.json` | NEW |

### Renamed Folders

| Old | New |
|-----|-----|
| `5_text_overlays/` | `6_text_overlays/` |
| `6_titles_social/` | `7_titles_social/` |
| `7_music/` | `8_music/` |
| `8_workflow_summary/` | `9_workflow_summary/` |

### Backend Files

| File | Change |
|------|--------|
| `backend/server/api/routes/streaming.py` | Read source_image from _resolved_inputs for img2vid |
| `backend/server/modules/media/generate.py` | Pass source_image to display_data |
| `backend/providers/media/openai/provider.py` | Implement Sora 2 img2vid |

---

## Implementation Notes

### Selected Image local_path

**Resolved:** Added DB query in `media/generate.py` `execute_with_response()`
to enrich `selected_content` with `local_path` from content repository.

### Leonardo img2vid - Source Image Upload

**Issue:** Leonardo img2vid API expects a Leonardo image ID, not a file path.

**Solution:** The Leonardo provider's `img2vid` method should automatically:
1. Detect if `source_image` is a local path (not a Leonardo ID)
2. Upload the image to Leonardo using their upload API
3. Use the resulting Leonardo image ID for video generation

This upload should happen transparently within the `img2vid` method.

**To implement in Phase 4.**

### Sora 2 Provider

OpenAI img2vid currently raises NotImplementedError. Sora 2 needs separate
implementation.

**Docs:** https://platform.openai.com/docs/guides/video-generation

**To address in Phase 5.**

---

## Plan of Action (POA)

Implementation ordered by dependencies. Each task marked with status.

### Phase 1: Workflow Structure Changes

**1.1 Rename step folders (5→6, 6→7, 7→8, 8→9)** ✓
- [x] Rename `5_text_overlays/` → `6_text_overlays/`
- [x] Rename `6_titles_social/` → `7_titles_social/`
- [x] Rename `7_music/` → `8_music/`
- [x] Rename `8_workflow_summary/` → `9_workflow_summary/`
- [x] Rename `4_video_prompts/` → `5_video_generation/`
- [x] Update workflow_v3.json with new folder paths and add step 4 reference

**1.2 Create new step 4: image_analysis** ✓
- [x] Create folder `workflows/cc/steps/4_image_analysis/`
- [x] Create `step.json` (api.llm module, no interaction)
- [x] Create `prompts/` folder
  - [x] `cc_role_visual_analyst.txt`
  - [x] `cc_image_analysis_system.txt`
  - [x] `cc_image_analysis_user.txt`
- [x] Create `schemas/` folder
  - [x] `cc_image_analysis_schema.json`

**1.3 Transform step 4 → step 5: video_generation** ✓
- [x] Rename folder `4_video_prompts/` → `5_video_generation/`
- [x] Rewrite `step.json` with 3 modules:
  - `user.select` (motion element selection)
  - `api.llm` (video prompt generation)
  - `media.generate` (video generation with img2vid)
- [x] Create/update prompts:
  - [x] `cc_role_motion_engineer.txt` (existing, kept)
  - [x] `cc_video_prompts_system.txt` (updated for Leonardo + Sora)
  - [x] `cc_video_prompts_user.txt` (updated with motion elements)
- [x] Create/update schemas:
  - [x] `cc_motion_elements_display_schema.json` (new)
  - [x] `cc_video_prompts_schema.json` (updated for Leonardo + Sora)
  - [x] `cc_video_generation_display_schema.json` (new)
- [x] Remove old `cc_video_prompts_display_schema.json`

### Phase 2: Backend - Source Image Pipeline

**2.1 Fix selected_content to include local_path** ✓
- [x] Investigate how `selected_content` is populated
  - Currently: `{content_id, url, metadata_id, prompt_key}`
  - Need: Add `local_path` from database
- [x] Query DB in `execute_with_response()` to get local_path
- [ ] Test: Verify `state.selected_image_data.local_path` is accessible

**2.2 Update media/generate.py** ✓
- [x] Add `source_image` to module inputs (reads from inputs if present)
- [x] Pass `source_image` to `display_data` in `get_interaction_request()`
- [ ] Test: Verify interaction request includes source_image

**2.3 Update worker/actors/media.py for img2vid** ✓
- [x] Add `_get_source_image_from_interaction()` helper
- [x] Look up source_image from interaction event's `_resolved_inputs`
- [ ] Test: Verify sub-action receives source_image without client round-trip

### Phase 3: WebUI - Video Generation UI

**3.1 Schema-driven source_image display** ✓
- [x] Include `_source_image` in display_data.data (generate.py)
- [x] Update display schema with `_source_image.url` render_as: "image"
- [x] Use passthrough display to render image automatically
- [ ] Test: Verify source image renders in UI

**3.2 Ensure img2vid sub-action works**
- [ ] Verify sub-action triggers correctly with `action_type: "img2vid"`
- [ ] Test video generation flow end-to-end

### Phase 4: Leonardo img2vid Provider

**4.1 Implement Leonardo img2vid with auto-upload** ✓
- [x] Add `_upload_init_image()` method for presigned URL upload
- [x] Update `img2vid()` to accept dict with local_path
- [x] Automatically upload local images before video generation
- [x] Set imageType="UPLOADED" when using uploaded images
- [ ] Test: Generate video from local image with Leonardo

### Phase 5: Sora 2 Provider

**5.1 Implement OpenAI Sora 2 img2vid** ✓
- [x] Read Sora 2 API documentation
- [x] Add `img2vid` method to OpenAI provider
- [x] Handle Sora-specific parameters (duration, aspect_ratio, quality)
- [x] Support both 720p and 1080p via (aspect_ratio, quality) → size mapping
- [x] Add quality selector to display schema
- [ ] Test: Generate video from image with Sora 2

### Phase 6: Integration Testing

**6.1 End-to-end workflow test**
- [ ] Run CC workflow from step 1 through step 5
- [ ] Verify image analysis runs without UI pause
- [ ] Verify motion element selection works
- [ ] Verify video prompts generate correctly
- [ ] Verify img2vid generation works
- [ ] Verify source image displays alongside video

---

## Progress Log

| Date | Task | Status | Notes |
|------|------|--------|-------|
| 2026-01-26 | POA created | ✓ | |
| 2026-01-26 | Phase 1.1: Rename step folders | ✓ | 5→6, 6→7, 7→8, 8→9, 4→5 |
| 2026-01-26 | Update workflow_v3.json refs | ✓ | Added step 4 ref |
| 2026-01-26 | Phase 1.2: Create step 4 image_analysis | ✓ | step.json, prompts, schema |
| 2026-01-26 | Phase 1.3: Transform step 5 video_generation | ✓ | 3 modules, new schemas |
| 2026-01-26 | Phase 2: Backend source_image pipeline | ✓ | local_path, worker lookup |
| 2026-01-26 | Phase 3: Schema-driven source_image display | ✓ | No custom components |
| 2026-01-26 | Phase 4: Leonardo img2vid auto-upload | ✓ | Presigned URL flow |
| 2026-01-26 | Phase 5: Sora 2 img2vid provider | ✓ | 720p/1080p, async polling |
| | | | |
