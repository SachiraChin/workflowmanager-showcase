# CC Workflow Step 4: Video Generation Enhancement

## Summary

Enhance CC workflow by adding image analysis as a new step after image
generation, and enhancing video generation with motion element selection and
actual video generation.

## Step Restructure

```
1. user_input
2. scene_generation
3. image_prompts
4. image_analysis      ← NEW STEP
5. video_generation    ← was video_prompts (renamed + enhanced)
6. text_overlays
7. titles_social
8. music
9. workflow_summary
```

---

## Decisions Summary

| Item | Decision |
|------|----------|
| Image Analysis | New step 4 (no review UI) |
| Video Providers | Leonardo + Sora 2 |
| Interaction data access | Add current module's interaction to context |
| Sora 2 | Needs implementation |

---

## Source Image for img2vid - Implementation

### Approach: Add Current Module's Interaction to Context

Modify `backend/server/workflow/interaction.py` to add the current module's
interaction data to context. This gives the module access to its own
`_resolved_inputs` and other interaction data.

### Implementation in interaction.py

<!--the execute_with_response already have input param which has input data
which cotaains the original selected image-->

The `last_interaction` variable (line 62-66) already contains the current
module's interaction data including `_resolved_inputs`. Add this to context:

```python
# Around line 144, after creating context
context.current_module_name = module_name
context.current_module_index = module_index
context.step_id = step_id
context.retryable = module_config.get('retryable')
context.sub_actions = module_config.get('sub_actions')
context.cancel_event = cancel_event

# ADD: Current module's interaction data
# last_interaction contains this module's interaction_requested event
# including _resolved_inputs, interaction_id, interaction_type, etc.
context.current_interaction = last_interaction.get('data', {})
```

### What's Available in context.current_interaction

From the `interaction_requested` event:

```python
context.current_interaction = {
    'interaction_id': 'media_xxx',
    'interaction_type': 'media_generation',
    '_resolved_inputs': {
        'prompts': {...},
        'schema': {...},
        'title': '...'
    },
    # ... other interaction data
}
```

### For Source Image: Use State

For step 5's `media.generate` to access step 3's selected image, the data is
already in state via `outputs_to_state`:

- Step 3 outputs `selected_image_data` to state
- Step 5 can read from `context.module_outputs.get('selected_image_data')`

The `context.current_interaction` is useful for accessing the current module's
own resolved inputs, not for cross-step data access.

### How media.generate Uses It

```python
def get_interaction_request(self, inputs, context):
    prompts = inputs.get('prompts', {})
    schema = inputs.get('schema', {})
    title = self.get_input_value(inputs, 'title')

    sub_actions = getattr(context, 'sub_actions', None)
    retryable = getattr(context, 'retryable', None)

    # For img2vid, get source image from state (step 3 output)
    source_image = None
    if sub_actions:
        for action in sub_actions:
            if action.get('action_type') in ('img2vid', 'img2img'):
                # Access from state - step 3 saved this via outputs_to_state
                source_image = context.module_outputs.get('selected_image_data')
                break

    return InteractionRequest(
        interaction_type=InteractionType.MEDIA_GENERATION,
        interaction_id=f"media_{uuid7_str()}",
        title=title,
        display_data={
            "data": prompts,
            "schema": schema,
            "sub_actions": sub_actions,
            "source_image": source_image,
            "generations": {},
            "retryable": retryable
        },
        context={
            "module_id": self.module_id
        }
    )
```

### Data Flow

```
Step 3: Image Generation
├── media.generate outputs via outputs_to_state:
│   └── selected_image_data → state.selected_image_data
│   └── selected_content_id → state.selected_image_id
│   └── generations → state.image_generations

Step 5: Video Generation
├── media.generate reads from context.module_outputs:
│   └── source_image = context.module_outputs.get('selected_image_data')
├── Passes to WebUI in display_data.source_image
└── WebUI includes in sub-action request params
```

---

## Step 4: Image Analysis (NEW)

**Folder:** `workflows/cc/steps/4_image_analysis/`

### step.json

```json
{
  "step_id": "image_analysis",
  "name": "Step {step_number}: Image Analysis",
  "description": "Analyze selected image for motion elements and audio atmosphere",
  "modules": [
    {
      "module_id": "api.llm",
      "inputs": {
        "resolver_schema": {
          "type": "object",
          "properties": {
            "input": { "resolver": "server" },
            "system": { "resolver": "server" },
            "metadata": { "resolver": "server" }
          }
        },
        "input": [
          {
            "content": "{{ state.selected_image_data.local_path }}",
            "type": "image"
          },
          { "$ref": "prompts/cc_image_analysis_user.txt", "type": "text" }
        ],
        "output_schema": {
          "$ref": "schemas/cc_image_analysis_schema.json",
          "type": "json"
        },
        "metadata": {
          "step_id": "{{ step.step_id }}"
        },
        "system": [
          {
            "$ref": "prompts/cc_role_visual_analyst.txt",
            "type": "text",
            "cache_ttl": 10800
          },
          {
            "$ref": "prompts/cc_image_analysis_system.txt",
            "type": "text",
            "cache_ttl": 10800
          }
        ],
        "provider": "openai"
      },
      "outputs_to_state": {
        "response": "image_analysis"
      },
      "name": "analyze_image"
    }
  ]
}
```

### Prompts

#### cc_role_visual_analyst.txt

```
You are a visual analyst specializing in image-to-video production. Your role
is to analyze static images and identify elements suitable for subtle animation
in cute pet videos.

You have expertise in:
- Identifying animatable environmental elements (light, particles, fabric)
- Understanding motion physics for realistic subtle animation
- Recognizing atmospheric elements that enhance cozy aesthetics
- Analyzing audio characteristics implied by visual scenes

Your analysis directly informs video generation prompts and music selection,
so accuracy and specificity are critical.
```

#### cc_image_analysis_system.txt

```
# IMAGE ANALYSIS FOR PET VIDEO PRODUCTION

Analyze the provided image to extract environment details, motion elements, and
audio atmosphere for a cute pet video production.

## PART 1: ENVIRONMENT ANALYSIS

Extract scene details for narrative and caption generation.

**Identify:**
- Setting: Brief, specific description of the scene
- Time of day: dawn, morning, noon, afternoon, dusk, night
- Weather: If visible or implied (sunny, cloudy, rain, etc.)
- Location type: indoor, outdoor, specific venue type
- Key subjects: 3-6 main elements visible (include the pet!)
- Dominant colors: 3-5 colors defining the palette
- Atmosphere: The mood conveyed by the visuals
- Lighting: How light behaves in the scene

**Rules:**
- Only describe what is VISIBLE
- Be specific (e.g., "sunny living room with large windows" not "indoor")
- For key_subjects, prioritize the pet and their immediate environment

## PART 2: MOTION ELEMENT ANALYSIS

Identify elements for subtle animation suitable for short pet videos.

**Look for:**
- Light effects (sunbeams, lamp glow, reflections)
- Particles (dust motes, floating fur/hair)
- Fabric (pet bed, blankets, curtains)
- Environmental (plants swaying, water in bowl)
- Pet-adjacent elements (toys, food bowl steam)

**Constraints:**
- The PET must remain STATIC - do not suggest animating the pet itself
- Camera must remain STATIC - no zoom, pan, or movement
- Focus on SUBTLE, LOOPING motion suitable for 5-10 second clips
- Only identify elements CLEARLY VISIBLE in the image

**detected_elements (3-8 items):**
Best candidates for animation. For each provide:
- element: What could be animated
- motion_type: Type of motion (e.g., "gentle sway", "shimmer")
- location: Where in the frame

**all_potential_elements (5-20 items):**
Complete list of everything that COULD move (for negative prompts).
Include detected elements plus any others that might drift unintentionally.

## PART 3: AUDIO ATMOSPHERE ANALYSIS

Analyze for music generation context. This data will be used by the music
generation step to create appropriate background music.

**Identify:**
- implied_sounds: 2-5 sounds the scene suggests
- silence_level: very quiet / quiet / moderate / busy / loud
- emotional_intensity: serene / calm / neutral / tense / dramatic / intense
- temporal_feel: frozen/still / slow/meditative / gentle/flowing / moderate
- warmth: cold/clinical / cool / neutral / warm / very warm/intimate
- era_feel: Time period evoked (e.g., "cozy modern home")
- cultural_context: Geographic/cultural context if apparent
- suggested_music_genres: 2-3 genres that would complement
- musical_texture: Suggested texture (e.g., "warm and gentle")
```

#### cc_image_analysis_user.txt

```
Analyze this image of a pet scene and extract:
1. Environment details (setting, time, weather, location, subjects, colors,
   atmosphere, lighting)
2. Motion elements for animation (detected candidates and all potential
   elements)
3. Audio atmosphere (implied sounds, emotional qualities, music suggestions)

Focus on elements that would enhance a cute, heartwarming pet video.
```

### Schemas

#### cc_image_analysis_schema.json

```json
{
  "type": "object",
  "properties": {
    "environment": {
      "type": "object",
      "properties": {
        "setting": { "type": "string" },
        "time_of_day": { "type": "string" },
        "weather": { "type": "string" },
        "location_type": { "type": "string" },
        "key_subjects": {
          "type": "array",
          "items": { "type": "string" },
          "minItems": 3,
          "maxItems": 6
        },
        "dominant_colors": {
          "type": "array",
          "items": { "type": "string" },
          "minItems": 3,
          "maxItems": 5
        },
        "atmosphere": { "type": "string" },
        "lighting": { "type": "string" }
      },
      "required": ["setting", "time_of_day", "weather", "location_type",
                   "key_subjects", "dominant_colors", "atmosphere", "lighting"],
      "additionalProperties": false
    },
    "motion_elements": {
      "type": "object",
      "properties": {
        "detected_elements": {
          "type": "array",
          "items": {
            "type": "object",
            "properties": {
              "element": { "type": "string" },
              "motion_type": { "type": "string" },
              "location": { "type": "string" }
            },
            "required": ["element", "motion_type", "location"],
            "additionalProperties": false
          },
          "minItems": 3,
          "maxItems": 8
        },
        "all_potential_elements": {
          "type": "array",
          "items": { "type": "string" },
          "minItems": 5,
          "maxItems": 20
        }
      },
      "required": ["detected_elements", "all_potential_elements"],
      "additionalProperties": false
    },
    "audio_atmosphere": {
      "type": "object",
      "properties": {
        "implied_sounds": {
          "type": "array",
          "items": { "type": "string" },
          "minItems": 2,
          "maxItems": 5
        },
        "silence_level": {
          "type": "string",
          "enum": ["very quiet", "quiet", "moderate", "busy", "loud"]
        },
        "emotional_intensity": {
          "type": "string",
          "enum": ["serene", "calm", "neutral", "tense", "dramatic", "intense"]
        },
        "temporal_feel": {
          "type": "string",
          "enum": ["frozen/still", "slow/meditative", "gentle/flowing",
                   "moderate/steady", "dynamic/energetic", "urgent/fast"]
        },
        "warmth": {
          "type": "string",
          "enum": ["cold/clinical", "cool", "neutral", "warm",
                   "very warm/intimate"]
        },
        "era_feel": { "type": "string" },
        "cultural_context": { "type": "string" },
        "suggested_music_genres": {
          "type": "array",
          "items": { "type": "string" },
          "minItems": 2,
          "maxItems": 3
        },
        "musical_texture": { "type": "string" }
      },
      "required": ["implied_sounds", "silence_level", "emotional_intensity",
                   "temporal_feel", "warmth", "era_feel", "cultural_context",
                   "suggested_music_genres", "musical_texture"],
      "additionalProperties": false
    }
  },
  "required": ["environment", "motion_elements", "audio_atmosphere"],
  "additionalProperties": false
}
```

---

## Step 5: Video Generation (ENHANCED)

**Folder:** `workflows/cc/steps/5_video_generation/`

### step.json

```json
{
  "step_id": "video_generation",
  "name": "Step {step_number}: Video Generation",
  "description": "Select motion elements, generate video prompts, and create videos",
  "modules": [
    {
      "module_id": "user.select",
      "inputs": {
        "resolver_schema": {
          "type": "object",
          "properties": {
            "data": { "resolver": "server" }
          }
        },
        "data": {
          "detected": "{{ state.image_analysis.motion_elements.detected_elements }}"
        },
        "schema": {
          "$ref": "schemas/cc_motion_elements_display_schema.json",
          "type": "json"
        },
        "prompt": "Select motion elements to animate in your video",
        "multi_select": true,
        "mode": "select"
      },
      "outputs_to_state": {
        "selected_indices": "selected_motion_element_indices",
        "selected_data": "selected_motion_elements"
      },
      "name": "select_motion_elements"
    },
    {
      "module_id": "api.llm",
      "inputs": {
        "resolver_schema": {
          "type": "object",
          "properties": {
            "input": { "resolver": "server" },
            "system": { "resolver": "server" },
            "metadata": { "resolver": "server" }
          }
        },
        "input": {
          "$ref": "prompts/cc_video_prompts_user.txt",
          "type": "jinja2"
        },
        "output_schema": {
          "$ref": "schemas/cc_video_prompts_schema.json",
          "type": "json"
        },
        "metadata": {
          "step_id": "{{ step.step_id }}"
        },
        "system": [
          {
            "$ref": "prompts/cc_role_motion_engineer.txt",
            "type": "text",
            "cache_ttl": 10800
          },
          {
            "$ref": "prompts/cc_video_prompts_system.txt",
            "type": "text",
            "cache_ttl": 10800
          }
        ],
        "provider": "openai"
      },
      "outputs_to_state": {
        "response": "video_prompts"
      },
      "name": "generate_video_prompts"
    },
    {
      "module_id": "media.generate",
      "inputs": {
        "resolver_schema": {
          "type": "object",
          "properties": {
            "prompts": { "resolver": "server" }
          }
        },
        "prompts": "{{ state.video_prompts.prompts }}",
        "schema": {
          "$ref": "schemas/cc_video_generation_display_schema.json",
          "type": "json"
        },
        "title": "Generate and Select Videos"
      },
      "outputs_to_state": {
        "selected_content_id": "selected_video_id",
        "selected_content": "selected_video_data",
        "generations": "video_generations"
      },
      "sub_actions": [
        {
          "id": "generate",
          "label": "Generate Videos",
          "action_type": "img2vid",
          "loading_label": "Generating video...",
          "result_key": "generations"
        }
      ],
      "retryable": {
        "default_option": "continue",
        "options": [
          {
            "id": "continue",
            "mode": "continue",
            "label": "Accept and continue"
          },
          {
            "id": "retry_prompts",
            "mode": "retry",
            "label": "Regenerate video prompts",
            "shortcut": "r",
            "target_module": "generate_video_prompts",
            "feedback": {
              "enabled": true,
              "prompt": "What should be different?",
              "default_message": "Please generate different video prompts."
            }
          },
          {
            "id": "change_motion_elements",
            "mode": "jump",
            "label": "Change motion elements",
            "target_step": "video_generation",
            "target_module": "select_motion_elements"
          },
          {
            "id": "change_image",
            "mode": "jump",
            "label": "Go back to image selection",
            "target_step": "image_prompts",
            "target_module": "generate_and_select_images"
          }
        ]
      },
      "name": "generate_and_select_videos"
    }
  ]
}
```

### Prompts

#### cc_video_prompts_system.txt

```
Generate video/motion prompts for the selected scene. Create prompts optimized
for Leonardo Motion and Sora 2.

## Context
You have:
- A scene concept with narrative and visual moments
- Image analysis with detected motion elements
- User-selected elements to animate
- Elements to block in negative prompts

## CRITICAL: Use Selected Motion Elements

The user has specifically selected which elements to animate. Your prompts
must prioritize including animation of these selected elements. All other
potentially movable elements should be included in the negative prompt to
keep them static.

## Critical: Negative Prompts
Negative prompts are ESSENTIAL for quality video generation. They prevent:
- Morphing/deformation artifacts
- Unnatural movement
- Flickering and inconsistency
- Anatomical errors during motion

### Standard Negative Elements
Always include in negative prompts:
- morphing, deformation, distortion
- extra limbs, missing limbs, fused limbs
- blurry, out of focus, low quality
- flickering, strobing, jittering
- unnatural movement, robotic motion
- face distortion, eye glitches
- sudden scene changes, jump cuts

### Pet-Specific Negatives
- extra legs, missing legs, leg fusion
- tail splitting, multiple tails
- ear morphing, floating ears
- eye color change, pupil distortion
- fur texture changing, bald patches appearing
- body stretching, size fluctuation

### Block Unselected Motion Elements
Add ALL elements from `all_potential_elements` that are NOT in the user's
selected list to the negative prompt.

## Model-Specific Guidelines

### Leonardo Motion
**Docs:** https://docs.leonardo.ai/reference/createimagetovideogeneration

**Positive Prompt:** Focus on selected motion elements, describe motion type
**Negative Prompt:** Unselected elements + standard artifacts

### Sora 2
**Docs:** https://platform.openai.com/docs/guides/video-generation

**Prompt:** Natural language description with motion focus
**Style Notes:** Visual consistency, motion intensity, lighting preservation
```

#### cc_video_prompts_user.txt

```
Generate video/motion prompts for this pet scene:

## Scene Details

**Title:** {{ state.selected_scene.title }}
**Narrative:** {{ state.selected_scene.narrative }}
**Pet Behavior Focus:** {{ state.selected_scene.pet_behavior }}
**Setting:** {{ state.selected_scene.setting }}
**Emotional Arc:** {{ state.selected_scene.emotional_arc }}

## Context

**Pet Type:** {{ state.pet_type_selection.label }}
**Visual Tone:** {{ state.aesthetic_selection.visual_tone.mood }}
**Duration Target:** {{ state.duration_selection.video_duration }} seconds

## Image Analysis

**Environment:**
- Setting: {{ state.image_analysis.environment.setting }}
- Atmosphere: {{ state.image_analysis.environment.atmosphere }}
- Lighting: {{ state.image_analysis.environment.lighting }}

## IMPORTANT: Selected Motion Elements

The user has specifically chosen these elements to animate. Your prompts must
prioritize including animation of following elements:

{% for element in state.selected_motion_elements %}
- **{{ element.element }}**: {{ element.motion_type }} ({{ element.location }})
{% endfor %}

## Elements to Block in Negative Prompts

All of these elements should be kept STATIC:
{{ state.image_analysis.motion_elements.all_potential_elements | join(', ') }}

Generate Leonardo Motion and Sora 2 video prompts with natural, subtle motion.
The pet and all unselected elements must remain static.
```

### Schemas

#### cc_motion_elements_display_schema.json

```json
{
  "type": "object",
  "_ux.display": "passthrough",
  "properties": {
    "detected": {
      "type": "array",
      "_ux": {
        "display": "visible",
        "display_label": "Detected Motion Elements",
        "render_as": "content-panel"
      },
      "items": {
        "type": "object",
        "_ux": {
          "display": "visible",
          "render_as": "card",
          "nudges": ["index-badge"],
          "selectable": true
        },
        "properties": {
          "element": {
            "type": "string",
            "_ux": {
              "display": true,
              "display_label": "Element",
              "render_as": "card-title",
              "highlight": true,
              "highlight_color": "#4ECDC4"
            }
          },
          "motion_type": {
            "type": "string",
            "_ux.display": true,
            "_ux.display_label": "Motion Type"
          },
          "location": {
            "type": "string",
            "_ux.display": true,
            "_ux.display_label": "Location"
          }
        }
      }
    }
  }
}
```

#### cc_video_prompts_schema.json

```json
{
  "type": "object",
  "properties": {
    "scene_title": { "type": "string" },
    "motion_summary": { "type": "string" },
    "prompts": {
      "type": "object",
      "properties": {
        "leonardo": {
          "type": "object",
          "properties": {
            "positive_prompt": { "type": "string" },
            "negative_prompt": { "type": "string" },
            "motion_strength": {
              "type": "string",
              "enum": ["subtle", "moderate", "dynamic"]
            },
            "motion_notes": { "type": "string" }
          },
          "required": ["positive_prompt", "negative_prompt", "motion_strength",
                       "motion_notes"],
          "additionalProperties": false
        },
        "sora": {
          "type": "object",
          "properties": {
            "prompt": { "type": "string" },
            "style_notes": { "type": "string" }
          },
          "required": ["prompt", "style_notes"],
          "additionalProperties": false
        }
      },
      "required": ["leonardo", "sora"],
      "additionalProperties": false
    }
  },
  "required": ["scene_title", "motion_summary", "prompts"],
  "additionalProperties": false
}
```

#### cc_video_generation_display_schema.json

```json
{
  "type": "object",
  "_ux.display": "passthrough",
  "properties": {
    "leonardo": {
      "type": "object",
      "_ux": {
        "display": "visible",
        "tab_label": "Leonardo",
        "render_as": "tab.media",
        "provider": "leonardo",
        "input_schema": {
          "type": "object",
          "_ux": {
            "layout": "grid",
            "layout_columns": 3,
            "layout_columns_sm": 2
          },
          "properties": {
            "_text": {
              "type": "string",
              "title": "Prompt",
              "destination_field": "prompt",
              "_ux": {
                "input_type": "textarea",
                "col_span": "full",
                "rows": 4,
                "source_field": "positive_prompt"
              }
            },
            "negative_prompt": {
              "type": "string",
              "title": "Negative Prompt",
              "_ux": {
                "input_type": "textarea",
                "col_span": "full",
                "rows": 2,
                "source_field": "negative_prompt"
              }
            },
            "motion_strength": {
              "type": "integer",
              "title": "Motion Strength",
              "minimum": 1,
              "maximum": 10,
              "default": 5,
              "step": 1,
              "_ux": { "input_type": "slider" }
            }
          }
        }
      },
      "properties": {
        "positive_prompt": { "type": "string", "_ux.display": false },
        "negative_prompt": { "type": "string", "_ux.display": false },
        "motion_strength": {
          "type": "string",
          "_ux.display": true,
          "_ux.display_label": "Motion"
        },
        "motion_notes": {
          "type": "string",
          "_ux.display": true,
          "_ux.display_label": "Notes",
          "_ux.highlight": true,
          "_ux.highlight_color": "#7B68EE"
        }
      }
    },
    "sora": {
      "type": "object",
      "_ux": {
        "display": "visible",
        "tab_label": "Sora 2",
        "render_as": "tab.media",
        "provider": "openai",
        "input_schema": {
          "type": "object",
          "_ux": {
            "layout": "grid",
            "layout_columns": 3,
            "layout_columns_sm": 2
          },
          "properties": {
            "_text": {
              "type": "string",
              "title": "Prompt",
              "destination_field": "prompt",
              "_ux": {
                "input_type": "textarea",
                "col_span": "full",
                "rows": 4,
                "source_field": "prompt"
              }
            },
            "duration": {
              "type": "string",
              "title": "Duration",
              "enum": ["5", "10", "15", "20"],
              "default": "5",
              "enum_labels": {
                "5": "5 seconds",
                "10": "10 seconds",
                "15": "15 seconds",
                "20": "20 seconds"
              },
              "_ux": { "input_type": "select" }
            },
            "aspect_ratio": {
              "type": "string",
              "title": "Aspect Ratio",
              "enum": ["16:9", "9:16", "1:1"],
              "default": "9:16",
              "_ux": { "input_type": "select" }
            }
          }
        }
      },
      "properties": {
        "prompt": { "type": "string", "_ux.display": false },
        "style_notes": {
          "type": "string",
          "_ux.display": true,
          "_ux.display_label": "Style",
          "_ux.highlight": true,
          "_ux.highlight_color": "#4ECDC4"
        }
      }
    }
  }
}
```

---

## Files to Create/Modify

### New Step: `4_image_analysis/`

| File | Purpose |
|------|---------|
| `step.json` | Step definition |
| `prompts/cc_role_visual_analyst.txt` | Role prompt |
| `prompts/cc_image_analysis_system.txt` | System prompt |
| `prompts/cc_image_analysis_user.txt` | User prompt |
| `schemas/cc_image_analysis_schema.json` | LLM output schema |

### Modified: `5_video_generation/`

| File | Change |
|------|--------|
| `step.json` | Rename, add modules |
| `prompts/cc_video_prompts_system.txt` | Leonardo + Sora |
| `prompts/cc_video_prompts_user.txt` | Motion elements |
| `schemas/cc_video_prompts_schema.json` | New structure |
| `schemas/cc_motion_elements_display_schema.json` | NEW |
| `schemas/cc_video_generation_display_schema.json` | NEW |

### Renamed Folders

| Old | New |
|-----|-----|
| `5_text_overlays/` | `6_text_overlays/` |
| `6_titles_social/` | `7_titles_social/` |
| `7_music/` | `8_music/` |
| `8_workflow_summary/` | `9_workflow_summary/` |

### Backend Files

| File | Change |
|------|--------|
| `backend/server/workflow/interaction.py` | Add `context.current_interaction` |
| `backend/server/modules/media/generate.py` | Read source_image from state |
| `backend/providers/media/openai/provider.py` | Implement Sora 2 img2vid |

---

## Implementation Notes

### Selected Image local_path

Update step 3 to include `local_path` in `selected_content` output.
Address during implementation.

### Sora 2 Provider

Implement in OpenAI provider. Docs:
https://platform.openai.com/docs/guides/video-generation
