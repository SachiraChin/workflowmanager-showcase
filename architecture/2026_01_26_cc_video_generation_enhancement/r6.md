# CC Workflow Step 4: Video Generation Enhancement

## Summary

Enhance CC workflow by adding image analysis as a new step after image
generation, and enhancing video generation with motion element selection and
actual video generation.

## Step Restructure

```
1. user_input
2. scene_generation
3. image_prompts
4. image_analysis      ← NEW STEP
5. video_generation    ← was video_prompts (renamed + enhanced)
6. text_overlays
7. titles_social
8. music
9. workflow_summary
```

---

## Decisions Summary

| Item | Decision |
|------|----------|
| Image Analysis | New step 4 (no review UI) |
| Video Providers | Leonardo + Sora 2 |
| Source image access | Add previous interactions to context in interaction.py |
| Sora 2 | Needs implementation |

---

## Source Image for img2vid - Implementation

### Approach: Add Previous Interactions to Context

Modify `backend/server/workflow/interaction.py` to automatically add previous
interaction data to the `WorkflowExecutionContext`. This is versatile and
allows any future module to access data from earlier interactions.

### Implementation in interaction.py

In `continue_after_interaction`, after creating the context (around line 144):

```python
context = WorkflowExecutionContext(
    workflow_run_id=workflow_run_id,
    db=self.db,
    module_outputs=module_outputs,
    ...
)
context.current_module_name = module_name
context.current_module_index = module_index
context.step_id = step_id
context.retryable = module_config.get('retryable')
context.sub_actions = module_config.get('sub_actions')
context.cancel_event = cancel_event

# ADD: Load previous interactions into context

<!--NOT all interactions for workflow, only interaction for that module.
interaction_id can be seen at #171 of same file.-->

context.previous_interactions = self._load_previous_interactions(
    workflow_run_id
)
```

New method in `InteractionHandler`:

```python
def _load_previous_interactions(
    self,
    workflow_run_id: str
) -> Dict[str, Dict[str, Any]]:
    """
    Load all previous interaction data for this workflow.

    Returns dict keyed by step_id, then by module_name, containing:
    - _resolved_inputs: The inputs that were passed to the interaction
    - response: The user's response data (if available)

    Structure:
    {
        "image_prompts": {
            "generate_and_select_images": {
                "_resolved_inputs": {...},
                "response": {...}
            }
        },
        ...
    }
    """
    previous = {}

    # Get all interaction_requested events for this workflow
    events = self.db.event_repo.get_events_by_type(
        workflow_run_id=workflow_run_id,
        event_type=DbEventType.INTERACTION_REQUESTED
    )

    for event in events:
        step_id = event.get('step_id')
        module_name = event.get('module_name')
        data = event.get('data', {})

        if step_id not in previous:
            previous[step_id] = {}

        previous[step_id][module_name] = {
            '_resolved_inputs': data.get('_resolved_inputs', {}),
            'interaction_type': data.get('interaction_type'),
            'interaction_id': data.get('interaction_id'),
        }

    # Also get interaction responses to capture user selections
    responses = self.db.event_repo.get_events_by_type(
        workflow_run_id=workflow_run_id,
        event_type=DbEventType.INTERACTION_RESPONSE
    )

    for event in responses:
        step_id = event.get('step_id')
        module_name = event.get('module_name')
        data = event.get('data', {})

        if step_id in previous and module_name in previous[step_id]:
            previous[step_id][module_name]['response'] = data

    return previous
```

### How media.generate Uses It

In `backend/server/modules/media/generate.py`:

```python
def get_interaction_request(self, inputs, context):
    prompts = inputs.get('prompts', {})
    schema = inputs.get('schema', {})
    title = self.get_input_value(inputs, 'title')

    sub_actions = getattr(context, 'sub_actions', None)
    retryable = getattr(context, 'retryable', None)

    # For img2vid/img2img, get source image from previous interactions
    source_image = None
    if sub_actions:
        for action in sub_actions:
            if action.get('action_type') in ('img2vid', 'img2img'):
                # Access previous interaction data from context
                prev = getattr(context, 'previous_interactions', {})
                image_step = prev.get('image_prompts', {})
                image_module = image_step.get('generate_and_select_images', {})
                response = image_module.get('response', {})

                # selected_content comes from the interaction response
                source_image = response.get('selected_content')
                break

    return InteractionRequest(
        interaction_type=InteractionType.MEDIA_GENERATION,
        interaction_id=f"media_{uuid7_str()}",
        title=title,
        display_data={
            "data": prompts,
            "schema": schema,
            "sub_actions": sub_actions,
            "source_image": source_image,  # Pass to WebUI
            "generations": {},
            "retryable": retryable
        },
        context={
            "module_id": self.module_id
        }
    )
```

### WebUI Behavior

When user clicks "Generate" for img2vid:

```javascript
// In MediaGeneration component
const handleGenerate = (provider, params) => {
  const { source_image } = displayData;

  // Include source_image in params for img2vid
  const requestParams = {
    ...params,
    source_image: source_image?.url || source_image?.local_path
  };

  // Send sub-action request
  executeSubAction(provider, requestParams);
};
```

### Data Flow

```
Step 3: Image Generation
├── User selects image
├── Interaction response stored in events:
│   └── event_type: "interaction_response"
│   └── data: { selected_content: {...}, generations: {...} }
└── Also saved to state via outputs_to_state

Step 5: Video Generation
├── InteractionHandler creates context
├── _load_previous_interactions() queries events:
│   └── Gets interaction_requested + interaction_response events
│   └── Builds previous_interactions dict
├── context.previous_interactions = {
│     "image_prompts": {
│       "generate_and_select_images": {
│         "_resolved_inputs": {...},
│         "response": {
│           "selected_content": {
│             "content_id": "gc_xxx",
│             "url": "/workflow/wf_xxx/media/gc_xxx.jpg",
│             ...
│           }
│         }
│       }
│     }
│   }
├── media.generate reads source_image from context.previous_interactions
├── Passes to WebUI in display_data.source_image
└── WebUI includes in sub-action request params
```

### Benefits

1. **Versatile**: Any module can access any previous interaction's data
2. **Future-proof**: New modules automatically have access without code changes
3. **Centralized**: Logic is in one place (interaction.py)
4. **Complete data**: Both inputs and responses available

---

## Step 4: Image Analysis (NEW)

**Folder:** `workflows/cc/steps/4_image_analysis/`

### step.json

```json
{
  "step_id": "image_analysis",
  "name": "Step {step_number}: Image Analysis",
  "description": "Analyze selected image for motion elements and audio atmosphere",
  "modules": [
    {
      "module_id": "api.llm",
      "inputs": {
        "resolver_schema": {
          "type": "object",
          "properties": {
            "input": { "resolver": "server" },
            "system": { "resolver": "server" },
            "metadata": { "resolver": "server" }
          }
        },
        "input": [
          {
            "content": "{{ state.selected_image_data.local_path }}",
            "type": "image"
          },
          { "$ref": "prompts/cc_image_analysis_user.txt", "type": "text" }
        ],
        "output_schema": {
          "$ref": "schemas/cc_image_analysis_schema.json",
          "type": "json"
        },
        "metadata": {
          "step_id": "{{ step.step_id }}"
        },
        "system": [
          {
            "$ref": "prompts/cc_role_visual_analyst.txt",
            "type": "text",
            "cache_ttl": 10800
          },
          {
            "$ref": "prompts/cc_image_analysis_system.txt",
            "type": "text",
            "cache_ttl": 10800
          }
        ],
        "provider": "openai"
      },
      "outputs_to_state": {
        "response": "image_analysis"
      },
      "name": "analyze_image"
    }
  ]
}
```

### Prompts

#### cc_role_visual_analyst.txt

```
You are a visual analyst specializing in image-to-video production. Your role
is to analyze static images and identify elements suitable for subtle animation
in cute pet videos.

You have expertise in:
- Identifying animatable environmental elements (light, particles, fabric)
- Understanding motion physics for realistic subtle animation
- Recognizing atmospheric elements that enhance cozy aesthetics
- Analyzing audio characteristics implied by visual scenes

Your analysis directly informs video generation prompts and music selection,
so accuracy and specificity are critical.
```

#### cc_image_analysis_system.txt

```
# IMAGE ANALYSIS FOR PET VIDEO PRODUCTION

Analyze the provided image to extract environment details, motion elements, and
audio atmosphere for a cute pet video production.

## PART 1: ENVIRONMENT ANALYSIS

Extract scene details for narrative and caption generation.

**Identify:**
- Setting: Brief, specific description of the scene
- Time of day: dawn, morning, noon, afternoon, dusk, night
- Weather: If visible or implied (sunny, cloudy, rain, etc.)
- Location type: indoor, outdoor, specific venue type
- Key subjects: 3-6 main elements visible (include the pet!)
- Dominant colors: 3-5 colors defining the palette
- Atmosphere: The mood conveyed by the visuals
- Lighting: How light behaves in the scene

**Rules:**
- Only describe what is VISIBLE
- Be specific (e.g., "sunny living room with large windows" not "indoor")
- For key_subjects, prioritize the pet and their immediate environment

## PART 2: MOTION ELEMENT ANALYSIS

Identify elements for subtle animation suitable for short pet videos.

**Look for:**
- Light effects (sunbeams, lamp glow, reflections)
- Particles (dust motes, floating fur/hair)
- Fabric (pet bed, blankets, curtains)
- Environmental (plants swaying, water in bowl)
- Pet-adjacent elements (toys, food bowl steam)

**Constraints:**
- The PET must remain STATIC - do not suggest animating the pet itself
- Camera must remain STATIC - no zoom, pan, or movement
- Focus on SUBTLE, LOOPING motion suitable for 5-10 second clips
- Only identify elements CLEARLY VISIBLE in the image

**detected_elements (3-8 items):**
Best candidates for animation. For each provide:
- element: What could be animated
- motion_type: Type of motion (e.g., "gentle sway", "shimmer")
- location: Where in the frame

**all_potential_elements (5-20 items):**
Complete list of everything that COULD move (for negative prompts).
Include detected elements plus any others that might drift unintentionally.

## PART 3: AUDIO ATMOSPHERE ANALYSIS

Analyze for music generation context. This data will be used by the music
generation step to create appropriate background music.

**Identify:**
- implied_sounds: 2-5 sounds the scene suggests
- silence_level: very quiet / quiet / moderate / busy / loud
- emotional_intensity: serene / calm / neutral / tense / dramatic / intense
- temporal_feel: frozen/still / slow/meditative / gentle/flowing / moderate
- warmth: cold/clinical / cool / neutral / warm / very warm/intimate
- era_feel: Time period evoked (e.g., "cozy modern home")
- cultural_context: Geographic/cultural context if apparent
- suggested_music_genres: 2-3 genres that would complement
- musical_texture: Suggested texture (e.g., "warm and gentle")
```

#### cc_image_analysis_user.txt

```
Analyze this image of a pet scene and extract:
1. Environment details (setting, time, weather, location, subjects, colors,
   atmosphere, lighting)
2. Motion elements for animation (detected candidates and all potential
   elements)
3. Audio atmosphere (implied sounds, emotional qualities, music suggestions)

Focus on elements that would enhance a cute, heartwarming pet video.
```

### Schemas

#### cc_image_analysis_schema.json

```json
{
  "type": "object",
  "properties": {
    "environment": {
      "type": "object",
      "properties": {
        "setting": { "type": "string" },
        "time_of_day": { "type": "string" },
        "weather": { "type": "string" },
        "location_type": { "type": "string" },
        "key_subjects": {
          "type": "array",
          "items": { "type": "string" },
          "minItems": 3,
          "maxItems": 6
        },
        "dominant_colors": {
          "type": "array",
          "items": { "type": "string" },
          "minItems": 3,
          "maxItems": 5
        },
        "atmosphere": { "type": "string" },
        "lighting": { "type": "string" }
      },
      "required": ["setting", "time_of_day", "weather", "location_type",
                   "key_subjects", "dominant_colors", "atmosphere", "lighting"],
      "additionalProperties": false
    },
    "motion_elements": {
      "type": "object",
      "properties": {
        "detected_elements": {
          "type": "array",
          "items": {
            "type": "object",
            "properties": {
              "element": { "type": "string" },
              "motion_type": { "type": "string" },
              "location": { "type": "string" }
            },
            "required": ["element", "motion_type", "location"],
            "additionalProperties": false
          },
          "minItems": 3,
          "maxItems": 8
        },
        "all_potential_elements": {
          "type": "array",
          "items": { "type": "string" },
          "minItems": 5,
          "maxItems": 20
        }
      },
      "required": ["detected_elements", "all_potential_elements"],
      "additionalProperties": false
    },
    "audio_atmosphere": {
      "type": "object",
      "properties": {
        "implied_sounds": {
          "type": "array",
          "items": { "type": "string" },
          "minItems": 2,
          "maxItems": 5
        },
        "silence_level": {
          "type": "string",
          "enum": ["very quiet", "quiet", "moderate", "busy", "loud"]
        },
        "emotional_intensity": {
          "type": "string",
          "enum": ["serene", "calm", "neutral", "tense", "dramatic", "intense"]
        },
        "temporal_feel": {
          "type": "string",
          "enum": ["frozen/still", "slow/meditative", "gentle/flowing",
                   "moderate/steady", "dynamic/energetic", "urgent/fast"]
        },
        "warmth": {
          "type": "string",
          "enum": ["cold/clinical", "cool", "neutral", "warm",
                   "very warm/intimate"]
        },
        "era_feel": { "type": "string" },
        "cultural_context": { "type": "string" },
        "suggested_music_genres": {
          "type": "array",
          "items": { "type": "string" },
          "minItems": 2,
          "maxItems": 3
        },
        "musical_texture": { "type": "string" }
      },
      "required": ["implied_sounds", "silence_level", "emotional_intensity",
                   "temporal_feel", "warmth", "era_feel", "cultural_context",
                   "suggested_music_genres", "musical_texture"],
      "additionalProperties": false
    }
  },
  "required": ["environment", "motion_elements", "audio_atmosphere"],
  "additionalProperties": false
}
```

---

## Step 5: Video Generation (ENHANCED)

**Folder:** `workflows/cc/steps/5_video_generation/`

### step.json

```json
{
  "step_id": "video_generation",
  "name": "Step {step_number}: Video Generation",
  "description": "Select motion elements, generate video prompts, and create videos",
  "modules": [
    {
      "module_id": "user.select",
      "inputs": {
        "resolver_schema": {
          "type": "object",
          "properties": {
            "data": { "resolver": "server" }
          }
        },
        "data": {
          "detected": "{{ state.image_analysis.motion_elements.detected_elements }}"
        },
        "schema": {
          "$ref": "schemas/cc_motion_elements_display_schema.json",
          "type": "json"
        },
        "prompt": "Select motion elements to animate in your video",
        "multi_select": true,
        "mode": "select"
      },
      "outputs_to_state": {
        "selected_indices": "selected_motion_element_indices",
        "selected_data": "selected_motion_elements"
      },
      "name": "select_motion_elements"
    },
    {
      "module_id": "api.llm",
      "inputs": {
        "resolver_schema": {
          "type": "object",
          "properties": {
            "input": { "resolver": "server" },
            "system": { "resolver": "server" },
            "metadata": { "resolver": "server" }
          }
        },
        "input": {
          "$ref": "prompts/cc_video_prompts_user.txt",
          "type": "jinja2"
        },
        "output_schema": {
          "$ref": "schemas/cc_video_prompts_schema.json",
          "type": "json"
        },
        "metadata": {
          "step_id": "{{ step.step_id }}"
        },
        "system": [
          {
            "$ref": "prompts/cc_role_motion_engineer.txt",
            "type": "text",
            "cache_ttl": 10800
          },
          {
            "$ref": "prompts/cc_video_prompts_system.txt",
            "type": "text",
            "cache_ttl": 10800
          }
        ],
        "provider": "openai"
      },
      "outputs_to_state": {
        "response": "video_prompts"
      },
      "name": "generate_video_prompts"
    },
    {
      "module_id": "media.generate",
      "inputs": {
        "resolver_schema": {
          "type": "object",
          "properties": {
            "prompts": { "resolver": "server" }
          }
        },
        "prompts": "{{ state.video_prompts.prompts }}",
        "schema": {
          "$ref": "schemas/cc_video_generation_display_schema.json",
          "type": "json"
        },
        "title": "Generate and Select Videos"
      },
      "outputs_to_state": {
        "selected_content_id": "selected_video_id",
        "selected_content": "selected_video_data",
        "generations": "video_generations"
      },
      "sub_actions": [
        {
          "id": "generate",
          "label": "Generate Videos",
          "action_type": "img2vid",
          "loading_label": "Generating video...",
          "result_key": "generations"
        }
      ],
      "retryable": {
        "default_option": "continue",
        "options": [
          {
            "id": "continue",
            "mode": "continue",
            "label": "Accept and continue"
          },
          {
            "id": "retry_prompts",
            "mode": "retry",
            "label": "Regenerate video prompts",
            "shortcut": "r",
            "target_module": "generate_video_prompts",
            "feedback": {
              "enabled": true,
              "prompt": "What should be different?",
              "default_message": "Please generate different video prompts."
            }
          },
          {
            "id": "change_motion_elements",
            "mode": "jump",
            "label": "Change motion elements",
            "target_step": "video_generation",
            "target_module": "select_motion_elements"
          },
          {
            "id": "change_image",
            "mode": "jump",
            "label": "Go back to image selection",
            "target_step": "image_prompts",
            "target_module": "generate_and_select_images"
          }
        ]
      },
      "name": "generate_and_select_videos"
    }
  ]
}
```

### Prompts

#### cc_video_prompts_system.txt

```
Generate video/motion prompts for the selected scene. Create prompts optimized
for Leonardo Motion and Sora 2.

## Context
You have:
- A scene concept with narrative and visual moments
- Image analysis with detected motion elements
- User-selected elements to animate
- Elements to block in negative prompts

## CRITICAL: Use Selected Motion Elements

The user has specifically selected which elements to animate. Your prompts
must prioritize including animation of these selected elements. All other
potentially movable elements should be included in the negative prompt to
keep them static.

## Critical: Negative Prompts
Negative prompts are ESSENTIAL for quality video generation. They prevent:
- Morphing/deformation artifacts
- Unnatural movement
- Flickering and inconsistency
- Anatomical errors during motion

### Standard Negative Elements
Always include in negative prompts:
- morphing, deformation, distortion
- extra limbs, missing limbs, fused limbs
- blurry, out of focus, low quality
- flickering, strobing, jittering
- unnatural movement, robotic motion
- face distortion, eye glitches
- sudden scene changes, jump cuts

### Pet-Specific Negatives
- extra legs, missing legs, leg fusion
- tail splitting, multiple tails
- ear morphing, floating ears
- eye color change, pupil distortion
- fur texture changing, bald patches appearing
- body stretching, size fluctuation

### Block Unselected Motion Elements
Add ALL elements from `all_potential_elements` that are NOT in the user's
selected list to the negative prompt.

## Model-Specific Guidelines

### Leonardo Motion
**Docs:** https://docs.leonardo.ai/reference/createimagetovideogeneration

**Positive Prompt:** Focus on selected motion elements, describe motion type
**Negative Prompt:** Unselected elements + standard artifacts

### Sora 2
**Docs:** https://platform.openai.com/docs/guides/video-generation

**Prompt:** Natural language description with motion focus
**Style Notes:** Visual consistency, motion intensity, lighting preservation
```

#### cc_video_prompts_user.txt

```
Generate video/motion prompts for this pet scene:

## Scene Details

**Title:** {{ state.selected_scene.title }}
**Narrative:** {{ state.selected_scene.narrative }}
**Pet Behavior Focus:** {{ state.selected_scene.pet_behavior }}
**Setting:** {{ state.selected_scene.setting }}
**Emotional Arc:** {{ state.selected_scene.emotional_arc }}

## Context

**Pet Type:** {{ state.pet_type_selection.label }}
**Visual Tone:** {{ state.aesthetic_selection.visual_tone.mood }}
**Duration Target:** {{ state.duration_selection.video_duration }} seconds

## Image Analysis

**Environment:**
- Setting: {{ state.image_analysis.environment.setting }}
- Atmosphere: {{ state.image_analysis.environment.atmosphere }}
- Lighting: {{ state.image_analysis.environment.lighting }}

## IMPORTANT: Selected Motion Elements

The user has specifically chosen these elements to animate. Your prompts must
prioritize including animation of following elements:

{% for element in state.selected_motion_elements %}
- **{{ element.element }}**: {{ element.motion_type }} ({{ element.location }})
{% endfor %}

## Elements to Block in Negative Prompts

All of these elements should be kept STATIC:
{{ state.image_analysis.motion_elements.all_potential_elements | join(', ') }}

Generate Leonardo Motion and Sora 2 video prompts with natural, subtle motion.
The pet and all unselected elements must remain static.
```

### Schemas

(Same as r5 - cc_motion_elements_display_schema.json, cc_video_prompts_schema.json, cc_video_generation_display_schema.json)

---

## Files to Create/Modify

### New Step: `4_image_analysis/`

| File | Purpose |
|------|---------|
| `step.json` | Step definition |
| `prompts/cc_role_visual_analyst.txt` | Role prompt |
| `prompts/cc_image_analysis_system.txt` | System prompt |
| `prompts/cc_image_analysis_user.txt` | User prompt |
| `schemas/cc_image_analysis_schema.json` | LLM output schema |

### Modified: `5_video_generation/`

| File | Change |
|------|--------|
| `step.json` | Rename, add modules |
| `prompts/cc_video_prompts_system.txt` | Leonardo + Sora |
| `prompts/cc_video_prompts_user.txt` | Motion elements |
| `schemas/cc_video_prompts_schema.json` | New structure |
| `schemas/cc_motion_elements_display_schema.json` | NEW |
| `schemas/cc_video_generation_display_schema.json` | NEW |

### Renamed Folders

| Old | New |
|-----|-----|
| `5_text_overlays/` | `6_text_overlays/` |
| `6_titles_social/` | `7_titles_social/` |
| `7_music/` | `8_music/` |
| `8_workflow_summary/` | `9_workflow_summary/` |

### Backend Files

| File | Change |
|------|--------|
| `backend/server/workflow/interaction.py` | Add `_load_previous_interactions()` |
| `backend/server/modules/media/generate.py` | Read source_image from context |
| `backend/providers/media/openai/provider.py` | Implement Sora 2 img2vid |

---

## Implementation Notes

### Selected Image local_path

Update step 3 to include `local_path` in `selected_content` when needed.
Address during implementation.

### Sora 2 Provider

Implement in OpenAI provider. Docs:
https://platform.openai.com/docs/guides/video-generation
