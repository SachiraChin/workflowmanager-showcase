# CC Workflow Step 4: Video Generation Enhancement

## Summary

Enhance CC workflow step 4 to add image-based motion detection, motion element
selection, and video generation UX (similar to step 3's image generation). This
transforms step 4 from a simple prompt review step into a full video production
step.

## Current State Analysis

### CC Step 3 (Image Prompts) - Current

```
1. generate_image_prompts    → LLM generates prompts
2. fetch_sd_models           → Fetch SD model categories
3. generate_and_select_images → media.generate with tabs for generation/selection
```

**Key Features:**
- `media.generate` module with provider tabs (Midjourney, Leonardo, OpenAI, SD)
- User can modify prompts, select parameters, generate, and select images
- Selected image stored in `state.selected_image_data`

### CC Step 4 (Video Prompts) - Current

```
1. generate_video_prompts → LLM generates video prompts (Motion 2.0, MJ Animate)
2. review_video_prompts   → user.select in review mode (no generation)
```

**Key Features:**
- LLM generates prompts based on scene data + image prompt reference
- Simple review UI (no actual video generation)
- No motion analysis of the actual selected image

### OMS Step 3 (Image Analysis) - Reference Pattern

Uses `api.llm` with vision to analyze an image and extract:
- `environment`: setting, time, weather, location, subjects, colors, atmosphere
- `motion_elements.detected_elements`: Array of animatable elements with:
  - `element`: What could be animated (e.g., "rain droplets")
  - `motion_type`: Type of motion (e.g., "falling, trickling")
  - `location`: Position in frame
- `motion_elements.all_potential_elements`: All movable elements (for negatives)
- `audio_atmosphere`: Audio/music characteristics

### OMS Step 4 (Video Prompt Generation) - Reference Pattern

```
1. select_motion_anchors     → User selects from planned + detected elements
2. select_video_groups       → User selects video model groups
3. build_video_prompt        → Dynamic system prompt from selections
4. build_video_schema        → Dynamic output schema from selections
5. generate_video_prompts_api → LLM generates prompts with selected anchors
6. build_video_display_schema → Display schema for review
7. review_video_prompts      → User reviews generated prompts
```

**Key difference from CC:** OMS has both "planned" (from scene summary) and
"detected" (from image analysis) motion anchors. CC will only have detected.

## Proposed Design

### New CC Step 4 Flow

```
1. analyze_image             → [NEW] LLM vision analyzes selected image
2. save_image_analysis       → [NEW] Save analysis to JSON
3. select_motion_elements    → [NEW] User selects detected motion elements
4. generate_video_prompts    → [MODIFIED] Use selected motion elements
5. generate_and_select_videos → [NEW] media.generate for video (like step 3)
```

### Module Details

#### 1. analyze_image (NEW)

```json
{
  "module_id": "api.llm",
  "inputs": {
    "input": [

        <!--following is wrong, we need base64 image to send to llm, and
        selected_image_data has access to image in filesystem, where you can
        directly access the file and generate base64 value-->

      { "content": "{{ state.selected_image_data.url }}", "type": "image" },
      { "$ref": "prompts/cc_image_analysis_user.txt", "type": "text" }
    ],
    "system": [
      { "$ref": "prompts/cc_role_visual_analyst.txt", "type": "text" },
      { "$ref": "prompts/cc_image_analysis_system.txt", "type": "text" }
    ],
    "output_schema": { "$ref": "schemas/cc_image_analysis_schema.json" },
    "provider": "openai"
  },
  "outputs_to_state": {
    "response": "image_analysis"
  }
}
```

**Schema:** Simplified version of OMS schema focusing on:
- `motion_elements.detected_elements` - Animatable elements
- `motion_elements.all_potential_elements` - For negative prompts
- Optional: `environment` subset if useful for prompt generation

<!--please add prompts here so i have an idea on how it looks.-->

#### 2. select_motion_elements (NEW)

```json
{
  "module_id": "user.select",
  "inputs": {
    "data": {
      "detected": "{{ state.image_analysis.motion_elements.detected_elements }}"
    },
    "schema": { "$ref": "schemas/cc_motion_elements_display_schema.json" },
    "prompt": "Select motion elements to animate",
    "multi_select": true,
    "mode": "select"
  },
  "outputs_to_state": {
    "selected_indices": "selected_motion_element_indices"
  }
}
```

**Key difference from OMS:** Only "detected" panel, no "planned" panel.

#### 3. generate_video_prompts (MODIFIED)

Update system prompt to include:
- Selected motion elements (what to animate)
- All potential elements (what to block in negatives)
- Scene context from state

#### 4. generate_and_select_videos (NEW)

```json
{
  "module_id": "media.generate",
  "inputs": {
    "prompts": "{{ state.video_prompts }}",
    "schema": { "$ref": "schemas/cc_video_prompts_display_schema.json" },
    "title": "Generate and Select Videos"
  },
  "outputs_to_state": {
    "selected_content_id": "selected_video_id",
    "selected_content": "selected_video_data",
    "generations": "video_generations"
  },
  "sub_actions": [
    {
      "id": "generate",
      "label": "Generate Videos",
      "action_type": "img2vid",
      "loading_label": "Generating...",
      "result_key": "generations"
    }
  ]
}
```

<!--add display schema here.-->

## New Files Required

### Schemas

| File | Purpose |
|------|---------|
| `cc_image_analysis_schema.json` | LLM output for image analysis |
| `cc_motion_elements_display_schema.json` | Motion element selection UI |
| `cc_video_generation_display_schema.json` | Video generation tabs UI |

### Prompts

| File | Purpose |
|------|---------|
| `cc_role_visual_analyst.txt` | Role for image analysis |
| `cc_image_analysis_system.txt` | System instructions for analysis |
| `cc_image_analysis_user.txt` | User prompt for analysis |

### Modified Files

| File | Change |
|------|--------|
| `step.json` | Add new modules, modify flow |
| `cc_video_prompts_user.txt` | Include selected motion elements |
| `cc_video_prompts_display_schema.json` | Update for media.generate |

## Data Flow

```
state.selected_image_data (from step 3)
    ↓
[analyze_image] → state.image_analysis
    ↓
[select_motion_elements] → state.selected_motion_element_indices
    ↓
[generate_video_prompts] → state.video_prompts
    ↓
[generate_and_select_videos] → state.selected_video_data
```

## Questions for Review

### 1. Image Analysis Scope

The OMS image analysis includes `environment` and `audio_atmosphere` sections.
For CC, should we:

- **A)** Only extract motion elements (minimal, focused)?
- **B)** Also extract environment (useful for prompt enrichment)?
- **C)** Full OMS schema (environment, motion, audio)?

<!--C-->

### 2. Video Providers

Which video generation providers should be supported in the tabs?

Current providers in codebase:
- `motion_2_0` - Motion 2.0
- `midjourney_animate` - Midjourney Animate

Are there other video providers to include (e.g., Runway, Pika, Kling)?

<!--no mj due to limitation, it will leonardo (not only motion 2.0) and Sora 2-->

### 3. Selected Image Reference

For video generation, how should the selected image be passed?

- `state.selected_image_data.url` - Direct URL
- `state.selected_image_data.local_path` - Local file path

Need to verify what `media.generate` expects for `img2vid` action type.

<!--check generated_content collection in workflow_db in
mongodb://172.18.48.1:27017 db, you should be able to clear idea on what we
have access to -->

### 4. Motion Element Selection UI

The OMS schema shows both "Planned Animation" and "Detected Element" panels.
Since CC won't have planned elements:

- **A)** Single panel labeled "Detected Motion Elements"?
- **B)** Reuse OMS schema but only populate "detected"?

<!-- A -->

### 5. Backwards Compatibility

Current step 4 outputs `state.video_prompts` in a specific structure.
Are there downstream steps (5, 6, 7, 8) that depend on this structure?

If yes, we need to maintain the output structure or update all consumers.

<!--there shouldnt be, but check anyways-->

### 6. Retry/Navigation Options

Should the retryable options include:
- Jump back to motion element selection?
- Jump back to image selection (step 3)?
- Jump back to scene selection (step 2)?

What navigation granularity is desired?

<!--for now add all options in existing preview module-->

### 7. Source Image for img2vid

For `img2vid` generation, the source image must be passed. Where in the schema
should this be specified:

- In `sub_actions[0]` configuration?
- In `input_schema` with a hidden field?
- Automatically derived from state?

Need to verify how `media.generate` with `action_type: "img2vid"` works.

<!--this is a good question, i think we have to play around how modules work to
figure this out. as you may have seen claude.md, everything we make has to be
generic modules which can work everywhere, but specific module can have its on
parameters and read structures to access params. what i can think is as
solution to add input field to media.generate for input_image which will be
extracted from selection in step 3. quetion here is what is the excact
structure for this input, is it full image object, or only path. For now, i'm
leaning towards full object and let m,odule pick what it needs. the module
support img2img, img2vid modes anyways, we can declare that these 2 modes
require this field. I am open to discuss the structure of this field,
extraction, task creation and all nuances around it. it would be nice to see
some proposals around this with end to end data fliw.-->
