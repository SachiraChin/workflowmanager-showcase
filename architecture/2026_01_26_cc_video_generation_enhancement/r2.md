# CC Workflow Step 4: Video Generation Enhancement

## Summary

Enhance CC workflow step 4 to add image-based motion detection, motion element
selection, and video generation UX (similar to step 3's image generation). This
transforms step 4 from a simple prompt review step into a full video production
step.

## Decisions from R1 Review

| Question | Decision |
|----------|----------|
| Image Analysis Scope | Full OMS schema (environment, motion, audio) |
| Video Providers | Leonardo (full, not just Motion 2.0) + Sora 2 |
| Motion Element UI | Single panel "Detected Motion Elements" |
| Backwards Compatibility | Update `workflow_summary.txt` template |
| Retry/Navigation | Mirror existing step 4 options |

## Current vs Proposed Flow

### Current CC Step 4

```
1. generate_video_prompts  → LLM generates prompts
2. review_video_prompts    → user.select in review mode
```

### Proposed CC Step 4

```
1. analyze_image             → LLM vision analyzes selected image
2. select_motion_elements    → User picks elements to animate
3. generate_video_prompts    → LLM uses selected motion elements
4. generate_and_select_videos → media.generate with img2vid
```

## Module Specifications

### 1. analyze_image (NEW)

```json
{
  "module_id": "api.llm",
  "inputs": {
    "resolver_schema": {
      "type": "object",
      "properties": {
        "input": { "resolver": "server" },
        "system": { "resolver": "server" },
        "metadata": { "resolver": "server" }
      }
    },
    "input": [
      {
        "content": "{{ state.selected_image_data.local_path }}",
        "type": "image_file"
      },
      { "$ref": "prompts/cc_image_analysis_user.txt", "type": "text" }
    ],
    "output_schema": {
      "$ref": "schemas/cc_image_analysis_schema.json",
      "type": "json"
    },
    "metadata": {
      "step_id": "{{ step.step_id }}"
    },
    "system": [
      {
        "$ref": "prompts/cc_role_visual_analyst.txt",
        "type": "text",
        "cache_ttl": 10800
      },
      {
        "$ref": "prompts/cc_image_analysis_system.txt",
        "type": "text",
        "cache_ttl": 10800
      }
    ],
    "provider": "openai"
  },
  "outputs_to_state": {
    "response": "image_analysis"
  },
  "name": "analyze_image"
}
```

<!--image type below is wrong, it has to be "type": "image", it has been
validated manu times in oms workflow, if image is in filesystem, it will read
and covert it to correct format. read _encode_image use in
backend/server/modules/api/providers/openai/provider.py -->

**Note on image input:** The `type: "image_file"` indicates the server should
read the file from `local_path` and convert to base64 for the LLM API. This
requires the input resolver to handle this type.

**Alternative:** If server-side base64 conversion isn't supported, we could:
- Add a new resolver type for image files
- Pre-process in a separate module before the LLM call

### 2. select_motion_elements (NEW)

```json
{
  "module_id": "user.select",
  "inputs": {
    "resolver_schema": {
      "type": "object",
      "properties": {
        "data": { "resolver": "server" }
      }
    },
    "data": {
      "detected": "{{ state.image_analysis.motion_elements.detected_elements }}"
    },
    "schema": {
      "$ref": "schemas/cc_motion_elements_display_schema.json",
      "type": "json"
    },
    "prompt": "Select motion elements to animate in your video",
    "multi_select": true,
    "mode": "select"
  },
  "outputs_to_state": {
    "selected_indices": "selected_motion_element_indices",
    "selected_data": "selected_motion_elements"
  },
  "name": "select_motion_elements"
}
```

### 3. generate_video_prompts (MODIFIED)

```json
{
  "module_id": "api.llm",
  "inputs": {
    "resolver_schema": {
      "type": "object",
      "properties": {
        "input": { "resolver": "server" },
        "system": { "resolver": "server" },
        "metadata": { "resolver": "server" }
      }
    },
    "ai_config": {},
    "input": {
      "$ref": "prompts/cc_video_prompts_user.txt",
      "type": "jinja2"
    },
    "output_schema": {
      "$ref": "schemas/cc_video_prompts_schema.json",
      "type": "json"
    },
    "metadata": {
      "step_id": "{{ step.step_id }}"
    },
    "system": [
      {
        "$ref": "prompts/cc_role_motion_engineer.txt",
        "type": "text",
        "cache_ttl": 10800
      },
      {
        "$ref": "prompts/cc_video_prompts_system.txt",
        "type": "text",
        "cache_ttl": 10800
      }
    ],
    "provider": "openai"
  },
  "outputs_to_state": {
    "response": "video_prompts"
  },
  "name": "generate_video_prompts"
}
```

### 4. generate_and_select_videos (NEW)

```json
{
  "module_id": "media.generate",
  "inputs": {
    "resolver_schema": {
      "type": "object",
      "properties": {
        "prompts": { "resolver": "server" },
        "input_image": { "resolver": "server" }
      }
    },
    "prompts": "{{ state.video_prompts.prompts }}",
    "input_image": "{{ state.selected_image_data }}",
    "schema": {
      "$ref": "schemas/cc_video_generation_display_schema.json",
      "type": "json"
    },
    "title": "Generate and Select Videos"
  },
  "outputs_to_state": {
    "selected_content_id": "selected_video_id",
    "selected_content": "selected_video_data",
    "generations": "video_generations"
  },
  "sub_actions": [
    {
      "id": "generate",
      "label": "Generate Videos",
      "action_type": "img2vid",
      "loading_label": "Generating video...",
      "result_key": "generations"
    }
  ],
  "retryable": {
    "default_option": "continue",
    "options": [
      {
        "id": "continue",
        "mode": "continue",
        "label": "Accept and continue"
      },
      {
        "id": "retry_prompts",
        "mode": "retry",
        "label": "Regenerate video prompts",
        "shortcut": "r",
        "target_module": "generate_video_prompts",
        "feedback": {
          "enabled": true,
          "prompt": "What should be different?",
          "default_message": "Please generate different video prompts."
        }
      },
      {
        "id": "change_motion_elements",
        "mode": "jump",
        "label": "Change motion elements",
        "target_step": "video_prompts",
        "target_module": "select_motion_elements"
      },
      {
        "id": "change_image",
        "mode": "jump",
        "label": "Go back to image selection",
        "target_step": "image_prompts",
        "target_module": "generate_and_select_images"
      }
    ]
  },
  "name": "generate_and_select_videos"
}
```

---

## Prompts


### cc_role_visual_analyst.txt (NEW)

```
You are a visual analyst specializing in image-to-video production. Your role
is to analyze static images and identify elements suitable for subtle animation
in lofi/anime style videos.

You have expertise in:
- Identifying animatable environmental elements (water, particles, light)
- Understanding motion physics for realistic subtle animation
- Recognizing atmospheric elements that enhance lofi aesthetics
- Analyzing audio characteristics implied by visual scenes

Your analysis directly informs video generation prompts, so accuracy and
specificity are critical.
```

### cc_image_analysis_system.txt (NEW)

```
# IMAGE ANALYSIS FOR PET VIDEO PRODUCTION

Analyze the provided image to extract environment details, motion elements, and
audio atmosphere for a cute pet video production.

## PART 1: ENVIRONMENT ANALYSIS

Extract scene details for narrative and caption generation.

**Identify:**
- Setting: Brief, specific description of the scene
- Time of day: dawn, morning, noon, afternoon, dusk, night
- Weather: If visible or implied (sunny, cloudy, rain, etc.)
- Location type: indoor, outdoor, specific venue type
- Key subjects: 3-6 main elements visible (include the pet!)
- Dominant colors: 3-5 colors defining the palette
- Atmosphere: The mood conveyed by the visuals
- Lighting: How light behaves in the scene

**Rules:**
- Only describe what is VISIBLE
- Be specific (e.g., "sunny living room with large windows" not "indoor")
- For key_subjects, prioritize the pet and their immediate environment

## PART 2: MOTION ELEMENT ANALYSIS

Identify elements for subtle animation suitable for short pet videos.

**Look for:**
- Light effects (sunbeams, lamp glow, reflections)
- Particles (dust motes, floating fur/hair)
- Fabric (pet bed, blankets, curtains)
- Environmental (plants swaying, water in bowl)
- Pet-adjacent elements (toys, food bowl steam)

**Constraints:**
- The PET must remain STATIC - do not suggest animating the pet itself
- Camera must remain STATIC - no zoom, pan, or movement
- Focus on SUBTLE, LOOPING motion suitable for 5-10 second clips
- Only identify elements CLEARLY VISIBLE in the image

**detected_elements (3-8 items):**
Best candidates for animation. For each provide:
- element: What could be animated
- motion_type: Type of motion (e.g., "gentle sway", "shimmer")
- location: Where in the frame

**all_potential_elements (5-20 items):**
Complete list of everything that COULD move (for negative prompts).
Include detected elements plus any others that might drift unintentionally.

## PART 3: AUDIO ATMOSPHERE ANALYSIS

Analyze for music generation context.

**Identify:**
- implied_sounds: 2-5 sounds the scene suggests
- silence_level: very quiet / quiet / moderate / busy / loud
- emotional_intensity: serene / calm / neutral / tense / dramatic / intense
- temporal_feel: frozen/still / slow/meditative / gentle/flowing / moderate
- warmth: cold/clinical / cool / neutral / warm / very warm/intimate
- era_feel: Time period evoked (e.g., "cozy modern home")
- cultural_context: Geographic/cultural context if apparent
- suggested_music_genres: 2-3 genres that would complement
- musical_texture: Suggested texture (e.g., "warm and gentle")
```

### cc_image_analysis_user.txt (NEW)

```
Analyze this image of a pet scene and extract:
1. Environment details (setting, time, weather, location, subjects, colors,
   atmosphere, lighting)
2. Motion elements for animation (detected candidates and all potential
   elements)
3. Audio atmosphere (implied sounds, emotional qualities, music suggestions)

Focus on elements that would enhance a cute, heartwarming pet video.
```

### cc_video_prompts_user.txt (MODIFIED)

```
Generate video/motion prompts for this pet scene:

## Scene Details

**Title:** {{ state.selected_scene.title }}
**Narrative:** {{ state.selected_scene.narrative }}
**Pet Behavior Focus:** {{ state.selected_scene.pet_behavior }}
**Setting:** {{ state.selected_scene.setting }}
**Emotional Arc:** {{ state.selected_scene.emotional_arc }}

**Key Visual Moments:**
{% for moment in state.selected_scene.visual_moments %}
- {{ moment }}
{% endfor %}

## Context

**Pet Type:** {{ state.pet_type_selection.label }}

**Visual Tone:**
- Mood: {{ state.aesthetic_selection.visual_tone.mood }}
- Lighting: {{ state.aesthetic_selection.visual_tone.lighting }}
- Energy: {{ state.aesthetic_selection.visual_tone.energy }}

**Duration Target:** {{ state.duration_selection.video_duration }} seconds

{% if state.user_direction %}
**User Direction:** {{ state.user_direction }}
{% endif %}

## Image Analysis

**Environment:**
- Setting: {{ state.image_analysis.environment.setting }}
- Atmosphere: {{ state.image_analysis.environment.atmosphere }}
- Lighting: {{ state.image_analysis.environment.lighting }}

<!--emphasize the need to use these elements-->
**Selected Motion Elements to Animate:**
{% for element in state.selected_motion_elements %}
- {{ element.element }}: {{ element.motion_type }} ({{ element.location }})
{% endfor %}

**Elements to Block (for negative prompts):**
{{ state.image_analysis.motion_elements.all_potential_elements | join(', ') }}

<!--following is not needed?-->
## Image Prompt Reference
**Midjourney:** {{ state.image_prompts.prompts.midjourney.prompt }}

Generate Leonardo and Sora 2 video prompts that bring this scene to life with
natural, subtle motion. The pet should remain static - only animate the
selected environmental elements. Include comprehensive negative prompts.
```


<!--where's updated workflows/cc/steps/4_video_prompts/prompts/cc_video_prompts_system.txt?-->

---


## Schemas

### cc_image_analysis_schema.json (NEW)

```json
{
  "type": "object",
  "properties": {
    "environment": {
      "type": "object",
      "properties": {
        "setting": {
          "type": "string",
          "description": "Brief description of the scene setting"
        },
        "time_of_day": {
          "type": "string",
          "description": "Time of day depicted"
        },
        "weather": {
          "type": "string",
          "description": "Weather conditions if apparent"
        },
        "location_type": {
          "type": "string",
          "description": "Type of location"
        },
        "key_subjects": {
          "type": "array",
          "items": { "type": "string" },
          "minItems": 3,
          "maxItems": 6
        },
        "dominant_colors": {
          "type": "array",
          "items": { "type": "string" },
          "minItems": 3,
          "maxItems": 5
        },
        "atmosphere": {
          "type": "string",
          "description": "Overall mood conveyed by visuals"
        },
        "lighting": {
          "type": "string",
          "description": "Lighting characteristics"
        }
      },
      "required": ["setting", "time_of_day", "weather", "location_type",
                   "key_subjects", "dominant_colors", "atmosphere", "lighting"]
    },
    "motion_elements": {
      "type": "object",
      "properties": {
        "detected_elements": {
          "type": "array",
          "items": {
            "type": "object",
            "properties": {
              "element": { "type": "string" },
              "motion_type": { "type": "string" },
              "location": { "type": "string" }
            },
            "required": ["element", "motion_type", "location"]
          },
          "minItems": 3,
          "maxItems": 8
        },
        "all_potential_elements": {
          "type": "array",
          "items": { "type": "string" },
          "minItems": 5,
          "maxItems": 20
        }
      },
      "required": ["detected_elements", "all_potential_elements"]
    },
    "audio_atmosphere": {
      "type": "object",
      "properties": {
        "implied_sounds": {
          "type": "array",
          "items": { "type": "string" },
          "minItems": 2,
          "maxItems": 5
        },
        "silence_level": {
          "type": "string",
          "enum": ["very quiet", "quiet", "moderate", "busy", "loud"]
        },
        "emotional_intensity": {
          "type": "string",
          "enum": ["serene", "calm", "neutral", "tense", "dramatic", "intense"]
        },
        "temporal_feel": {
          "type": "string",
          "enum": ["frozen/still", "slow/meditative", "gentle/flowing",
                   "moderate/steady", "dynamic/energetic", "urgent/fast"]
        },
        "warmth": {
          "type": "string",
          "enum": ["cold/clinical", "cool", "neutral", "warm",
                   "very warm/intimate"]
        },
        "era_feel": { "type": "string" },
        "cultural_context": { "type": "string" },
        "suggested_music_genres": {
          "type": "array",
          "items": { "type": "string" },
          "minItems": 2,
          "maxItems": 3
        },
        "musical_texture": { "type": "string" }
      },
      "required": ["implied_sounds", "silence_level", "emotional_intensity",
                   "temporal_feel", "warmth", "era_feel", "cultural_context",
                   "suggested_music_genres", "musical_texture"]
    }
  },
  "required": ["environment", "motion_elements", "audio_atmosphere"]
}
```

### cc_motion_elements_display_schema.json (NEW)

Single panel for detected elements only (no "planned" panel like OMS).

```json
{
  "type": "object",
  "_ux.display": "passthrough",
  "properties": {
    "detected": {
      "type": "array",
      "_ux": {
        "display": "visible",
        "display_label": "Detected Motion Elements",
        "render_as": "content-panel"
      },
      "items": {
        "type": "object",
        "_ux": {
          "display": "visible",
          "render_as": "card",
          "nudges": ["index-badge"],
          "selectable": true
        },
        "properties": {
          "element": {
            "type": "string",
            "_ux": {
              "display": true,
              "display_label": "Element",
              "render_as": "card-title",
              "highlight": true,
              "highlight_color": "#4ECDC4"
            }
          },
          "motion_type": {
            "type": "string",
            "_ux.display": true,
            "_ux.display_label": "Motion Type"
          },
          "location": {
            "type": "string",
            "_ux.display": true,
            "_ux.display_label": "Location"
          }
        }
      }
    }
  }
}
```

### cc_video_generation_display_schema.json (NEW)

Display schema for video generation with Leonardo and Sora 2 tabs.

```json
{
  "type": "object",
  "_ux.display": "passthrough",
  "properties": {
    "prompts": {
      "type": "object",
      "_ux": {
        "display": "visible",
        "render_as": "tabs"
      },
      "properties": {
        "leonardo": {
          "type": "object",
          "_ux": {
            "display": "visible",
            "tab_label": "Leonardo",
            "render_as": "tab.media",
            "provider": "leonardo",
            "input_schema": {
              "type": "object",
              "_ux": {
                "layout": "grid",
                "layout_columns": 3,
                "layout_columns_sm": 2
              },
              "properties": {
                "_text": {
                  "type": "string",
                  "title": "Prompt",
                  "destination_field": "prompt",
                  "_ux": {
                    "input_type": "textarea",
                    "col_span": "full",
                    "rows": 4,
                    "source_field": "positive_prompt"
                  }
                },
                "negative_prompt": {
                  "type": "string",
                  "title": "Negative Prompt",
                  "_ux": {
                    "input_type": "textarea",
                    "col_span": "full",
                    "rows": 2,
                    "source_field": "negative_prompt"
                  }
                },
                "motion_strength": {
                  "type": "integer",
                  "title": "Motion Strength",
                  "minimum": 1,
                  "maximum": 10,
                  "default": 5,
                  "step": 1,
                  "_ux": {
                    "input_type": "slider"
                  }
                },
                "duration": {
                  "type": "string",
                  "title": "Duration",
                  "enum": ["5", "10"],
                  "default": "5",
                  "enum_labels": {
                    "5": "5 seconds",
                    "10": "10 seconds"
                  },
                  "_ux": {
                    "input_type": "select"
                  }
                }
              }
            }
          },
          "properties": {
            "positive_prompt": {
              "type": "string",
              "_ux.display": false
            },
            "negative_prompt": {
              "type": "string",
              "_ux.display": false
            },
            "motion_strength": {
              "type": "integer",
              "_ux.display": true,
              "_ux.display_label": "Motion"
            }
          }
        },
        "sora": {
          "type": "object",
          "_ux": {
            "display": "visible",
            "tab_label": "Sora 2",
            "render_as": "tab.media",
            "provider": "sora",
            "input_schema": {
              "type": "object",
              "_ux": {
                "layout": "grid",
                "layout_columns": 3,
                "layout_columns_sm": 2
              },
              "properties": {
                "_text": {
                  "type": "string",
                  "title": "Prompt",
                  "destination_field": "prompt",
                  "_ux": {
                    "input_type": "textarea",
                    "col_span": "full",
                    "rows": 4,
                    "source_field": "prompt"
                  }
                },
                "duration": {
                  "type": "string",
                  "title": "Duration",
                  "enum": ["5", "10", "15", "20"],
                  "default": "5",
                  "enum_labels": {
                    "5": "5 seconds",
                    "10": "10 seconds",
                    "15": "15 seconds",
                    "20": "20 seconds"
                  },
                  "_ux": {
                    "input_type": "select"
                  }
                },
                "resolution": {
                  "type": "string",
                  "title": "Resolution",
                  "enum": ["480p", "720p", "1080p"],
                  "default": "720p",
                  "_ux": {
                    "input_type": "select"
                  }
                },
                "aspect_ratio": {
                  "type": "string",
                  "title": "Aspect Ratio",
                  "enum": ["16:9", "9:16", "1:1"],
                  "default": "9:16",
                  "enum_labels": {
                    "16:9": "16:9 (Landscape)",
                    "9:16": "9:16 (Portrait)",
                    "1:1": "1:1 (Square)"
                  },
                  "_ux": {
                    "input_type": "select"
                  }
                }
              }
            }
          },
          "properties": {
            "prompt": {
              "type": "string",
              "_ux.display": false
            },
            "style_notes": {
              "type": "string",
              "_ux.display": true,
              "_ux.display_label": "Style"
            }
          }
        }
      }
    }
  }
}
```

### cc_video_prompts_schema.json (MODIFIED)

Update to output Leonardo and Sora prompts instead of Motion 2.0 and MJ.

```json
{
  "type": "object",
  "properties": {
    "prompts": {
      "type": "object",
      "properties": {
        "leonardo": {
          "type": "object",
          "properties": {
            "positive_prompt": {
              "type": "string",
              "description": "Motion description focusing on selected elements"
            },
            "negative_prompt": {
              "type": "string",
              "description": "Elements to keep static, artifacts to avoid"
            },
            "motion_strength": {
              "type": "integer",
              "minimum": 1,
              "maximum": 10,
              "description": "Suggested motion intensity (1=subtle, 10=dynamic)"
            }
          },
          "required": ["positive_prompt", "negative_prompt", "motion_strength"]
        },
        "sora": {
          "type": "object",
          "properties": {
            "prompt": {
              "type": "string",
              "description": "Natural language video description for Sora"
            },
            "style_notes": {
              "type": "string",
              "description": "Style guidance notes"
            }
          },
          "required": ["prompt", "style_notes"]
        }
      },
      "required": ["leonardo", "sora"]
    }
  },
  "required": ["prompts"]
}
```

---

## Input Image for img2vid - Design Proposal

### Problem Statement

The `media.generate` module needs access to the source image for `img2vid`
operations. Currently, `sub_action.py` expects `source_image` in params:

```python
elif request.action_type == "img2vid":
    source_image = request.params.get("source_image", "")
    future = loop.run_in_executor(
        executor,
        lambda: method(source_image, prompt, params, progress_callback=...)
    )
```

The challenge is how to:
1. Pass the image data from workflow state to the module
<!--you can pass the value from state to module in the same way we do that everywhere, you will find that more than enough examples of doing this throghout the cc workflow.-->
2. Have the module extract what it needs
<!--what does it need to extact?-->
3. Have the WebUI include it in generation requests
<!--same way do it in step 3, i am confused now, what are yyou trying to rerieve and send?-->

### Data Available in State

From `generated_content` collection and step 3 output:

```python
state.selected_image_data = {
    "content_id": "gc_019bd7b10df97f2b8ce27cdbaa688d37",
    "url": "/workflow/wf_xxx/media/gc_xxx.jpg",  # Server-relative URL
    "metadata_id": "cgm_xxx",
    "prompt_key": "midjourney"
}
```

From `generated_content` DB record:

```python
{
    "generated_content_id": "gc_xxx",
    "local_path": "/mnt/g/wm/images/cgm_xxx_gc_xxx_0.jpg",
    "provider_url": "https://tempfile.aiquickdraw.com/...",
    "extension": "jpg",
    "content_type": "image"
}
```

### Proposal A: Full Object Input (Recommended)

Pass the full `selected_image_data` object to `media.generate`. The module
decides what to use.

**Workflow JSON:**
```json
{
  "module_id": "media.generate",
  "inputs": {
    "prompts": "{{ state.video_prompts.prompts }}",
    "input_image": "{{ state.selected_image_data }}",
    "schema": { "$ref": "schemas/cc_video_generation_display_schema.json" },
    "title": "Generate and Select Videos"
  },
  "sub_actions": [
    {
      "id": "generate",
      "action_type": "img2vid",
      ...
    }
  ]
}
```

**Module Changes (media/generate.py):**
```python
@property
def inputs(self) -> List[ModuleInput]:
    return [
        ModuleInput(
            name="prompts",
            type="object",
            required=True,
            description="Prompts grouped by provider"
        ),
        ModuleInput(
            name="input_image",
            type="object",
            required=False,
            description="Source image for img2vid/img2img operations. "
                        "Object with content_id, url, metadata_id, prompt_key"
        ),
        ...
    ]
```

**Interaction Request:**
```python
def get_interaction_request(self, inputs, context):
    input_image = inputs.get('input_image')

    return InteractionRequest(
        interaction_type=InteractionType.MEDIA_GENERATION,
        display_data={
            "data": prompts,
            "schema": schema,
            "sub_actions": sub_actions,
            "input_image": input_image,  # Pass to WebUI
            ...
        }
    )
```

**WebUI Behavior:**

When user clicks "Generate" for img2vid:

<!--i dont like this solution, i think best option is to add previous
interactions to context here: backend/server/workflow/interaction.py, it his
much more versitile and make sense in significant way as data of an interaction
may need to be used by later stage, this will help future modules to have data
like this. -->

1. WebUI reads `input_image` from interaction display_data
2. Includes `source_image: input_image.url` in sub-action request params
3. Server receives and passes to provider

**Sub-action Request Flow:**
```
WebUI sends:
{
    "provider": "leonardo",
    "action_type": "img2vid",
    "prompt_id": "leonardo",
    "params": {
        "prompt": "...",
        "motion_strength": 5,
        "source_image": "/workflow/wf_xxx/media/gc_xxx.jpg"
    }
}

Server (sub_action.py) extracts source_image from params
Provider receives source_image URL
```

### Proposal B: Enriched Object with Local Path

If providers need local file path (for base64 conversion), enrich the object:

**Add to media.generate module:**
```python
def get_interaction_request(self, inputs, context):
    input_image = inputs.get('input_image')

    # Enrich with local path from DB if needed
    if input_image and input_image.get('content_id'):
        content_record = db.content_repo.get_content(
            input_image['content_id']
        )
        if content_record:
            input_image['local_path'] = content_record.local_path
            input_image['provider_url'] = content_record.provider_url
```

**Enriched Object:**
```python
{
    "content_id": "gc_xxx",
    "url": "/workflow/wf_xxx/media/gc_xxx.jpg",
    "local_path": "/mnt/g/wm/images/cgm_xxx_gc_xxx_0.jpg",
    "provider_url": "https://tempfile.aiquickdraw.com/...",
    "metadata_id": "cgm_xxx",
    "prompt_key": "midjourney"
}
```

**Provider can use:**
- `url` - Server-relative URL (for API that accepts URLs)
- `local_path` - For base64 encoding
- `provider_url` - Original CDN URL

### End-to-End Data Flow

```
Step 3: Image Generation & Selection
├── User selects image
├── media.generate outputs:
│   └── selected_content = {
│         content_id: "gc_xxx",
│         url: "/workflow/wf_xxx/media/gc_xxx.jpg",
│         metadata_id: "cgm_xxx",
│         prompt_key: "midjourney"
│       }
└── Saved to state.selected_image_data

Step 4: Video Generation
├── analyze_image
│   └── Reads state.selected_image_data.local_path (needs DB lookup or
│       enrichment)
│   └── Sends base64 to LLM for analysis
│
├── select_motion_elements
│   └── User picks elements from analysis
│
├── generate_video_prompts
│   └── LLM generates prompts using selected elements
│
└── generate_and_select_videos
    ├── Module receives:
    │   ├── prompts: state.video_prompts.prompts
    │   └── input_image: state.selected_image_data
    │
    ├── Module enriches input_image with local_path (Proposal B)
    │
    ├── WebUI receives in display_data:
    │   └── input_image: { content_id, url, local_path, ... }
    │
    ├── User clicks "Generate" on Leonardo tab
    │
    ├── WebUI sends sub-action request:
    │   └── params: { prompt, source_image: input_image.url, ... }
    │
    ├── Server (sub_action.py):
    │   └── Extracts source_image from params
    │   └── Calls provider.img2vid(source_image, prompt, params)
    │
    └── Provider:
        └── Downloads/reads source_image
        └── Calls video generation API
        └── Returns video URLs
```

### Recommendation

**Use Proposal B (Enriched Object):**
- Full flexibility for different provider needs
- Module handles complexity, WebUI just passes URL
- Local path available for providers that need base64
- Server URL available for providers that accept URLs
- Provider URL available as fallback

**Implementation Order:**
1. Add `input_image` input to `media.generate` module
2. Add enrichment logic to fetch local_path from DB
3. Update WebUI to include `source_image` in params for img2vid
4. Verify Leonardo provider handles the URL correctly

---

## Files to Create/Modify

### New Files

| File | Purpose |
|------|---------|
| `prompts/cc_role_visual_analyst.txt` | Role for image analysis |
| `prompts/cc_image_analysis_system.txt` | System instructions |
| `prompts/cc_image_analysis_user.txt` | User prompt |
| `schemas/cc_image_analysis_schema.json` | LLM output schema |
| `schemas/cc_motion_elements_display_schema.json` | Motion selection UI |
| `schemas/cc_video_generation_display_schema.json` | Video generation UI |

### Modified Files

| File | Change |
|------|--------|
| `step.json` | Replace modules with new flow |
| `prompts/cc_video_prompts_user.txt` | Add motion elements context |
| `schemas/cc_video_prompts_schema.json` | Leonardo + Sora structure |
| `../8_workflow_summary/prompts/cc_workflow_summary.txt` | Update video section |
| `backend/server/modules/media/generate.py` | Add `input_image` input |

---

## Questions for Review

### 1. Image File Input Type

For `analyze_image`, I proposed `"type": "image_file"` which would signal the
server to read the file and convert to base64. Does this type exist in the
input resolver, or does it need to be added?

**Alternative:** Use a separate `io.read_file` module before the LLM call to
load the image as base64 into state.

<!-- added comment in section-->

### 2. Sora 2 Provider

Is there an existing Sora 2 provider in the codebase, or does it need to be
created? I didn't find one in `/backend/providers/media/`.

<!--there's openai provider, you can use that.-->

### 3. Leonardo Video Capabilities

Which Leonardo video generation method should be used? Looking at the provider,
there's `img2vid` but I need to confirm the exact parameters and capabilities.

<!-- docs: https://docs.leonardo.ai/reference/createimagetovideogeneration -->

### 4. Workflow Summary Update

The current `cc_workflow_summary.txt` references `motion_2_0` and
`midjourney_animate`. Should I update it to reference `leonardo` and `sora`,
or keep the old structure for backwards compatibility with existing runs?

<!-- yes -->

### 5. Step Renaming

Should step 4 be renamed from "video_prompts" to something like
"video_generation" since it now includes actual video generation, not just
prompt generation?

<!-- sure -->
