# CC Workflow Step 4: Video Generation Enhancement

## Summary

Enhance CC workflow step 4 to add image-based motion detection, motion element
selection, and video generation UX. This transforms step 4 from a prompt review
step into a full video production step.

**Step renamed:** `video_prompts` → `video_generation`

## Decisions Summary

| Item | Decision |
|------|----------|
| Image Analysis Scope | Full OMS schema (environment, motion, audio) |
| Video Providers | Leonardo (full) + Sora 2 (via OpenAI provider) |
| Motion Element UI | Single panel "Detected Motion Elements" |
| Image input type | `"type": "image"` (existing, handles file paths) |
| Interaction data access | Add previous interactions to context |
| Step name | Rename to `video_generation` |

---

## Proposed Flow

```
1. analyze_image             → LLM vision analyzes selected image
2. select_motion_elements    → User picks elements to animate
3. generate_video_prompts    → LLM generates prompts using selected elements
4. generate_and_select_videos → media.generate with img2vid
```

---

## Module Specifications

### 1. analyze_image (NEW)

```json
{
  "module_id": "api.llm",
  "inputs": {
    "resolver_schema": {
      "type": "object",
      "properties": {
        "input": { "resolver": "server" },
        "system": { "resolver": "server" },
        "metadata": { "resolver": "server" }
      }
    },
    "input": [
      {
        "content": "{{ state.selected_image_data.local_path }}",
        "type": "image"
      },
      { "$ref": "prompts/cc_image_analysis_user.txt", "type": "text" }
    ],
    "output_schema": {
      "$ref": "schemas/cc_image_analysis_schema.json",
      "type": "json"
    },
    "metadata": {
      "step_id": "{{ step.step_id }}"
    },
    "system": [
      {
        "$ref": "prompts/cc_role_visual_analyst.txt",
        "type": "text",
        "cache_ttl": 10800
      },
      {
        "$ref": "prompts/cc_image_analysis_system.txt",
        "type": "text",
        "cache_ttl": 10800
      }
    ],
    "provider": "openai"
  },
  "outputs_to_state": {
    "response": "image_analysis"
  },
  "name": "analyze_image"
}
```

**Note:** Using `"type": "image"` with a file path. The OpenAI provider's
`_encode_image` function handles reading the file and converting to base64.
This is validated in OMS workflow.

### 2. select_motion_elements (NEW)

```json
{
  "module_id": "user.select",
  "inputs": {
    "resolver_schema": {
      "type": "object",
      "properties": {
        "data": { "resolver": "server" }
      }
    },
    "data": {
      "detected": "{{ state.image_analysis.motion_elements.detected_elements }}"
    },
    "schema": {
      "$ref": "schemas/cc_motion_elements_display_schema.json",
      "type": "json"
    },
    "prompt": "Select motion elements to animate in your video",
    "multi_select": true,
    "mode": "select"
  },
  "outputs_to_state": {
    "selected_indices": "selected_motion_element_indices",
    "selected_data": "selected_motion_elements"
  },
  "name": "select_motion_elements"
}
```

### 3. generate_video_prompts (MODIFIED)

```json
{
  "module_id": "api.llm",
  "inputs": {
    "resolver_schema": {
      "type": "object",
      "properties": {
        "input": { "resolver": "server" },
        "system": { "resolver": "server" },
        "metadata": { "resolver": "server" }
      }
    },
    "ai_config": {},
    "input": {
      "$ref": "prompts/cc_video_prompts_user.txt",
      "type": "jinja2"
    },
    "output_schema": {
      "$ref": "schemas/cc_video_prompts_schema.json",
      "type": "json"
    },
    "metadata": {
      "step_id": "{{ step.step_id }}"
    },
    "system": [
      {
        "$ref": "prompts/cc_role_motion_engineer.txt",
        "type": "text",
        "cache_ttl": 10800
      },
      {
        "$ref": "prompts/cc_video_prompts_system.txt",
        "type": "text",
        "cache_ttl": 10800
      }
    ],
    "provider": "openai"
  },
  "outputs_to_state": {
    "response": "video_prompts"
  },
  "name": "generate_video_prompts"
}
```

### 4. generate_and_select_videos (NEW)

```json
{
  "module_id": "media.generate",
  "inputs": {
    "resolver_schema": {
      "type": "object",
      "properties": {
        "prompts": { "resolver": "server" }
      }
    },
    "prompts": "{{ state.video_prompts.prompts }}",
    "schema": {
      "$ref": "schemas/cc_video_generation_display_schema.json",
      "type": "json"
    },
    "title": "Generate and Select Videos"
  },
  "outputs_to_state": {
    "selected_content_id": "selected_video_id",
    "selected_content": "selected_video_data",
    "generations": "video_generations"
  },
  "sub_actions": [
    {
      "id": "generate",
      "label": "Generate Videos",
      "action_type": "img2vid",
      "loading_label": "Generating video...",
      "result_key": "generations"
    }
  ],
  "retryable": {
    "default_option": "continue",
    "options": [
      {
        "id": "continue",
        "mode": "continue",
        "label": "Accept and continue"
      },
      {
        "id": "retry_prompts",
        "mode": "retry",
        "label": "Regenerate video prompts",
        "shortcut": "r",
        "target_module": "generate_video_prompts",
        "feedback": {
          "enabled": true,
          "prompt": "What should be different?",
          "default_message": "Please generate different video prompts."
        }
      },
      {
        "id": "change_motion_elements",
        "mode": "jump",
        "label": "Change motion elements",
        "target_step": "video_generation",
        "target_module": "select_motion_elements"
      },
      {
        "id": "change_image",
        "mode": "jump",
        "label": "Go back to image selection",
        "target_step": "image_prompts",
        "target_module": "generate_and_select_images"
      }
    ]
  },
  "name": "generate_and_select_videos"
}
```

---

## Prompts

### cc_role_visual_analyst.txt (NEW)

```
You are a visual analyst specializing in image-to-video production. Your role
is to analyze static images and identify elements suitable for subtle animation
in cute pet videos.

You have expertise in:
- Identifying animatable environmental elements (light, particles, fabric)
- Understanding motion physics for realistic subtle animation
- Recognizing atmospheric elements that enhance cozy aesthetics
- Analyzing audio characteristics implied by visual scenes

Your analysis directly informs video generation prompts, so accuracy and
specificity are critical.
```

### cc_image_analysis_system.txt (NEW)

```
# IMAGE ANALYSIS FOR PET VIDEO PRODUCTION

Analyze the provided image to extract environment details, motion elements, and
audio atmosphere for a cute pet video production.

## PART 1: ENVIRONMENT ANALYSIS

Extract scene details for narrative and caption generation.

**Identify:**
- Setting: Brief, specific description of the scene
- Time of day: dawn, morning, noon, afternoon, dusk, night
- Weather: If visible or implied (sunny, cloudy, rain, etc.)
- Location type: indoor, outdoor, specific venue type
- Key subjects: 3-6 main elements visible (include the pet!)
- Dominant colors: 3-5 colors defining the palette
- Atmosphere: The mood conveyed by the visuals
- Lighting: How light behaves in the scene

**Rules:**
- Only describe what is VISIBLE
- Be specific (e.g., "sunny living room with large windows" not "indoor")
- For key_subjects, prioritize the pet and their immediate environment

## PART 2: MOTION ELEMENT ANALYSIS

Identify elements for subtle animation suitable for short pet videos.

**Look for:**
- Light effects (sunbeams, lamp glow, reflections)
- Particles (dust motes, floating fur/hair)
- Fabric (pet bed, blankets, curtains)
- Environmental (plants swaying, water in bowl)
- Pet-adjacent elements (toys, food bowl steam)

**Constraints:**
- The PET must remain STATIC - do not suggest animating the pet itself
- Camera must remain STATIC - no zoom, pan, or movement
- Focus on SUBTLE, LOOPING motion suitable for 5-10 second clips
- Only identify elements CLEARLY VISIBLE in the image

**detected_elements (3-8 items):**
Best candidates for animation. For each provide:
- element: What could be animated
- motion_type: Type of motion (e.g., "gentle sway", "shimmer")
- location: Where in the frame

**all_potential_elements (5-20 items):**
Complete list of everything that COULD move (for negative prompts).
Include detected elements plus any others that might drift unintentionally.

## PART 3: AUDIO ATMOSPHERE ANALYSIS

Analyze for music generation context.

**Identify:**
- implied_sounds: 2-5 sounds the scene suggests
- silence_level: very quiet / quiet / moderate / busy / loud
- emotional_intensity: serene / calm / neutral / tense / dramatic / intense
- temporal_feel: frozen/still / slow/meditative / gentle/flowing / moderate
- warmth: cold/clinical / cool / neutral / warm / very warm/intimate
- era_feel: Time period evoked (e.g., "cozy modern home")
- cultural_context: Geographic/cultural context if apparent
- suggested_music_genres: 2-3 genres that would complement
- musical_texture: Suggested texture (e.g., "warm and gentle")
```

### cc_image_analysis_user.txt (NEW)

```
Analyze this image of a pet scene and extract:
1. Environment details (setting, time, weather, location, subjects, colors,
   atmosphere, lighting)
2. Motion elements for animation (detected candidates and all potential
   elements)
3. Audio atmosphere (implied sounds, emotional qualities, music suggestions)

Focus on elements that would enhance a cute, heartwarming pet video.
```

### cc_video_prompts_system.txt (MODIFIED)

```
Generate video/motion prompts for the selected scene. Create prompts optimized
for Leonardo Motion and Sora 2.

## Context
You have:
- A scene concept with narrative and visual moments
- Image analysis with detected motion elements
- User-selected elements to animate
- Elements to block in negative prompts

## CRITICAL: Use Selected Motion Elements

The user has specifically selected which elements to animate. Your prompts
MUST focus on animating ONLY these selected elements. All other potentially
movable elements should be included in the negative prompt to keep them static.

## Critical: Negative Prompts
Negative prompts are ESSENTIAL for quality video generation. They prevent:
- Morphing/deformation artifacts
- Unnatural movement
- Flickering and inconsistency
- Anatomical errors during motion

### Standard Negative Elements for Pet Videos
Always include these in negative prompts:
- morphing, deformation, distortion
- extra limbs, missing limbs, fused limbs
- blurry, out of focus, low quality
- flickering, strobing, jittering
- unnatural movement, robotic motion
- face distortion, eye glitches
- sudden scene changes, jump cuts

### Pet-Specific Negative Elements
- extra legs, missing legs, leg fusion
- tail splitting, multiple tails
- ear morphing, floating ears
- eye color change, pupil distortion
- fur texture changing, bald patches appearing
- body stretching, size fluctuation

### Block Unselected Motion Elements
Add ALL elements from `all_potential_elements` that are NOT in the user's
selected list to the negative prompt. This ensures only the intended elements
move.

## Model-Specific Guidelines

### Leonardo Motion
Leonardo Motion excels at subtle, natural movements from a source image.

**Positive Prompt Structure:**
- Focus on the selected motion elements
- Describe the type of motion (gentle sway, shimmer, drift)
- Keep pet and camera static
- Specify atmospheric motion if selected

**Negative Prompt:** Must include:
- All unselected potential motion elements
- Standard artifacts to avoid
- Pet-specific negative elements

**Docs:** https://docs.leonardo.ai/reference/createimagetovideogeneration

### Sora 2
Sora 2 creates high-quality video from images with natural motion.

**Prompt Structure:**
- Natural language description of the scene with motion
- Focus on selected elements
- Emphasize what should remain static
- Describe the mood and atmosphere

**Style Notes:** Include guidance on:
- Visual style consistency
- Motion intensity level
- Lighting preservation

## Motion Ideas for Pets (Environmental Only)

**Since pets must remain static, focus on:**
- Gentle breeze through nearby curtains/fabric
- Dust motes in sunbeams
- Subtle shadow movement from clouds
- Steam from food/water bowl
- Plant/leaf movement
- Light shimmer on reflective surfaces
- Fabric settling/breathing effect on pet bed
```

### cc_video_prompts_user.txt (MODIFIED)

```
Generate video/motion prompts for this pet scene:

## Scene Details

**Title:** {{ state.selected_scene.title }}
**Narrative:** {{ state.selected_scene.narrative }}
**Pet Behavior Focus:** {{ state.selected_scene.pet_behavior }}
**Setting:** {{ state.selected_scene.setting }}
**Emotional Arc:** {{ state.selected_scene.emotional_arc }}

**Key Visual Moments:**
{% for moment in state.selected_scene.visual_moments %}
- {{ moment }}
{% endfor %}

## Context

**Pet Type:** {{ state.pet_type_selection.label }}

**Visual Tone:**
- Mood: {{ state.aesthetic_selection.visual_tone.mood }}
- Lighting: {{ state.aesthetic_selection.visual_tone.lighting }}
- Energy: {{ state.aesthetic_selection.visual_tone.energy }}

**Duration Target:** {{ state.duration_selection.video_duration }} seconds

{% if state.user_direction %}
**User Direction:** {{ state.user_direction }}
{% endif %}

## Image Analysis

**Environment:**
- Setting: {{ state.image_analysis.environment.setting }}
- Atmosphere: {{ state.image_analysis.environment.atmosphere }}
- Lighting: {{ state.image_analysis.environment.lighting }}

## IMPORTANT: Selected Motion Elements

The user has specifically chosen these elements to animate. Your prompts MUST
animate ONLY these elements:

{% for element in state.selected_motion_elements %}
- **{{ element.element }}**: {{ element.motion_type }} ({{ element.location }})
{% endfor %}

## Elements to Block in Negative Prompts

All of these elements should be kept STATIC. Include them in negative prompts:
{{ state.image_analysis.motion_elements.all_potential_elements | join(', ') }}

Generate Leonardo Motion and Sora 2 video prompts that bring this scene to life
with natural, subtle motion. The pet and all unselected elements must remain
completely static - only animate the selected environmental elements.
```

---

## Schemas

### cc_image_analysis_schema.json (NEW)

```json
{
  "type": "object",
  "properties": {
    "environment": {
      "type": "object",
      "properties": {
        "setting": {
          "type": "string",
          "description": "Brief description of the scene setting"
        },
        "time_of_day": {
          "type": "string",
          "description": "Time of day depicted"
        },
        "weather": {
          "type": "string",
          "description": "Weather conditions if apparent"
        },
        "location_type": {
          "type": "string",
          "description": "Type of location"
        },
        "key_subjects": {
          "type": "array",
          "items": { "type": "string" },
          "minItems": 3,
          "maxItems": 6
        },
        "dominant_colors": {
          "type": "array",
          "items": { "type": "string" },
          "minItems": 3,
          "maxItems": 5
        },
        "atmosphere": {
          "type": "string",
          "description": "Overall mood conveyed by visuals"
        },
        "lighting": {
          "type": "string",
          "description": "Lighting characteristics"
        }
      },
      "required": ["setting", "time_of_day", "weather", "location_type",
                   "key_subjects", "dominant_colors", "atmosphere", "lighting"],
      "additionalProperties": false
    },
    "motion_elements": {
      "type": "object",
      "properties": {
        "detected_elements": {
          "type": "array",
          "items": {
            "type": "object",
            "properties": {
              "element": { "type": "string" },
              "motion_type": { "type": "string" },
              "location": { "type": "string" }
            },
            "required": ["element", "motion_type", "location"],
            "additionalProperties": false
          },
          "minItems": 3,
          "maxItems": 8
        },
        "all_potential_elements": {
          "type": "array",
          "items": { "type": "string" },
          "minItems": 5,
          "maxItems": 20
        }
      },
      "required": ["detected_elements", "all_potential_elements"],
      "additionalProperties": false
    },
    "audio_atmosphere": {
      "type": "object",
      "properties": {
        "implied_sounds": {
          "type": "array",
          "items": { "type": "string" },
          "minItems": 2,
          "maxItems": 5
        },
        "silence_level": {
          "type": "string",
          "enum": ["very quiet", "quiet", "moderate", "busy", "loud"]
        },
        "emotional_intensity": {
          "type": "string",
          "enum": ["serene", "calm", "neutral", "tense", "dramatic", "intense"]
        },
        "temporal_feel": {
          "type": "string",
          "enum": ["frozen/still", "slow/meditative", "gentle/flowing",
                   "moderate/steady", "dynamic/energetic", "urgent/fast"]
        },
        "warmth": {
          "type": "string",
          "enum": ["cold/clinical", "cool", "neutral", "warm",
                   "very warm/intimate"]
        },
        "era_feel": { "type": "string" },
        "cultural_context": { "type": "string" },
        "suggested_music_genres": {
          "type": "array",
          "items": { "type": "string" },
          "minItems": 2,
          "maxItems": 3
        },
        "musical_texture": { "type": "string" }
      },
      "required": ["implied_sounds", "silence_level", "emotional_intensity",
                   "temporal_feel", "warmth", "era_feel", "cultural_context",
                   "suggested_music_genres", "musical_texture"],
      "additionalProperties": false
    }
  },
  "required": ["environment", "motion_elements", "audio_atmosphere"],
  "additionalProperties": false
}
```

### cc_motion_elements_display_schema.json (NEW)

```json
{
  "type": "object",
  "_ux.display": "passthrough",
  "properties": {
    "detected": {
      "type": "array",
      "_ux": {
        "display": "visible",
        "display_label": "Detected Motion Elements",
        "render_as": "content-panel"
      },
      "items": {
        "type": "object",
        "_ux": {
          "display": "visible",
          "render_as": "card",
          "nudges": ["index-badge"],
          "selectable": true
        },
        "properties": {
          "element": {
            "type": "string",
            "_ux": {
              "display": true,
              "display_label": "Element",
              "render_as": "card-title",
              "highlight": true,
              "highlight_color": "#4ECDC4"
            }
          },
          "motion_type": {
            "type": "string",
            "_ux.display": true,
            "_ux.display_label": "Motion Type"
          },
          "location": {
            "type": "string",
            "_ux.display": true,
            "_ux.display_label": "Location"
          }
        }
      }
    }
  }
}
```

### cc_video_prompts_schema.json (MODIFIED)

```json
{
  "type": "object",
  "properties": {
    "scene_title": {
      "type": "string",
      "description": "The scene this video prompt is for"
    },
    "motion_summary": {
      "type": "string",
      "description": "Brief description of the primary motion in this video"
    },
    "prompts": {
      "type": "object",
      "properties": {
        "leonardo": {
          "type": "object",
          "properties": {
            "positive_prompt": {
              "type": "string",
              "description": "Motion prompt focusing on selected elements only"
            },
            "negative_prompt": {
              "type": "string",
              "description": "REQUIRED: Unselected elements + artifacts to avoid"
            },
            "motion_strength": {
              "type": "string",
              "enum": ["subtle", "moderate", "dynamic"],
              "description": "Recommended motion intensity level"
            },
            "motion_notes": {
              "type": "string",
              "description": "Brief notes on motion elements emphasized"
            }
          },
          "required": ["positive_prompt", "negative_prompt", "motion_strength",
                       "motion_notes"],
          "additionalProperties": false
        },
        "sora": {
          "type": "object",
          "properties": {
            "prompt": {
              "type": "string",
              "description": "Natural language video description for Sora 2"
            },
            "style_notes": {
              "type": "string",
              "description": "Style guidance notes"
            }
          },
          "required": ["prompt", "style_notes"],
          "additionalProperties": false
        }
      },
      "required": ["leonardo", "sora"],
      "additionalProperties": false
    }
  },
  "required": ["scene_title", "motion_summary", "prompts"],
  "additionalProperties": false
}
```

### cc_video_generation_display_schema.json (NEW)

```json
{
  "type": "object",
  "_ux.display": "passthrough",
  "properties": {
    "leonardo": {
      "type": "object",
      "_ux": {
        "display": "visible",
        "tab_label": "Leonardo",
        "render_as": "tab.media",
        "provider": "leonardo",
        "input_schema": {
          "type": "object",
          "_ux": {
            "layout": "grid",
            "layout_columns": 3,
            "layout_columns_sm": 2
          },
          "properties": {
            "_text": {
              "type": "string",
              "title": "Prompt",
              "destination_field": "prompt",
              "_ux": {
                "input_type": "textarea",
                "col_span": "full",
                "rows": 4,
                "source_field": "positive_prompt"
              }
            },
            "negative_prompt": {
              "type": "string",
              "title": "Negative Prompt",
              "_ux": {
                "input_type": "textarea",
                "col_span": "full",
                "rows": 2,
                "source_field": "negative_prompt"
              }
            },
            "motion_strength": {
              "type": "integer",
              "title": "Motion Strength",
              "minimum": 1,
              "maximum": 10,
              "default": 5,
              "step": 1,
              "_ux": {
                "input_type": "slider"
              }
            }
          }
        }
      },
      "properties": {
        "positive_prompt": {
          "type": "string",
          "_ux.display": false
        },
        "negative_prompt": {
          "type": "string",
          "_ux.display": false
        },
        "motion_strength": {
          "type": "string",
          "_ux.display": true,
          "_ux.display_label": "Motion"
        },
        "motion_notes": {
          "type": "string",
          "_ux.display": true,
          "_ux.display_label": "Notes",
          "_ux.highlight": true,
          "_ux.highlight_color": "#7B68EE"
        }
      }
    },
    "sora": {
      "type": "object",
      "_ux": {
        "display": "visible",
        "tab_label": "Sora 2",
        "render_as": "tab.media",
        "provider": "openai",
        "input_schema": {
          "type": "object",
          "_ux": {
            "layout": "grid",
            "layout_columns": 3,
            "layout_columns_sm": 2
          },
          "properties": {
            "_text": {
              "type": "string",
              "title": "Prompt",
              "destination_field": "prompt",
              "_ux": {
                "input_type": "textarea",
                "col_span": "full",
                "rows": 4,
                "source_field": "prompt"
              }
            },
            "duration": {
              "type": "string",
              "title": "Duration",
              "enum": ["5", "10", "15", "20"],
              "default": "5",
              "enum_labels": {
                "5": "5 seconds",
                "10": "10 seconds",
                "15": "15 seconds",
                "20": "20 seconds"
              },
              "_ux": {
                "input_type": "select"
              }
            },
            "resolution": {
              "type": "string",
              "title": "Resolution",
              "enum": ["480p", "720p", "1080p"],
              "default": "720p",
              "_ux": {
                "input_type": "select"
              }
            },
            "aspect_ratio": {
              "type": "string",
              "title": "Aspect Ratio",
              "enum": ["16:9", "9:16", "1:1"],
              "default": "9:16",
              "enum_labels": {
                "16:9": "16:9 (Landscape)",
                "9:16": "9:16 (Portrait)",
                "1:1": "1:1 (Square)"
              },
              "_ux": {
                "input_type": "select"
              }
            }
          }
        }
      },
      "properties": {
        "prompt": {
          "type": "string",
          "_ux.display": false
        },
        "style_notes": {
          "type": "string",
          "_ux.display": true,
          "_ux.display_label": "Style",
          "_ux.highlight": true,
          "_ux.highlight_color": "#4ECDC4"
        }
      }
    }
  }
}
```

---

## Source Image for img2vid - Design Proposal

### Problem

The `media.generate` module with `img2vid` action needs the source image.
Current `sub_action.py` expects `source_image` in params:

```python
elif request.action_type == "img2vid":
    source_image = request.params.get("source_image", "")
```

### Proposed Solution: Add Previous Interactions to Context

Instead of passing `input_image` as a module input, add previous interaction
data to the workflow context. This is more versatile as any later module can
access data from earlier interactions.

**Modification to `backend/server/workflow/interaction.py`:**

In `WorkflowExecutionContext` or during interaction handling, include a
`previous_interactions` dict that modules can access:

```python
# In context creation or interaction handler
context.previous_interactions = {
    "image_prompts": {
        "generate_and_select_images": {
            "selected_content_id": "gc_xxx",
            "selected_content": {
                "content_id": "gc_xxx",
                "url": "/workflow/wf_xxx/media/gc_xxx.jpg",
                "local_path": "/mnt/g/wm/images/cgm_xxx_gc_xxx_0.jpg",
                "metadata_id": "cgm_xxx",
                "prompt_key": "midjourney"
            },
            "generations": {...}
        }
    }
}
```

**Benefits:**
- Any module can access any previous interaction's data
- No need to explicitly pass `input_image` for each use case
- Future modules (img2img, video editing) can access the same data
- Data structure is consistent with how state already works

**How media.generate uses it:**

The module can read from context to get the source image:

```python
def get_interaction_request(self, inputs, context):
    # For img2vid, get source image from previous interaction
    sub_actions = getattr(context, 'sub_actions', None)
    if sub_actions:
        for action in sub_actions:
            if action.get('action_type') == 'img2vid':
                # Look up source image from previous interactions
                prev = getattr(context, 'previous_interactions', {})
                image_step = prev.get('image_prompts', {})
                image_module = image_step.get('generate_and_select_images', {})
                source_image = image_module.get('selected_content', {})

    return InteractionRequest(
        display_data={
            "data": prompts,
            "schema": schema,
            "sub_actions": sub_actions,
            "source_image": source_image,  # Pass to WebUI
            ...
        }
    )
```

**WebUI includes in sub-action request:**

```javascript
// When user clicks Generate for img2vid
const params = {
    prompt: editedPrompt,
    motion_strength: 5,
    source_image: displayData.source_image.url  // Or local_path
};
```

### Alternative: Module Input (Simpler but Less Flexible)

If the context approach is too complex for initial implementation:

```json
{
  "module_id": "media.generate",
  "inputs": {
    "prompts": "{{ state.video_prompts.prompts }}",
    "source_image": "{{ state.selected_image_data }}",
    ...
  }
}
```

Module validates that `source_image` is provided for `img2vid` actions.

---

## Workflow Summary Update

### cc_workflow_summary.txt (MODIFIED)

Update the VIDEO PROMPTS section:

```jinja2
================================================================================
VIDEO PROMPTS
================================================================================

{% if state.video_prompts %}
--- Leonardo Motion ---
Positive: {{ state.video_prompts.prompts.leonardo.positive_prompt }}
Negative: {{ state.video_prompts.prompts.leonardo.negative_prompt }}
Motion Strength: {{ state.video_prompts.prompts.leonardo.motion_strength }}
Notes: {{ state.video_prompts.prompts.leonardo.motion_notes }}

--- Sora 2 ---
Prompt: {{ state.video_prompts.prompts.sora.prompt }}
Style: {{ state.video_prompts.prompts.sora.style_notes }}
{% endif %}

{% if state.selected_video_data %}
--- Selected Video ---
Video ID: {{ state.selected_video_id }}
Provider: {{ state.selected_video_data.prompt_key }}
{% endif %}
```

---

## Files to Create/Modify

### New Files

| File | Purpose |
|------|---------|
| `prompts/cc_role_visual_analyst.txt` | Role for image analysis |
| `prompts/cc_image_analysis_system.txt` | System instructions |
| `prompts/cc_image_analysis_user.txt` | User prompt |
| `schemas/cc_image_analysis_schema.json` | LLM output schema |
| `schemas/cc_motion_elements_display_schema.json` | Motion selection UI |
| `schemas/cc_video_generation_display_schema.json` | Video generation UI |

### Modified Files

| File | Change |
|------|--------|
| `step.json` | Replace modules, rename step_id |
| `prompts/cc_video_prompts_system.txt` | Leonardo + Sora guidelines |
| `prompts/cc_video_prompts_user.txt` | Add motion elements context |
| `schemas/cc_video_prompts_schema.json` | Leonardo + Sora structure |
| `../8_workflow_summary/prompts/cc_workflow_summary.txt` | Update video section |

### Backend Files (for context approach)

| File | Change |
|------|--------|
| `backend/server/workflow/interaction.py` | Add previous_interactions to context |
| `backend/server/modules/media/generate.py` | Read source_image from context |

---

## Open Questions

### 1. Previous Interactions Scope

Should `previous_interactions` include:
- **A)** Only the most recent interaction per module?
- **B)** All interactions with timestamps?
- **C)** Configurable per workflow?

### 2. Local Path Resolution

The `selected_image_data` from step 3 includes `url` but may not include
`local_path`. Should:
- **A)** media.generate module look up `local_path` from DB using `content_id`?
- **B)** Step 3 output include `local_path` in `selected_content`?
- **C)** Providers accept server URL and resolve locally?

### 3. Sora 2 Provider Implementation

The OpenAI provider exists but does it have `img2vid` for Sora 2? Need to:
- Check OpenAI API for Sora 2 image-to-video endpoint
- Implement if not present
