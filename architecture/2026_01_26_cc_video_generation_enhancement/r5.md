# CC Workflow Step 4: Video Generation Enhancement

## Summary

Enhance CC workflow by adding image analysis as a new step after image
generation, and enhancing video generation with motion element selection and
actual video generation.

## Step Restructure

```
1. user_input
2. scene_generation
3. image_prompts
4. image_analysis      ← NEW STEP
5. video_generation    ← was video_prompts (renamed + enhanced)
6. text_overlays
7. titles_social
8. music
9. workflow_summary
```

### Rationale

Image analysis generates `audio_atmosphere` data used by the music step,
so it belongs as its own step rather than inside video generation.

---

## Decisions Summary

| Item | Decision |
|------|----------|
| Image Analysis | New step 4 (no review UI - data flows to other steps) |
| Image Analysis Scope | Full schema (environment, motion, audio) |
| Video Providers | Leonardo + Sora 2 |
| Motion Element UI | Single panel "Detected Motion Elements" |
| Image input type | `"type": "image"` (existing) |
| Source image access | Via previous interaction `_resolved_inputs` in events |
| Sora 2 | Needs implementation (docs below) |

---

## Step 4: Image Analysis (NEW)

**Folder:** `workflows/cc/steps/4_image_analysis/`

No display/review schema needed - data flows directly to subsequent steps.

### step.json

```json
{
  "step_id": "image_analysis",
  "name": "Step {step_number}: Image Analysis",
  "description": "Analyze selected image for motion elements and audio atmosphere",
  "modules": [
    {
      "module_id": "api.llm",
      "inputs": {
        "resolver_schema": {
          "type": "object",
          "properties": {
            "input": { "resolver": "server" },
            "system": { "resolver": "server" },
            "metadata": { "resolver": "server" }
          }
        },
        "input": [
          {
            "content": "{{ state.selected_image_data.local_path }}",
            "type": "image"
          },
          { "$ref": "prompts/cc_image_analysis_user.txt", "type": "text" }
        ],
        "output_schema": {
          "$ref": "schemas/cc_image_analysis_schema.json",
          "type": "json"
        },
        "metadata": {
          "step_id": "{{ step.step_id }}"
        },
        "system": [
          {
            "$ref": "prompts/cc_role_visual_analyst.txt",
            "type": "text",
            "cache_ttl": 10800
          },
          {
            "$ref": "prompts/cc_image_analysis_system.txt",
            "type": "text",
            "cache_ttl": 10800
          }
        ],
        "provider": "openai"
      },
      "outputs_to_state": {
        "response": "image_analysis"
      },
      "name": "analyze_image"
    }
  ]
}
```

### Prompts

#### cc_role_visual_analyst.txt

```
You are a visual analyst specializing in image-to-video production. Your role
is to analyze static images and identify elements suitable for subtle animation
in cute pet videos.

You have expertise in:
- Identifying animatable environmental elements (light, particles, fabric)
- Understanding motion physics for realistic subtle animation
- Recognizing atmospheric elements that enhance cozy aesthetics
- Analyzing audio characteristics implied by visual scenes

Your analysis directly informs video generation prompts and music selection,
so accuracy and specificity are critical.
```

#### cc_image_analysis_system.txt

```
# IMAGE ANALYSIS FOR PET VIDEO PRODUCTION

Analyze the provided image to extract environment details, motion elements, and
audio atmosphere for a cute pet video production.

## PART 1: ENVIRONMENT ANALYSIS

Extract scene details for narrative and caption generation.

**Identify:**
- Setting: Brief, specific description of the scene
- Time of day: dawn, morning, noon, afternoon, dusk, night
- Weather: If visible or implied (sunny, cloudy, rain, etc.)
- Location type: indoor, outdoor, specific venue type
- Key subjects: 3-6 main elements visible (include the pet!)
- Dominant colors: 3-5 colors defining the palette
- Atmosphere: The mood conveyed by the visuals
- Lighting: How light behaves in the scene

**Rules:**
- Only describe what is VISIBLE
- Be specific (e.g., "sunny living room with large windows" not "indoor")
- For key_subjects, prioritize the pet and their immediate environment

## PART 2: MOTION ELEMENT ANALYSIS

Identify elements for subtle animation suitable for short pet videos.

**Look for:**
- Light effects (sunbeams, lamp glow, reflections)
- Particles (dust motes, floating fur/hair)
- Fabric (pet bed, blankets, curtains)
- Environmental (plants swaying, water in bowl)
- Pet-adjacent elements (toys, food bowl steam)

**Constraints:**
- The PET must remain STATIC - do not suggest animating the pet itself
- Camera must remain STATIC - no zoom, pan, or movement
- Focus on SUBTLE, LOOPING motion suitable for 5-10 second clips
- Only identify elements CLEARLY VISIBLE in the image

**detected_elements (3-8 items):**
Best candidates for animation. For each provide:
- element: What could be animated
- motion_type: Type of motion (e.g., "gentle sway", "shimmer")
- location: Where in the frame

**all_potential_elements (5-20 items):**
Complete list of everything that COULD move (for negative prompts).
Include detected elements plus any others that might drift unintentionally.

## PART 3: AUDIO ATMOSPHERE ANALYSIS

Analyze for music generation context. This data will be used by the music
generation step to create appropriate background music.

**Identify:**
- implied_sounds: 2-5 sounds the scene suggests
- silence_level: very quiet / quiet / moderate / busy / loud
- emotional_intensity: serene / calm / neutral / tense / dramatic / intense
- temporal_feel: frozen/still / slow/meditative / gentle/flowing / moderate
- warmth: cold/clinical / cool / neutral / warm / very warm/intimate
- era_feel: Time period evoked (e.g., "cozy modern home")
- cultural_context: Geographic/cultural context if apparent
- suggested_music_genres: 2-3 genres that would complement
- musical_texture: Suggested texture (e.g., "warm and gentle")
```

#### cc_image_analysis_user.txt

```
Analyze this image of a pet scene and extract:
1. Environment details (setting, time, weather, location, subjects, colors,
   atmosphere, lighting)
2. Motion elements for animation (detected candidates and all potential
   elements)
3. Audio atmosphere (implied sounds, emotional qualities, music suggestions)

Focus on elements that would enhance a cute, heartwarming pet video.
```

### Schemas

#### cc_image_analysis_schema.json

```json
{
  "type": "object",
  "properties": {
    "environment": {
      "type": "object",
      "properties": {
        "setting": {
          "type": "string",
          "description": "Brief description of the scene setting"
        },
        "time_of_day": {
          "type": "string",
          "description": "Time of day depicted"
        },
        "weather": {
          "type": "string",
          "description": "Weather conditions if apparent"
        },
        "location_type": {
          "type": "string",
          "description": "Type of location"
        },
        "key_subjects": {
          "type": "array",
          "items": { "type": "string" },
          "minItems": 3,
          "maxItems": 6
        },
        "dominant_colors": {
          "type": "array",
          "items": { "type": "string" },
          "minItems": 3,
          "maxItems": 5
        },
        "atmosphere": {
          "type": "string",
          "description": "Overall mood conveyed by visuals"
        },
        "lighting": {
          "type": "string",
          "description": "Lighting characteristics"
        }
      },
      "required": ["setting", "time_of_day", "weather", "location_type",
                   "key_subjects", "dominant_colors", "atmosphere", "lighting"],
      "additionalProperties": false
    },
    "motion_elements": {
      "type": "object",
      "properties": {
        "detected_elements": {
          "type": "array",
          "items": {
            "type": "object",
            "properties": {
              "element": { "type": "string" },
              "motion_type": { "type": "string" },
              "location": { "type": "string" }
            },
            "required": ["element", "motion_type", "location"],
            "additionalProperties": false
          },
          "minItems": 3,
          "maxItems": 8
        },
        "all_potential_elements": {
          "type": "array",
          "items": { "type": "string" },
          "minItems": 5,
          "maxItems": 20
        }
      },
      "required": ["detected_elements", "all_potential_elements"],
      "additionalProperties": false
    },
    "audio_atmosphere": {
      "type": "object",
      "properties": {
        "implied_sounds": {
          "type": "array",
          "items": { "type": "string" },
          "minItems": 2,
          "maxItems": 5
        },
        "silence_level": {
          "type": "string",
          "enum": ["very quiet", "quiet", "moderate", "busy", "loud"]
        },
        "emotional_intensity": {
          "type": "string",
          "enum": ["serene", "calm", "neutral", "tense", "dramatic", "intense"]
        },
        "temporal_feel": {
          "type": "string",
          "enum": ["frozen/still", "slow/meditative", "gentle/flowing",
                   "moderate/steady", "dynamic/energetic", "urgent/fast"]
        },
        "warmth": {
          "type": "string",
          "enum": ["cold/clinical", "cool", "neutral", "warm",
                   "very warm/intimate"]
        },
        "era_feel": { "type": "string" },
        "cultural_context": { "type": "string" },
        "suggested_music_genres": {
          "type": "array",
          "items": { "type": "string" },
          "minItems": 2,
          "maxItems": 3
        },
        "musical_texture": { "type": "string" }
      },
      "required": ["implied_sounds", "silence_level", "emotional_intensity",
                   "temporal_feel", "warmth", "era_feel", "cultural_context",
                   "suggested_music_genres", "musical_texture"],
      "additionalProperties": false
    }
  },
  "required": ["environment", "motion_elements", "audio_atmosphere"],
  "additionalProperties": false
}
```

---

## Step 5: Video Generation (ENHANCED)

**Folder:** `workflows/cc/steps/5_video_generation/`

### step.json

```json
{
  "step_id": "video_generation",
  "name": "Step {step_number}: Video Generation",
  "description": "Select motion elements, generate video prompts, and create videos",
  "modules": [
    {
      "module_id": "user.select",
      "inputs": {
        "resolver_schema": {
          "type": "object",
          "properties": {
            "data": { "resolver": "server" }
          }
        },
        "data": {
          "detected": "{{ state.image_analysis.motion_elements.detected_elements }}"
        },
        "schema": {
          "$ref": "schemas/cc_motion_elements_display_schema.json",
          "type": "json"
        },
        "prompt": "Select motion elements to animate in your video",
        "multi_select": true,
        "mode": "select"
      },
      "outputs_to_state": {
        "selected_indices": "selected_motion_element_indices",
        "selected_data": "selected_motion_elements"
      },
      "name": "select_motion_elements"
    },
    {
      "module_id": "api.llm",
      "inputs": {
        "resolver_schema": {
          "type": "object",
          "properties": {
            "input": { "resolver": "server" },
            "system": { "resolver": "server" },
            "metadata": { "resolver": "server" }
          }
        },
        "ai_config": {},
        "input": {
          "$ref": "prompts/cc_video_prompts_user.txt",
          "type": "jinja2"
        },
        "output_schema": {
          "$ref": "schemas/cc_video_prompts_schema.json",
          "type": "json"
        },
        "metadata": {
          "step_id": "{{ step.step_id }}"
        },
        "system": [
          {
            "$ref": "prompts/cc_role_motion_engineer.txt",
            "type": "text",
            "cache_ttl": 10800
          },
          {
            "$ref": "prompts/cc_video_prompts_system.txt",
            "type": "text",
            "cache_ttl": 10800
          }
        ],
        "provider": "openai"
      },
      "outputs_to_state": {
        "response": "video_prompts"
      },
      "name": "generate_video_prompts"
    },
    {
      "module_id": "media.generate",
      "inputs": {
        "resolver_schema": {
          "type": "object",
          "properties": {
            "prompts": { "resolver": "server" }
          }
        },
        "prompts": "{{ state.video_prompts.prompts }}",
        "schema": {
          "$ref": "schemas/cc_video_generation_display_schema.json",
          "type": "json"
        },
        "title": "Generate and Select Videos"
      },
      "outputs_to_state": {
        "selected_content_id": "selected_video_id",
        "selected_content": "selected_video_data",
        "generations": "video_generations"
      },
      "sub_actions": [
        {
          "id": "generate",
          "label": "Generate Videos",
          "action_type": "img2vid",
          "loading_label": "Generating video...",
          "result_key": "generations"
        }
      ],
      "retryable": {
        "default_option": "continue",
        "options": [
          {
            "id": "continue",
            "mode": "continue",
            "label": "Accept and continue"
          },
          {
            "id": "retry_prompts",
            "mode": "retry",
            "label": "Regenerate video prompts",
            "shortcut": "r",
            "target_module": "generate_video_prompts",
            "feedback": {
              "enabled": true,
              "prompt": "What should be different?",
              "default_message": "Please generate different video prompts."
            }
          },
          {
            "id": "change_motion_elements",
            "mode": "jump",
            "label": "Change motion elements",
            "target_step": "video_generation",
            "target_module": "select_motion_elements"
          },
          {
            "id": "change_image",
            "mode": "jump",
            "label": "Go back to image selection",
            "target_step": "image_prompts",
            "target_module": "generate_and_select_images"
          }
        ]
      },
      "name": "generate_and_select_videos"
    }
  ]
}
```

### Prompts

#### cc_video_prompts_system.txt

```
Generate video/motion prompts for the selected scene. Create prompts optimized
for Leonardo Motion and Sora 2.

## Context
You have:
- A scene concept with narrative and visual moments
- Image analysis with detected motion elements
- User-selected elements to animate
- Elements to block in negative prompts

## CRITICAL: Use Selected Motion Elements

The user has specifically selected which elements to animate. Your prompts
must prioritize including animation of these selected elements. All other
potentially movable elements should be included in the negative prompt to
keep them static.

## Critical: Negative Prompts
Negative prompts are ESSENTIAL for quality video generation. They prevent:
- Morphing/deformation artifacts
- Unnatural movement
- Flickering and inconsistency
- Anatomical errors during motion

### Standard Negative Elements for Pet Videos
Always include these in negative prompts:
- morphing, deformation, distortion
- extra limbs, missing limbs, fused limbs
- blurry, out of focus, low quality
- flickering, strobing, jittering
- unnatural movement, robotic motion
- face distortion, eye glitches
- sudden scene changes, jump cuts

### Pet-Specific Negative Elements
- extra legs, missing legs, leg fusion
- tail splitting, multiple tails
- ear morphing, floating ears
- eye color change, pupil distortion
- fur texture changing, bald patches appearing
- body stretching, size fluctuation

### Block Unselected Motion Elements
Add ALL elements from `all_potential_elements` that are NOT in the user's
selected list to the negative prompt. This ensures only the intended elements
move.

## Model-Specific Guidelines

### Leonardo Motion
Leonardo Motion excels at subtle, natural movements from a source image.

**Positive Prompt Structure:**
- Focus on the selected motion elements
- Describe the type of motion (gentle sway, shimmer, drift)
- Keep pet and camera static
- Specify atmospheric motion if selected

**Negative Prompt:** Must include:
- All unselected potential motion elements
- Standard artifacts to avoid
- Pet-specific negative elements

**Docs:** https://docs.leonardo.ai/reference/createimagetovideogeneration

### Sora 2
Sora 2 creates high-quality video from images with natural motion.

**Prompt Structure:**
- Natural language description of the scene with motion
- Focus on selected elements
- Emphasize what should remain static
- Describe the mood and atmosphere

**Style Notes:** Include guidance on:
- Visual style consistency
- Motion intensity level
- Lighting preservation

**Docs:** https://platform.openai.com/docs/guides/video-generation

## Motion Ideas for Pets (Environmental Only)

**Since pets must remain static, focus on:**
- Gentle breeze through nearby curtains/fabric
- Dust motes in sunbeams
- Subtle shadow movement from clouds
- Steam from food/water bowl
- Plant/leaf movement
- Light shimmer on reflective surfaces
- Fabric settling/breathing effect on pet bed
```

#### cc_video_prompts_user.txt

```
Generate video/motion prompts for this pet scene:

## Scene Details

**Title:** {{ state.selected_scene.title }}
**Narrative:** {{ state.selected_scene.narrative }}
**Pet Behavior Focus:** {{ state.selected_scene.pet_behavior }}
**Setting:** {{ state.selected_scene.setting }}
**Emotional Arc:** {{ state.selected_scene.emotional_arc }}

**Key Visual Moments:**
{% for moment in state.selected_scene.visual_moments %}
- {{ moment }}
{% endfor %}

## Context

**Pet Type:** {{ state.pet_type_selection.label }}

**Visual Tone:**
- Mood: {{ state.aesthetic_selection.visual_tone.mood }}
- Lighting: {{ state.aesthetic_selection.visual_tone.lighting }}
- Energy: {{ state.aesthetic_selection.visual_tone.energy }}

**Duration Target:** {{ state.duration_selection.video_duration }} seconds

{% if state.user_direction %}
**User Direction:** {{ state.user_direction }}
{% endif %}

## Image Analysis

**Environment:**
- Setting: {{ state.image_analysis.environment.setting }}
- Atmosphere: {{ state.image_analysis.environment.atmosphere }}
- Lighting: {{ state.image_analysis.environment.lighting }}

## IMPORTANT: Selected Motion Elements

The user has specifically chosen these elements to animate. Your prompts must
prioritize including animation of following elements:

{% for element in state.selected_motion_elements %}
- **{{ element.element }}**: {{ element.motion_type }} ({{ element.location }})
{% endfor %}

## Elements to Block in Negative Prompts

All of these elements should be kept STATIC. Include them in negative prompts:
{{ state.image_analysis.motion_elements.all_potential_elements | join(', ') }}

Generate Leonardo Motion and Sora 2 video prompts that bring this scene to life
with natural, subtle motion. The pet and all unselected elements must remain
completely static - only animate the selected environmental elements.
```

### Schemas

#### cc_motion_elements_display_schema.json

```json
{
  "type": "object",
  "_ux.display": "passthrough",
  "properties": {
    "detected": {
      "type": "array",
      "_ux": {
        "display": "visible",
        "display_label": "Detected Motion Elements",
        "render_as": "content-panel"
      },
      "items": {
        "type": "object",
        "_ux": {
          "display": "visible",
          "render_as": "card",
          "nudges": ["index-badge"],
          "selectable": true
        },
        "properties": {
          "element": {
            "type": "string",
            "_ux": {
              "display": true,
              "display_label": "Element",
              "render_as": "card-title",
              "highlight": true,
              "highlight_color": "#4ECDC4"
            }
          },
          "motion_type": {
            "type": "string",
            "_ux.display": true,
            "_ux.display_label": "Motion Type"
          },
          "location": {
            "type": "string",
            "_ux.display": true,
            "_ux.display_label": "Location"
          }
        }
      }
    }
  }
}
```

#### cc_video_prompts_schema.json

```json
{
  "type": "object",
  "properties": {
    "scene_title": {
      "type": "string",
      "description": "The scene this video prompt is for"
    },
    "motion_summary": {
      "type": "string",
      "description": "Brief description of the primary motion in this video"
    },
    "prompts": {
      "type": "object",
      "properties": {
        "leonardo": {
          "type": "object",
          "properties": {
            "positive_prompt": {
              "type": "string",
              "description": "Motion prompt focusing on selected elements only"
            },
            "negative_prompt": {
              "type": "string",
              "description": "REQUIRED: Unselected elements + artifacts to avoid"
            },
            "motion_strength": {
              "type": "string",
              "enum": ["subtle", "moderate", "dynamic"],
              "description": "Recommended motion intensity level"
            },
            "motion_notes": {
              "type": "string",
              "description": "Brief notes on motion elements emphasized"
            }
          },
          "required": ["positive_prompt", "negative_prompt", "motion_strength",
                       "motion_notes"],
          "additionalProperties": false
        },
        "sora": {
          "type": "object",
          "properties": {
            "prompt": {
              "type": "string",
              "description": "Natural language video description for Sora 2"
            },
            "style_notes": {
              "type": "string",
              "description": "Style guidance notes"
            }
          },
          "required": ["prompt", "style_notes"],
          "additionalProperties": false
        }
      },
      "required": ["leonardo", "sora"],
      "additionalProperties": false
    }
  },
  "required": ["scene_title", "motion_summary", "prompts"],
  "additionalProperties": false
}
```

#### cc_video_generation_display_schema.json

```json
{
  "type": "object",
  "_ux.display": "passthrough",
  "properties": {
    "leonardo": {
      "type": "object",
      "_ux": {
        "display": "visible",
        "tab_label": "Leonardo",
        "render_as": "tab.media",
        "provider": "leonardo",
        "input_schema": {
          "type": "object",
          "_ux": {
            "layout": "grid",
            "layout_columns": 3,
            "layout_columns_sm": 2
          },
          "properties": {
            "_text": {
              "type": "string",
              "title": "Prompt",
              "destination_field": "prompt",
              "_ux": {
                "input_type": "textarea",
                "col_span": "full",
                "rows": 4,
                "source_field": "positive_prompt"
              }
            },
            "negative_prompt": {
              "type": "string",
              "title": "Negative Prompt",
              "_ux": {
                "input_type": "textarea",
                "col_span": "full",
                "rows": 2,
                "source_field": "negative_prompt"
              }
            },
            "motion_strength": {
              "type": "integer",
              "title": "Motion Strength",
              "minimum": 1,
              "maximum": 10,
              "default": 5,
              "step": 1,
              "_ux": {
                "input_type": "slider"
              }
            }
          }
        }
      },
      "properties": {
        "positive_prompt": {
          "type": "string",
          "_ux.display": false
        },
        "negative_prompt": {
          "type": "string",
          "_ux.display": false
        },
        "motion_strength": {
          "type": "string",
          "_ux.display": true,
          "_ux.display_label": "Motion"
        },
        "motion_notes": {
          "type": "string",
          "_ux.display": true,
          "_ux.display_label": "Notes",
          "_ux.highlight": true,
          "_ux.highlight_color": "#7B68EE"
        }
      }
    },
    "sora": {
      "type": "object",
      "_ux": {
        "display": "visible",
        "tab_label": "Sora 2",
        "render_as": "tab.media",
        "provider": "openai",
        "input_schema": {
          "type": "object",
          "_ux": {
            "layout": "grid",
            "layout_columns": 3,
            "layout_columns_sm": 2
          },
          "properties": {
            "_text": {
              "type": "string",
              "title": "Prompt",
              "destination_field": "prompt",
              "_ux": {
                "input_type": "textarea",
                "col_span": "full",
                "rows": 4,
                "source_field": "prompt"
              }
            },
            "duration": {
              "type": "string",
              "title": "Duration",
              "enum": ["5", "10", "15", "20"],
              "default": "5",
              "enum_labels": {
                "5": "5 seconds",
                "10": "10 seconds",
                "15": "15 seconds",
                "20": "20 seconds"
              },
              "_ux": {
                "input_type": "select"
              }
            },
            "resolution": {
              "type": "string",
              "title": "Resolution",
              "enum": ["480p", "720p", "1080p"],
              "default": "720p",
              "_ux": {
                "input_type": "select"
              }
            },
            "aspect_ratio": {
              "type": "string",
              "title": "Aspect Ratio",
              "enum": ["16:9", "9:16", "1:1"],
              "default": "9:16",
              "enum_labels": {
                "16:9": "16:9 (Landscape)",
                "9:16": "9:16 (Portrait)",
                "1:1": "1:1 (Square)"
              },
              "_ux": {
                "input_type": "select"
              }
            }
          }
        }
      },
      "properties": {
        "prompt": {
          "type": "string",
          "_ux.display": false
        },
        "style_notes": {
          "type": "string",
          "_ux.display": true,
          "_ux.display_label": "Style",
          "_ux.highlight": true,
          "_ux.highlight_color": "#4ECDC4"
        }
      }
    }
  }
}
```

---

## Source Image for img2vid

### Solution: Access via Previous Interaction Events

The `interaction_requested` events store `_resolved_inputs` which contains the
data passed to that interaction. For `media.generate` in step 5 to access the
selected image from step 3:

**Data Location:**
```
events collection
  → event_type: "interaction_requested"
  → step_id: "image_prompts"
  → module_name: "generate_and_select_images"
  → data._resolved_inputs: { prompts, schema, title }
```

**After interaction completes:**
```
state.module_outputs
  → selected_image_data: {
      content_id: "gc_xxx",
      url: "/workflow/wf_xxx/media/gc_xxx.jpg",
      metadata_id: "cgm_xxx",
      prompt_key: "midjourney"
    }
```

**Implementation Approach:**

<!--NO! this is not the approach, do even read my comment on
architecture/2026_01_26_cc_video_generation_enhancement/r2.md#951-955, does
this looks like what i mentioned there? -->

1. The `media.generate` module can look up previous interaction data from the
   events collection when `action_type` is `img2vid`

2. Alternatively, add utility in `WorkflowExecutionContext` to query previous
   interaction `_resolved_inputs` by step_id/module_name

3. For source image specifically, it's already in state as `selected_image_data`
   - Module can access via context state
   - Pass to WebUI in `display_data`
   - WebUI includes in sub-action request params

**In media.generate module:**
```python
def get_interaction_request(self, inputs, context):
    # Get source image from state for img2vid
    source_image = None
    sub_actions = getattr(context, 'sub_actions', None)
    if sub_actions:
        for action in sub_actions:
            if action.get('action_type') in ('img2vid', 'img2img'):
                # Access selected image from state
                source_image = context.module_outputs.get('selected_image_data')
                break

    return InteractionRequest(
        display_data={
            "data": prompts,
            "schema": schema,
            "sub_actions": sub_actions,
            "source_image": source_image,
            ...
        }
    )
```

---

## Workflow Summary Update

Update `cc_workflow_summary.txt`:

```jinja2
================================================================================
IMAGE ANALYSIS
================================================================================

{% if state.image_analysis %}
**Environment:**
- Setting: {{ state.image_analysis.environment.setting }}
- Time: {{ state.image_analysis.environment.time_of_day }}
- Atmosphere: {{ state.image_analysis.environment.atmosphere }}
- Lighting: {{ state.image_analysis.environment.lighting }}

**Detected Motion Elements:**
{% for elem in state.image_analysis.motion_elements.detected_elements %}
- {{ elem.element }}: {{ elem.motion_type }}
{% endfor %}

**Audio Atmosphere:**
- Silence Level: {{ state.image_analysis.audio_atmosphere.silence_level }}
- Emotional Intensity: {{ state.image_analysis.audio_atmosphere.emotional_intensity }}
- Warmth: {{ state.image_analysis.audio_atmosphere.warmth }}
- Suggested Genres: {{ state.image_analysis.audio_atmosphere.suggested_music_genres | join(', ') }}
{% endif %}

================================================================================
VIDEO PROMPTS
================================================================================

{% if state.selected_motion_elements %}
**Selected Motion Elements:**
{% for elem in state.selected_motion_elements %}
- {{ elem.element }}
{% endfor %}
{% endif %}

{% if state.video_prompts %}
--- Leonardo Motion ---
Positive: {{ state.video_prompts.prompts.leonardo.positive_prompt }}
Negative: {{ state.video_prompts.prompts.leonardo.negative_prompt }}
Motion Strength: {{ state.video_prompts.prompts.leonardo.motion_strength }}
Notes: {{ state.video_prompts.prompts.leonardo.motion_notes }}

--- Sora 2 ---
Prompt: {{ state.video_prompts.prompts.sora.prompt }}
Style: {{ state.video_prompts.prompts.sora.style_notes }}
{% endif %}

{% if state.selected_video_data %}
--- Selected Video ---
Video ID: {{ state.selected_video_id }}
Provider: {{ state.selected_video_data.prompt_key }}
{% endif %}
```

---

## Music Step Update

The music step can now use `state.image_analysis.audio_atmosphere`:

```jinja2
{# In cc_music_user.txt #}
## Audio Atmosphere from Image Analysis

{% if state.image_analysis.audio_atmosphere %}
**Implied Sounds:** {{ state.image_analysis.audio_atmosphere.implied_sounds | join(', ') }}
**Silence Level:** {{ state.image_analysis.audio_atmosphere.silence_level }}
**Emotional Intensity:** {{ state.image_analysis.audio_atmosphere.emotional_intensity }}
**Temporal Feel:** {{ state.image_analysis.audio_atmosphere.temporal_feel }}
**Warmth:** {{ state.image_analysis.audio_atmosphere.warmth }}
**Era Feel:** {{ state.image_analysis.audio_atmosphere.era_feel }}
**Suggested Genres:** {{ state.image_analysis.audio_atmosphere.suggested_music_genres | join(', ') }}
**Musical Texture:** {{ state.image_analysis.audio_atmosphere.musical_texture }}
{% endif %}
```

---

## Files to Create/Modify

### New Step Folder: `4_image_analysis/`

| File | Purpose |
|------|---------|
| `step.json` | Step definition (no review module) |
| `prompts/cc_role_visual_analyst.txt` | Role prompt |
| `prompts/cc_image_analysis_system.txt` | System prompt |
| `prompts/cc_image_analysis_user.txt` | User prompt |
| `schemas/cc_image_analysis_schema.json` | LLM output schema |

### Renamed/Modified: `5_video_generation/`

| File | Change |
|------|--------|
| `step.json` | Rename step_id, add modules |
| `prompts/cc_video_prompts_system.txt` | Leonardo + Sora guidelines |
| `prompts/cc_video_prompts_user.txt` | Add motion elements context |
| `schemas/cc_video_prompts_schema.json` | Leonardo + Sora structure |
| `schemas/cc_motion_elements_display_schema.json` | NEW |
| `schemas/cc_video_generation_display_schema.json` | NEW |

### Renamed Folders

| Old | New |
|-----|-----|
| `5_text_overlays/` | `6_text_overlays/` |
| `6_titles_social/` | `7_titles_social/` |
| `7_music/` | `8_music/` |
| `8_workflow_summary/` | `9_workflow_summary/` |

### Modified Files

| File | Change |
|------|--------|
| `workflow_v3.json` | Update step references |
| `8_music/prompts/cc_music_user.txt` | Add audio_atmosphere context |
| `9_workflow_summary/prompts/cc_workflow_summary.txt` | Add sections |

### Backend Files

| File | Change |
|------|--------|
| `backend/server/modules/media/generate.py` | Read source_image from state |

---

## Implementation Notes

### Selected Image local_path

We can update step 3 to include `local_path` in `selected_content` output when
needed. Address during implementation.

### Sora 2 Provider

Needs implementation in OpenAI provider. Docs:
https://platform.openai.com/docs/guides/video-generation

Key endpoints:
- POST `/v1/videos/generations` - Create video
- GET `/v1/videos/{id}` - Get video status/result
