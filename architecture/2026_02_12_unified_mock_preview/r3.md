# Unified Mock Preview System - Revision 3

## Summary

Replace the current dual-preview system (static preview + virtual runtime) with a
single unified preview system that uses mock data by default. Every module
provides mock outputs, allowing the virtual runtime to execute complete workflow
graphs without external API calls.

## Changes from R2

Based on operator feedback:

1. **Use `jsf` library for schema-aware mock generation** - Instead of custom
   lorem ipsum generator, use the `jsf` (JSON Schema Faker) library which
   handles JSON Schema parsing and generates fake data automatically.

2. **Use `Faker` library for lorem ipsum text** - The `jsf` library integrates
   with `Faker`, which provides proper lorem ipsum generation via
   `faker.text()`, `faker.sentence()`, etc.

## Dependencies

New Python dependencies to add:

```
jsf>=0.11.2
Faker>=40.0.0
```

Both are MIT licensed and actively maintained.

## Current State

### Two Preview Types (Problem)

1. **Static Preview** (`UxSchemaEditor`, `SchemaPreview`)
   - Renders UI using `SchemaRenderer` with inline/sample data
   - No workflow execution
   - Breaks on complex scenarios (e.g., `media.generate` with dynamic data)
   - Only works when data is hardcoded in module config

2. **Real Preview** (Virtual Runtime)
   - Executes actual workflow via `/virtual/*` endpoints
   - Makes real LLM calls, real API calls
   - Expensive and slow for preview purposes
   - Required for accurate previews of dynamic data

### Why This Is a Problem

- Static preview cannot show realistic data for modules that depend on upstream
  outputs (e.g., `user.select` after `api.llm`)
- Real preview is expensive - each preview might trigger multiple LLM calls
- Two code paths to maintain with different behaviors
- Confusing UX - preview might work in static mode but fail in real mode

## Proposed Solution

### Core Concept

Every module implements `get_mock_output(inputs)` as an internal method. The
`execute()` method checks if running in mock context and calls `get_mock_output()`
instead of making real API calls. This keeps all execution logic in one place.

```
Normal Execution:
  api.llm.execute(inputs, context) 
    → context.mock_mode is False
    → makes real LLM call
    → returns real outputs

Mock Execution:
  api.llm.execute(inputs, context)
    → context.mock_mode is True
    → calls self.get_mock_output(inputs)
    → returns mock outputs
```

### Module Base Class Changes

```python
# engine/module_interface.py

class ModuleBase(ABC):
    # ... existing code ...

    @abstractmethod
    def get_mock_output(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
        """
        Generate mock output data for preview mode.
        
        This is an ABSTRACT method - all modules MUST implement it.
        Called internally by execute() when context.mock_mode is True.

        Mock data should:
        1. Match the module's output contract (self.outputs)
        2. Honor any schema constraints from inputs (e.g., output_schema)
        3. Use lorem ipsum style data - obviously fake but structurally correct
        4. Be deterministic for consistent previews

        Args:
            inputs: Resolved inputs (may contain mock data from upstream)

        Returns:
            Dict matching the module's outputs contract
        """
        pass
```

### Execution Pattern

Each module's `execute()` method handles mock mode internally:

```python
# Example: api/llm_call.py

class LLMCallModule(ExecutableModule):
    
    def execute(self, inputs: Dict[str, Any], context) -> Dict[str, Any]:
        # Check mock mode first
        if getattr(context, 'mock_mode', False):
            return self.get_mock_output(inputs)
        
        # Normal execution - real LLM call
        provider = ProviderRegistry.get(provider_id)
        result = provider.call(model, messages, context, ...)
        
        return {
            "response": result.get("content"),
            "response_text": result.get("content"),
            "model": model,
            "usage": result.get("usage", {})
        }
    
    def get_mock_output(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
        output_schema = inputs.get('output_schema')
        
        if output_schema:
            # Generate lorem ipsum data conforming to schema using jsf
            mock_response = generate_mock_from_schema(output_schema)
        else:
            # Use Faker for plain lorem ipsum text
            mock_response = generate_lorem_text()
        
        return {
            "response": mock_response,
            "response_text": str(mock_response) if not isinstance(
                mock_response, str) else mock_response,
            "model": "mock-preview",
            "usage": {"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0}
        }
```

### Mock Output Requirements

Mock outputs must:

1. **Match output contract**: Return all fields defined in `self.outputs`
2. **Honor input schemas**: If `api.llm` receives `output_schema`, mock output
   must conform to that schema structure
3. **Use lorem ipsum style**: Data should be obviously fake - lorem ipsum text,
   placeholder numbers, etc. NOT contextually meaningful data.
4. **Be deterministic**: Same inputs produce consistent mock outputs (via seeding)

### Why Lorem Ipsum Style?

The purpose of mock preview is to show **how the workflow will look visually**,
not to simulate real content. Benefits:

- No risk of confusing mock data with real results
- No need for complex contextual generation
- Consistent, predictable output
- Clear visual indicator that preview is not real data
- If users want real data, they click "Reload with Real Data"

## Schema-Aware Mock Generation with jsf + Faker

### Library Overview

**jsf** (JSON Schema Faker):
- https://pypi.org/project/jsf/
- https://github.com/ghandic/jsf
- Purpose-built for generating fake JSON from JSON Schema
- Supports custom providers via `$provider` field
- Handles all JSON Schema types, enums, required fields, nested structures
- Can be seeded for deterministic output

**Faker**:
- https://pypi.org/project/faker/
- Industry-standard fake data generation
- Provides `faker.text()`, `faker.sentence()`, `faker.paragraph()` for lorem ipsum
- Seedable for deterministic output

### Implementation

```python
# backend/server/utils/mock_generator.py

from typing import Any, Dict, Optional
from jsf import JSF
from faker import Faker

# Global Faker instance for lorem ipsum generation
_faker = Faker()

# Default seed for deterministic mock data
DEFAULT_SEED = 42


def generate_mock_from_schema(
    schema: Dict[str, Any],
    seed: Optional[int] = DEFAULT_SEED
) -> Any:
    """
    Generate mock data that conforms to a JSON schema.
    
    Uses the jsf library with Faker providers for lorem ipsum style text.
    Output is deterministic when seed is provided.
    
    Args:
        schema: JSON Schema definition
        seed: Random seed for deterministic output (default: 42)
    
    Returns:
        Mock data matching the schema structure with lorem ipsum text
    
    Example:
        schema = {
            "type": "object",
            "properties": {
                "title": {"type": "string"},
                "items": {
                    "type": "array",
                    "items": {
                        "type": "object",
                        "properties": {
                            "name": {"type": "string"},
                            "score": {"type": "number"}
                        }
                    }
                }
            }
        }
        
        result = generate_mock_from_schema(schema)
        # {
        #     "title": "Lorem ipsum dolor sit amet.",
        #     "items": [
        #         {"name": "Consectetur adipiscing elit.", "score": 42.5},
        #         {"name": "Sed do eiusmod tempor.", "score": 43.5}
        #     ]
        # }
    """
    # Create JSF instance with the schema
    faker = JSF(schema)
    
    # Seed for deterministic output
    if seed is not None:
        faker.seed(seed)
    
    return faker.generate()


def generate_lorem_text(
    sentences: int = 2,
    seed: Optional[int] = DEFAULT_SEED
) -> str:
    """
    Generate lorem ipsum text using Faker.
    
    Args:
        sentences: Number of sentences to generate
        seed: Random seed for deterministic output
    
    Returns:
        Lorem ipsum text string
    """
    if seed is not None:
        Faker.seed(seed)
    
    return _faker.paragraph(nb_sentences=sentences)


def generate_lorem_sentence(seed: Optional[int] = DEFAULT_SEED) -> str:
    """
    Generate a single lorem ipsum sentence using Faker.
    
    Args:
        seed: Random seed for deterministic output
    
    Returns:
        Single lorem ipsum sentence
    """
    if seed is not None:
        Faker.seed(seed)
    
    return _faker.sentence()


def generate_lorem_word(seed: Optional[int] = DEFAULT_SEED) -> str:
    """
    Generate a single lorem ipsum word using Faker.
    
    Args:
        seed: Random seed for deterministic output
    
    Returns:
        Single lorem ipsum word
    """
    if seed is not None:
        Faker.seed(seed)
    
    return _faker.word()
```

### Custom Providers for jsf

For more control over generated data, we can register custom providers that
explicitly use Faker's lorem ipsum:

```python
# backend/server/utils/mock_generator.py (continued)

from jsf import JSF

def create_lorem_jsf(schema: Dict[str, Any], seed: int = DEFAULT_SEED) -> JSF:
    """
    Create a JSF instance with custom lorem ipsum providers.
    
    This allows schemas to use $provider hints for specific lorem types:
    - $provider: "lorem.sentence" -> single sentence
    - $provider: "lorem.paragraph" -> paragraph (2-3 sentences)
    - $provider: "lorem.word" -> single word
    - $provider: "lorem.title" -> title-case words (2-4 words)
    
    Args:
        schema: JSON Schema definition
        seed: Random seed for deterministic output
    
    Returns:
        Configured JSF instance
    """
    faker = JSF(schema)
    
    if seed is not None:
        faker.seed(seed)
        Faker.seed(seed)
    
    # Register custom lorem providers
    faker.register_provider("lorem.sentence", lambda: _faker.sentence())
    faker.register_provider("lorem.paragraph", lambda: _faker.paragraph())
    faker.register_provider("lorem.word", lambda: _faker.word())
    faker.register_provider("lorem.title", lambda: _faker.sentence(nb_words=3)[:-1])
    faker.register_provider("lorem.text", lambda: _faker.text(max_nb_chars=200))
    
    return faker
```

### Usage in Modules

```python
# Example: api/llm_call.py

from utils.mock_generator import generate_mock_from_schema, generate_lorem_text

class LLMCallModule(ExecutableModule):
    
    def get_mock_output(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
        output_schema = inputs.get('output_schema')
        
        if output_schema:
            # Schema provided - generate conforming mock data
            mock_response = generate_mock_from_schema(output_schema)
        else:
            # No schema - return plain lorem ipsum text
            mock_response = generate_lorem_text(sentences=3)
        
        return {
            "response": mock_response,
            "response_text": str(mock_response) if not isinstance(
                mock_response, str) else mock_response,
            "model": "mock-preview",
            "usage": {"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0}
        }
```

### Module-Specific Mock Implementations

#### api.llm

```python
def get_mock_output(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
    output_schema = inputs.get('output_schema')
    
    if output_schema:
        mock_response = generate_mock_from_schema(output_schema)
    else:
        mock_response = generate_lorem_text(sentences=3)
    
    return {
        "response": mock_response,
        "response_text": str(mock_response) if not isinstance(
            mock_response, str) else mock_response,
        "model": "mock-preview",
        "usage": {"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0}
    }
```

#### api.fetch

```python
def get_mock_output(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
    return {
        "response": {"mock": True, "message": generate_lorem_sentence()},
        "status_code": 200,
        "success": True,
        "headers": {"content-type": "application/json"}
    }
```

#### db.query

```python
def get_mock_output(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
    return {
        "results": [
            {"_id": "mock-001", "title": generate_lorem_sentence()},
            {"_id": "mock-002", "title": generate_lorem_sentence()}
        ],
        "count": 2
    }
```

#### Transform Modules

Transform modules execute normally - they perform pure data transformations
without external API calls. They don't need mock mode handling.

```python
# transform.extract, transform.query, transform.reshape, etc.
# These execute normally even in mock mode - no get_mock_output needed

def execute(self, inputs: Dict[str, Any], context) -> Dict[str, Any]:
    # No mock check - always execute real transform logic
    # Input may be mock data, but transform is applied normally
    ...
```

### Media Generate - VirtualDB Integration

For `media.generate`, mock mode must create proper records in VirtualDB so the
content system works correctly:

```python
# modules/media/generate.py

class MediaGenerateModule(InteractiveModule):
    
    def get_mock_output(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
        # This is called after user interaction in mock mode
        # Should return mock selection result
        return {
            "selected_content_id": "mock-content-001",
            "selected_content": {
                "content_id": "mock-content-001",
                "url": MOCK_PLACEHOLDER_URL,
                "local_path": MOCK_PLACEHOLDER_PATH,
                "metadata_id": "mock-metadata-001",
                "prompt_key": "mock"
            },
            "generations": {}
        }
```

#### Mock Sub-Actions

Sub-actions must create real VirtualDB records with placeholder images:

```python
async def sub_action(self, context):
    """Execute media generation sub-action."""
    
    if getattr(context, 'mock_mode', False):
        return await self._mock_sub_action(context)
    
    # Normal execution...

async def _mock_sub_action(self, context):
    """Mock sub-action that creates placeholder content in VirtualDB."""
    params = context.params
    prompt_id = params.get("prompt_id")
    
    # Simulate brief processing delay
    yield ("progress", {
        "status": "processing",
        "progress": {"elapsed_ms": 200, "message": "Generating mock content..."}
    })
    await asyncio.sleep(0.3)
    
    # Create mock content record in VirtualDB
    # Uses actual placeholder image from filesystem
    content_id = f"mock-{uuid7_str()[:8]}"
    metadata_id = f"mock-meta-{uuid7_str()[:8]}"
    
    # Store in VirtualDB content repo
    if hasattr(context, 'db') and context.db:
        context.db.content_repo.create_content({
            "_id": content_id,
            "type": "image",
            "local_path": MOCK_PLACEHOLDER_RELATIVE_PATH,
            "metadata_id": metadata_id,
            "mock": True
        })
    
    yield ("result", {
        "metadata_id": metadata_id,
        "content_ids": [content_id],
        "urls": [f"/workflow/{context.workflow_run_id}/media/{MOCK_PLACEHOLDER_FILENAME}"],
        "prompt_id": prompt_id
    })
```

#### Placeholder Image Strategy

**Recommendation**: Static placeholder file for simplicity.

- Store a single placeholder image: `backend/static/mock/placeholder.png`
- Image should be clearly labeled "MOCK PREVIEW" or similar
- All mock generations reference this file
- Simple, reliable, no generation needed

### API Changes

#### Virtual Endpoints

Add `mock` parameter to all virtual endpoints that can trigger execution:

```python
# backend/server/api/routes/virtual.py

class VirtualStartRequest(BaseModel):
    workflow: Dict[str, Any]
    virtual_db: Optional[str] = None
    target_step_id: str
    target_module_name: str
    mock: bool = True  # Default to mock mode

class VirtualRespondRequest(BaseModel):
    workflow: Dict[str, Any]
    virtual_db: str
    virtual_run_id: str
    target_step_id: str
    target_module_name: str
    interaction_id: str
    response: Dict[str, Any]
    mock: bool = True  # Default to mock mode

class VirtualResumeConfirmRequest(BaseModel):
    workflow: Dict[str, Any]
    virtual_db: str
    target_step_id: str
    target_module_name: str
    mock: bool = True  # Default to mock mode
```

#### Execution Context

Pass mock mode through execution context:

```python
# backend/server/workflow/context.py (or wherever context is defined)

@dataclass
class WorkflowExecutionContext:
    # ... existing fields ...
    mock_mode: bool = False
```

The executor passes this to modules, which check it in their `execute()` method.

### Optional Custom Mock Data

Workflow JSON can optionally specify custom mock data per module instance:

```json
{
  "steps": [
    {
      "step_id": "generate_ideas",
      "modules": [
        {
          "module_type": "api.llm",
          "name": "generate_ideas",
          "inputs": { ... },
          "mock_output": {
            "response": [
              {"title": "Custom Mock Idea 1", "description": "Lorem ipsum..."},
              {"title": "Custom Mock Idea 2", "description": "Dolor sit amet..."}
            ]
          }
        }
      ]
    }
  ]
}
```

When `mock_output` is specified in the module config:
1. Executor checks for `mock_output` in module config first
2. If present and `mock_mode` is True, uses that instead of calling module
3. Otherwise falls back to module's `get_mock_output()`

This allows workflow authors to test specific scenarios without modifying
module code.

### Frontend Changes

#### VirtualRuntime

```typescript
// ui/editor/src/runtime/VirtualRuntime.ts

async runToModule(
  workflow: WorkflowDefinition,
  target: ModuleLocation,
  selections: ModuleSelection[],
  options?: { 
    openPanel?: "preview" | "state" | "none";
    mock?: boolean;  // New option, defaults to true
  }
): Promise<RunResult>
```

#### virtual-api.ts

```typescript
// ui/editor/src/runtime/virtual-api.ts

export async function virtualStart(params: {
  workflow: WorkflowDefinition;
  virtual_db: string | null;
  target_step_id: string;
  target_module_name: string;
  mock?: boolean;  // Add to all relevant functions
}): Promise<VirtualWorkflowResponse>
```

#### VirtualRuntimePanel

```typescript
// ui/editor/src/runtime/VirtualRuntimePanel.tsx

function VirtualRuntimePanel({ runtime, workflow, target, ... }) {
  const [isMockMode, setIsMockMode] = useState(true);
  
  const handleReloadWithRealData = async () => {
    setIsMockMode(false);
    // Reset runtime and re-run with mock=false
    runtime.reset();
    await runtime.actions.runToModule(workflow, target, [], { mock: false });
  };
  
  const handleReloadWithMockData = async () => {
    setIsMockMode(true);
    runtime.reset();
    await runtime.actions.runToModule(workflow, target, [], { mock: true });
  };
  
  return (
    <Sheet>
      <SheetHeader>
        <div className="flex items-center justify-between">
          <SheetTitle>Preview</SheetTitle>
          <div className="flex items-center gap-2">
            <Badge variant={isMockMode ? "secondary" : "default"}>
              {isMockMode ? "Mock Data" : "Real Data"}
            </Badge>
            {isMockMode ? (
              <Button 
                variant="outline" 
                size="sm"
                onClick={handleReloadWithRealData}
              >
                Reload with Real Data
              </Button>
            ) : (
              <Button 
                variant="outline" 
                size="sm"
                onClick={handleReloadWithMockData}
              >
                Back to Mock Data
              </Button>
            )}
          </div>
        </div>
      </SheetHeader>
      {/* ... existing content ... */}
    </Sheet>
  );
}
```

### Removing Static Preview

Once mock preview is working:

1. Remove `UxSchemaEditor` "Live Preview" section that uses `SchemaRenderer`
   directly with inline data
2. Remove `SchemaPreview` component from `MediaGenerateNode`
3. Remove `onLoadPreviewData` callback pattern from module nodes
4. All previews go through VirtualRuntime with mock mode

The UX schema editor component remains for configuring display schemas, but
preview functionality moves to the unified system.

## Implementation Plan

### Phase 1: Backend Mock Infrastructure

1. Add `jsf` and `Faker` to dependencies
2. Create `backend/server/utils/mock_generator.py` with jsf/Faker integration
3. Add abstract `get_mock_output(inputs)` to `ModuleBase`
4. Add `mock_mode` to `WorkflowExecutionContext`
5. Implement `get_mock_output()` for `api.llm` (with schema-aware generation)
6. Implement `get_mock_output()` for `api.fetch`
7. Implement `get_mock_output()` for `db.query`
8. Update each module's `execute()` to check mock mode and call `get_mock_output()`
9. Add `mock` parameter to virtual endpoints

### Phase 2: Interactive Module Mocks

1. Add placeholder image to `backend/static/mock/`
2. Implement `_mock_sub_action()` for `media.generate`
3. Ensure mock sub-action creates VirtualDB records
4. Test interactive modules with mock upstream data
5. Verify metadata endpoints still work (not mocked)

### Phase 3: Frontend Integration

1. Add `mock` option to `VirtualRuntime.runToModule()`
2. Update `virtual-api.ts` to pass mock parameter
3. Add "Reload with Real Data" / "Back to Mock Data" buttons
4. Add mock/real indicator badge
5. Track mock mode state in VirtualRuntime

### Phase 4: Optional Custom Mock Data

1. Add `mock_output` field to module schema in workflow JSON
2. Update executor to check for custom mock output
3. Document custom mock data feature

### Phase 5: Remove Static Preview

1. Remove static preview from `UxSchemaEditor`
2. Remove `SchemaPreview` from `MediaGenerateNode`
3. Remove `onLoadPreviewData` pattern from all module nodes
4. Update all module nodes to use unified preview button

### Phase 6: Testing & Polish

1. Test all module types with mock mode
2. Test complex workflows (multiple LLM calls, nested selections)
3. Test "Reload with Real Data" toggle
4. Test custom mock data in workflow JSON
5. Update any affected tests

## File Changes Summary

### New Files

- `backend/server/utils/mock_generator.py` - jsf/Faker integration for mock data
- `backend/static/mock/placeholder.png` - Placeholder image for mock media

### New Dependencies

Add to `requirements.txt` or `pyproject.toml`:
```
jsf>=0.11.2
Faker>=40.0.0
```

### Modified Files

**Backend:**
- `backend/server/engine/module_interface.py` - Add abstract `get_mock_output()`
- `backend/server/workflow/context.py` - Add `mock_mode` field
- `backend/server/api/routes/virtual.py` - Add `mock` parameter to requests
- `backend/server/modules/api/llm_call.py` - Add mock mode handling
- `backend/server/modules/api/fetch.py` - Add mock mode handling
- `backend/server/modules/db/query.py` - Add mock mode handling
- `backend/server/modules/media/generate.py` - Add mock sub-action

**Frontend:**
- `ui/editor/src/runtime/VirtualRuntime.ts` - Add mock option
- `ui/editor/src/runtime/virtual-api.ts` - Pass mock parameter
- `ui/editor/src/runtime/VirtualRuntimePanel.tsx` - Add toggle button
- `ui/editor/src/components/ux-schema-editor/UxSchemaEditor.tsx` - Remove static
  preview section
- `ui/editor/src/modules/media/generate/MediaGenerateNode.tsx` - Remove
  `SchemaPreview`, `onLoadPreviewData`
- `ui/editor/src/modules/user/select/UserSelectNode.tsx` - Remove
  `onLoadPreviewData` if present

## Open Questions

None - all questions from R1 and R2 have been addressed.
