# Unified Mock Preview System

## Summary

Replace the current dual-preview system (static preview + virtual runtime) with a
single unified preview system that uses mock data by default. Every module
provides mock outputs, allowing the virtual runtime to execute complete workflow
graphs without external API calls.

## Current State

### Two Preview Types (Problem)

1. **Static Preview** (`UxSchemaEditor`, `SchemaPreview`)
   - Renders UI using `SchemaRenderer` with inline/sample data
   - No workflow execution
   - Breaks on complex scenarios (e.g., `media.generate` with dynamic data)
   - Only works when data is hardcoded in module config

2. **Real Preview** (Virtual Runtime)
   - Executes actual workflow via `/virtual/*` endpoints
   - Makes real LLM calls, real API calls
   - Expensive and slow for preview purposes
   - Required for accurate previews of dynamic data

### Why This Is a Problem

- Static preview cannot show realistic data for modules that depend on upstream
  outputs (e.g., `user.select` after `api.llm`)
- Real preview is expensive - each preview might trigger multiple LLM calls
- Two code paths to maintain with different behaviors
- Confusing UX - preview might work in static mode but fail in real mode

## Proposed Solution

### Core Concept

Every module implements `get_mock_output(inputs)` that returns synthetic outputs
matching the module's output contract. When the workflow runs in "mock mode":

1. `ExecutableModule` returns `get_mock_output()` instead of `execute()`

<!--i feel like instead of we call different method, make execute aware of mock
context, and use mock data in correct places. reason i think this is the best
solution is because, each excute method has its own complexity, and we change
them frequently when there are bugs. i dont want to maintain 2 "execute"
methods here, I want to make sure core logic is in one place, but provide
result based on context.-->

2. `InteractiveModule` uses mock inputs for `get_interaction_request()`
3. Sub-actions return mock results instead of triggering real tasks
4. Transform modules execute normally (pure data transforms, no external calls)

### Module Base Class Changes

```python
# engine/module_interface.py

class ModuleBase(ABC):
    # ... existing code ...

    def get_mock_output(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
        """
        Generate mock output data for preview mode.

        This method receives resolved inputs (which may themselves be mock data
        from upstream modules) and returns synthetic outputs that:
        1. Match the module's output contract (self.outputs)
        2. Honor any schema constraints from inputs (e.g., output_schema)
        3. Provide realistic-looking data for downstream modules

        Default implementation returns empty dict - modules should override.

        Args:
            inputs: Resolved inputs (may contain mock data from upstream)

        Returns:
            Dict matching the module's outputs contract
        """
        return {}
```

<!--i agree with this, but this is internal module method, reason i wanted to
add it this way is because when we implement new modules, we are forced to /
reminded to implement mock aspect of the module.-->

### Execution Flow

```
Normal Execution:
  api.llm.execute(inputs) → real LLM call → real outputs
  user.select.get_interaction_request(real_outputs) → interaction

Mock Execution:
  api.llm.get_mock_output(inputs) → synthetic outputs
  user.select.get_interaction_request(mock_outputs) → interaction
```

### Mock Output Requirements

Mock outputs must:

1. **Match output contract**: Return all fields defined in `self.outputs`
2. **Honor input schemas**: If `api.llm` receives `output_schema`, mock output
   must conform to that schema
3. **Be deterministic**: Same inputs should produce consistent mock outputs
   (helps with debugging and testing)
4. **Be realistic**: Data should look plausible for UI preview purposes

<!--i do agree on #3 and #4, but how do we do it consistantly. one big problem
i have is, i dont want to return data which has actual meaning unless theres
smart way to do it, for example, lets say workflow is about pets, can we return
mock data about pets without making llm call? if workflow is about dancing, can
we return data about dancing without making llm call? if not, we should not use
meaningful data, instead we should use lorem ipsum, which is objvious that its
not real data, but it will help user preview how it will look their work real
workflow. also, just to set expectation, in mock mode, purpose is to show how
the workflow will look visually, if they want to real data, they can always
decide to run preview on real data mode.-->

### Module-Specific Mock Implementations

#### api.llm

```python
def get_mock_output(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
    output_schema = inputs.get('output_schema')
    
    if output_schema:
        # Generate mock data conforming to schema
        mock_response = self._generate_mock_from_schema(output_schema)
    else:
        # Return generic mock text
        mock_response = "This is a mock LLM response for preview purposes."
    
    return {
        "response": mock_response,
        "response_text": str(mock_response) if not isinstance(
            mock_response, str) else mock_response,
        "model": "mock-preview",
        "usage": {"prompt_tokens": 100, "completion_tokens": 50, "total_tokens": 150}
    }

def _generate_mock_from_schema(self, schema: Dict) -> Any:
    """Generate mock data that conforms to a JSON schema."""
    # Implementation generates realistic mock data based on schema
    # e.g., for {"type": "array", "items": {"type": "object", 
    #            "properties": {"title": {"type": "string"}}}}
    # Returns: [{"title": "Mock Title 1"}, {"title": "Mock Title 2"}]
```

#### api.fetch

```python
def get_mock_output(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
    return {
        "response": {"mock": True, "data": "Mock API response"},
        "status_code": 200,
        "success": True,
        "headers": {"content-type": "application/json"}
    }
```

#### user.select (InteractiveModule)

Interactive modules don't need special mock handling for outputs since they
wait for user response. However, they receive mock inputs from upstream modules.

```python
# No change needed - get_interaction_request() works with mock inputs
# execute_with_response() works normally after user interaction
```

#### media.generate

```python
def get_mock_output(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
    return {
        "selected_content_id": "mock-content-001",
        "selected_content": {
            "content_id": "mock-content-001",
            "url": "/mock/preview-image.png",
            "metadata_id": "mock-metadata-001",
            "prompt_key": "midjourney"
        },
        "generations": {
            "midjourney": [{
                "content_ids": ["mock-content-001", "mock-content-002"],
                "urls": ["/mock/preview-image.png", "/mock/preview-image-2.png"],
                "metadata_id": "mock-metadata-001"
            }]
        }
    }
```

<!-- for this, we need to add data to virtual db with mock data, not just
return it. we can have mock image path in filesystem somewhere which will work
on how content generation and metadata would work, and then use them. we need
better plan here. -->

#### db.query

```python
def get_mock_output(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
    # Return mock query results
    return {
        "results": [
            {"_id": "mock-1", "name": "Mock Record 1"},
            {"_id": "mock-2", "name": "Mock Record 2"}
        ],
        "count": 2
    }
```

#### Transform Modules

Transform modules (`transform.extract`, `transform.query`, `transform.reshape`,
etc.) execute normally - they perform pure data transformations without external
API calls. They receive mock inputs and produce real transformed outputs.

```python
# transform.extract - executes normally
# transform.query - executes normally (uses mongomock internally)
# transform.reshape - executes normally
# No get_mock_output override needed
```

### Sub-Action Mocks

For `media.generate`, sub-actions need to return mock results:

```python
async def sub_action(self, context):
    """Execute media generation sub-action."""
    
    # Check if running in mock mode
    if getattr(context, 'mock_mode', False):
        # Return mock result without creating real task
        yield ("progress", {
            "status": "processing",
            "progress": {"elapsed_ms": 500, "message": "Mock generation..."}
        })
        
        await asyncio.sleep(0.5)  # Simulate brief delay
        
        yield ("result", {
            "metadata_id": f"mock-{uuid7_str()[:8]}",
            "content_ids": [f"mock-content-{uuid7_str()[:8]}"],
            "urls": ["/mock/generated-image.png"],
            "prompt_id": context.params.get("prompt_id")
        })
        return
    
    # Normal execution - create real task
    # ... existing code ...
```

### API Changes

#### Virtual Endpoints

Add `mock` parameter to virtual endpoints:

```python
# backend/server/api/routes/virtual.py

class VirtualStartRequest(BaseModel):
    workflow: Dict[str, Any]
    virtual_db: Optional[str] = None
    target_step_id: str
    target_module_name: str
    mock: bool = True  # Default to mock mode

class VirtualRespondRequest(BaseModel):
    workflow: Dict[str, Any]
    virtual_db: str
    virtual_run_id: str
    target_step_id: str
    target_module_name: str
    interaction_id: str
    response: Dict[str, Any]
    mock: bool = True  # Default to mock mode
```

#### Execution Context

Pass mock mode through execution context:

```python
# backend/server/workflow/executor.py

class WorkflowExecutionContext:
    # ... existing fields ...
    mock_mode: bool = False

# In execute_step_modules:
if context.mock_mode and hasattr(module, 'get_mock_output'):
    # Use mock output instead of execute
    outputs = module.get_mock_output(resolved_inputs)
else:
    outputs = module.execute(resolved_inputs, context)
```

### Frontend Changes

#### VirtualRuntime

```typescript
// ui/editor/src/runtime/VirtualRuntime.ts

async runToModule(
  workflow: WorkflowDefinition,
  target: ModuleLocation,
  selections: ModuleSelection[],
  options?: { 
    openPanel?: "preview" | "state" | "none";
    mock?: boolean;  // New option, defaults to true
  }
): Promise<RunResult>
```

#### VirtualRuntimePanel

Add "Reload with Real Data" button:

```typescript
// ui/editor/src/runtime/VirtualRuntimePanel.tsx

function VirtualRuntimePanel({ runtime, ... }) {
  const [isMockMode, setIsMockMode] = useState(true);
  
  const handleReloadWithRealData = async () => {
    setIsMockMode(false);
    await runtime.actions.runToModule(workflow, target, [], { mock: false });
  };
  
  return (
    <Sheet>
      {/* ... existing content ... */}
      <div className="flex items-center gap-2">
        {isMockMode && (
          <Button 
            variant="outline" 
            size="sm"
            onClick={handleReloadWithRealData}
          >
            Reload with Real Data
          </Button>
        )}
        <Badge variant={isMockMode ? "secondary" : "default"}>
          {isMockMode ? "Mock Data" : "Real Data"}
        </Badge>
      </div>
    </Sheet>
  );
}
```

### Removing Static Preview

Once mock preview is working:

1. Remove `UxSchemaEditor` preview section that uses `SchemaRenderer` directly
2. Remove `SchemaPreview` component from `MediaGenerateNode`
3. Remove `onLoadPreviewData` callback pattern
4. All previews go through VirtualRuntime with mock mode

The UX schema editor will still exist for configuring display schemas, but the
"Live Preview" section will use the unified preview system.

## Implementation Plan

### Phase 1: Backend Mock Infrastructure

1. Add `get_mock_output(inputs)` to `ModuleBase`
2. Implement mock outputs for `api.llm` (with schema-aware generation)
3. Implement mock outputs for `api.fetch`
4. Implement mock outputs for `db.query`
5. Add `mock` parameter to virtual endpoints
6. Pass mock mode through `WorkflowExecutionContext`
7. Update executor to use mock outputs when in mock mode

### Phase 2: Interactive Module Mocks

1. Implement mock sub-action for `media.generate`
2. Test interactive modules with mock upstream data
3. Ensure metadata endpoints still work (not mocked)

### Phase 3: Frontend Integration

1. Add `mock` option to `VirtualRuntime.runToModule()`
2. Update `virtual-api.ts` to pass mock parameter
3. Add "Reload with Real Data" button to `VirtualRuntimePanel`
4. Add mock/real indicator badge

### Phase 4: Remove Static Preview

1. Remove static preview from `UxSchemaEditor`
2. Remove `SchemaPreview` from `MediaGenerateNode`
3. Remove `onLoadPreviewData` pattern
4. Update all module nodes to use unified preview

### Phase 5: Testing & Polish

1. Test all module types with mock mode
2. Test complex workflows (multiple LLM calls, nested selections)
3. Test "Reload with Real Data" functionality
4. Update any affected tests

## Schema-Aware Mock Generation

The most critical piece is generating mock data that conforms to JSON schemas.
This is needed for `api.llm` when `output_schema` is provided.

### Mock Data Generator

```python
# backend/server/utils/mock_generator.py

def generate_mock_from_schema(schema: Dict[str, Any]) -> Any:
    """
    Generate mock data that conforms to a JSON schema.
    
    Handles:
    - Primitive types (string, number, integer, boolean)
    - Objects with properties
    - Arrays with items schema
    - Enums
    - Required fields
    - Nested structures
    """
    schema_type = schema.get('type')
    
    if schema_type == 'string':
        if 'enum' in schema:
            return schema['enum'][0]
        return _generate_mock_string(schema)
    
    elif schema_type == 'number':
        return 42.5
    
    elif schema_type == 'integer':
        return 42
    
    elif schema_type == 'boolean':
        return True
    
    elif schema_type == 'array':
        items_schema = schema.get('items', {})
        # Generate 2-3 items by default
        return [generate_mock_from_schema(items_schema) for _ in range(2)]
    
    elif schema_type == 'object':
        obj = {}
        properties = schema.get('properties', {})
        required = schema.get('required', [])
        
        for prop_name, prop_schema in properties.items():
            obj[prop_name] = generate_mock_from_schema(prop_schema)
        
        return obj
    
    return None

def _generate_mock_string(schema: Dict[str, Any]) -> str:
    """Generate mock string based on schema hints."""
    title = schema.get('title', '')
    description = schema.get('description', '')
    
    # Use title/description to generate contextual mock
    if 'title' in title.lower():
        return "Mock Title"
    if 'description' in title.lower() or 'description' in description.lower():
        return "This is a mock description for preview purposes."
    if 'name' in title.lower():
        return "Mock Name"
    if 'url' in title.lower():
        return "https://example.com/mock"
    
    return f"Mock {title or 'value'}"
```

## Questions for Review

1. Should mock mode be persisted in the editor session, or reset on each
   preview?

   <!---it will be same concept as vdb, its per session, and it should emulate
   all functions, store in db, storing files in db etc.-->

2. For `media.generate`, should the mock sub-action return placeholder image
   URLs that actually resolve to a real placeholder image, or is a fake URL
   sufficient?
   
   <!--it should be real placeholder image. i am open to hear options to do
   this. -->

3. Should there be a way to provide custom mock data per-module-instance in
   the workflow JSON (for testing specific scenarios)?

   <!--its good options to have, if its not too complex, lets go for it.-->

4. How should we handle modules that are already in a completed state when
   re-running in mock mode? Should we re-execute with mocks or preserve
   previous real results?

   <!--in completed state, there wont be mock data use as it will user
   interaction data and state to render ui-->
