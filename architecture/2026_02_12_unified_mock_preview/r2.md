# Unified Mock Preview System - Revision 2

## Summary

Replace the current dual-preview system (static preview + virtual runtime) with a
single unified preview system that uses mock data by default. Every module
provides mock outputs, allowing the virtual runtime to execute complete workflow
graphs without external API calls.

## Changes from R1

Based on operator feedback:

1. **Keep `execute()` as single source of truth** - Instead of calling a separate
   `get_mock_output()` method, the `execute()` method checks mock context
   internally and uses mock data where appropriate. This avoids maintaining two
   parallel execution paths.

2. **`get_mock_output()` is internal helper** - Still exists as a method modules
   must implement, but it's called from within `execute()`, not from the executor.
   This ensures new modules are reminded to implement mock behavior.

3. **Use lorem ipsum style data** - Mock data should be obviously fake (lorem
   ipsum style) rather than attempting to be contextually meaningful. Purpose is
   visual preview, not semantic accuracy.

4. **Media generate needs VirtualDB integration** - Mock media generation must
   create real records in VirtualDB with placeholder images, not just return
   fake URLs.

5. **Mock mode persisted per session** - Like VirtualDB, mock state persists
   for the editor session and emulates all functions including DB storage.

6. **Real placeholder images** - Mock sub-actions return URLs to actual
   placeholder images that can be rendered.

7. **Optional custom mock data** - Workflow JSON can optionally specify custom
   mock data per module instance.

## Current State

### Two Preview Types (Problem)

1. **Static Preview** (`UxSchemaEditor`, `SchemaPreview`)
   - Renders UI using `SchemaRenderer` with inline/sample data
   - No workflow execution
   - Breaks on complex scenarios (e.g., `media.generate` with dynamic data)
   - Only works when data is hardcoded in module config

2. **Real Preview** (Virtual Runtime)
   - Executes actual workflow via `/virtual/*` endpoints
   - Makes real LLM calls, real API calls
   - Expensive and slow for preview purposes
   - Required for accurate previews of dynamic data

### Why This Is a Problem

- Static preview cannot show realistic data for modules that depend on upstream
  outputs (e.g., `user.select` after `api.llm`)
- Real preview is expensive - each preview might trigger multiple LLM calls
- Two code paths to maintain with different behaviors
- Confusing UX - preview might work in static mode but fail in real mode

## Proposed Solution

### Core Concept

Every module implements `get_mock_output(inputs)` as an internal method. The
`execute()` method checks if running in mock context and calls `get_mock_output()`
instead of making real API calls. This keeps all execution logic in one place.

```
Normal Execution:
  api.llm.execute(inputs, context) 
    → context.mock_mode is False
    → makes real LLM call
    → returns real outputs

Mock Execution:
  api.llm.execute(inputs, context)
    → context.mock_mode is True
    → calls self.get_mock_output(inputs)
    → returns mock outputs
```

### Module Base Class Changes

```python
# engine/module_interface.py

class ModuleBase(ABC):
    # ... existing code ...

    @abstractmethod
    def get_mock_output(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
        """
        Generate mock output data for preview mode.
        
        This is an ABSTRACT method - all modules MUST implement it.
        Called internally by execute() when context.mock_mode is True.

        Mock data should:
        1. Match the module's output contract (self.outputs)
        2. Honor any schema constraints from inputs (e.g., output_schema)
        3. Use lorem ipsum style data - obviously fake but structurally correct
        4. Be deterministic for consistent previews

        Args:
            inputs: Resolved inputs (may contain mock data from upstream)

        Returns:
            Dict matching the module's outputs contract
        """
        pass
```

### Execution Pattern

Each module's `execute()` method handles mock mode internally:

```python
# Example: api/llm_call.py

class LLMCallModule(ExecutableModule):
    
    def execute(self, inputs: Dict[str, Any], context) -> Dict[str, Any]:
        # Check mock mode first
        if getattr(context, 'mock_mode', False):
            return self.get_mock_output(inputs)
        
        # Normal execution - real LLM call
        provider = ProviderRegistry.get(provider_id)
        result = provider.call(model, messages, context, ...)
        
        return {
            "response": result.get("content"),
            "response_text": result.get("content"),
            "model": model,
            "usage": result.get("usage", {})
        }
    
    def get_mock_output(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
        output_schema = inputs.get('output_schema')
        
        if output_schema:
            # Generate lorem ipsum data conforming to schema
            mock_response = generate_mock_from_schema(output_schema)
        else:
            mock_response = "Lorem ipsum dolor sit amet, consectetur adipiscing."
        
        return {
            "response": mock_response,
            "response_text": str(mock_response) if not isinstance(
                mock_response, str) else mock_response,
            "model": "mock-preview",
            "usage": {"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0}
        }
```

### Mock Output Requirements

Mock outputs must:

1. **Match output contract**: Return all fields defined in `self.outputs`
2. **Honor input schemas**: If `api.llm` receives `output_schema`, mock output
   must conform to that schema structure
3. **Use lorem ipsum style**: Data should be obviously fake - "Lorem ipsum",
   "Mock Item 1", placeholder numbers, etc. NOT contextually meaningful data.
4. **Be deterministic**: Same inputs produce consistent mock outputs

### Why Lorem Ipsum Style?

The purpose of mock preview is to show **how the workflow will look visually**,
not to simulate real content. Benefits:

- No risk of confusing mock data with real results
- No need for complex contextual generation
- Consistent, predictable output
- Clear visual indicator that preview is not real data
- If users want real data, they click "Reload with Real Data"

### Module-Specific Mock Implementations

#### api.llm

```python
def get_mock_output(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
    output_schema = inputs.get('output_schema')
    
    if output_schema:
        mock_response = generate_mock_from_schema(output_schema)
    else:
        mock_response = "Lorem ipsum dolor sit amet, consectetur adipiscing elit."
    
    return {
        "response": mock_response,
        "response_text": str(mock_response) if not isinstance(
            mock_response, str) else mock_response,
        "model": "mock-preview",
        "usage": {"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0}
    }
```

#### api.fetch

```python
def get_mock_output(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
    return {
        "response": {"mock": True, "message": "Lorem ipsum mock response"},
        "status_code": 200,
        "success": True,
        "headers": {"content-type": "application/json"}
    }
```

#### db.query

```python
def get_mock_output(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
    return {
        "results": [
            {"_id": "mock-001", "title": "Lorem Ipsum Item 1"},
            {"_id": "mock-002", "title": "Lorem Ipsum Item 2"}
        ],
        "count": 2
    }
```

#### Transform Modules

Transform modules execute normally - they perform pure data transformations
without external API calls. They don't need mock mode handling.

```python
# transform.extract, transform.query, transform.reshape, etc.
# These execute normally even in mock mode - no get_mock_output needed

def execute(self, inputs: Dict[str, Any], context) -> Dict[str, Any]:
    # No mock check - always execute real transform logic
    # Input may be mock data, but transform is applied normally
    ...
```

### Media Generate - VirtualDB Integration

For `media.generate`, mock mode must create proper records in VirtualDB so the
content system works correctly:

```python
# modules/media/generate.py

class MediaGenerateModule(InteractiveModule):
    
    def get_mock_output(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
        # This is called after user interaction in mock mode
        # Should return mock selection result
        return {
            "selected_content_id": "mock-content-001",
            "selected_content": {
                "content_id": "mock-content-001",
                "url": MOCK_PLACEHOLDER_URL,
                "local_path": MOCK_PLACEHOLDER_PATH,
                "metadata_id": "mock-metadata-001",
                "prompt_key": "mock"
            },
            "generations": {}
        }
```

#### Mock Sub-Actions

Sub-actions must create real VirtualDB records with placeholder images:

```python
async def sub_action(self, context):
    """Execute media generation sub-action."""
    
    if getattr(context, 'mock_mode', False):
        return await self._mock_sub_action(context)
    
    # Normal execution...

async def _mock_sub_action(self, context):
    """Mock sub-action that creates placeholder content in VirtualDB."""
    params = context.params
    prompt_id = params.get("prompt_id")
    
    # Simulate brief processing delay
    yield ("progress", {
        "status": "processing",
        "progress": {"elapsed_ms": 200, "message": "Generating mock content..."}
    })
    await asyncio.sleep(0.3)
    
    # Create mock content record in VirtualDB
    # Uses actual placeholder image from filesystem
    content_id = f"mock-{uuid7_str()[:8]}"
    metadata_id = f"mock-meta-{uuid7_str()[:8]}"
    
    # Store in VirtualDB content repo
    if hasattr(context, 'db') and context.db:
        context.db.content_repo.create_content({
            "_id": content_id,
            "type": "image",
            "local_path": MOCK_PLACEHOLDER_RELATIVE_PATH,
            "metadata_id": metadata_id,
            "mock": True
        })
    
    yield ("result", {
        "metadata_id": metadata_id,
        "content_ids": [content_id],
        "urls": [f"/workflow/{context.workflow_run_id}/media/{MOCK_PLACEHOLDER_FILENAME}"],
        "prompt_id": prompt_id
    })
```

#### Placeholder Image Strategy

Options for placeholder images:

**Option A: Static placeholder file**
- Store a single placeholder image in the codebase
- Path: `backend/static/mock/placeholder.png`
- All mock generations reference this file
- Simple, reliable, no generation needed

**Option B: Color-coded placeholders**
- Store multiple placeholder images (different colors/patterns)
- Cycle through them for visual distinction
- Helps differentiate multiple generated items

**Option C: Dynamic SVG placeholder**
- Generate SVG placeholders on the fly
- Can include text like "Mock Image 1", "Mock Image 2"
- No file storage needed

**Recommendation**: Option A (static placeholder) for simplicity, with the
placeholder clearly labeled "MOCK PREVIEW" or similar.

### API Changes

#### Virtual Endpoints

Add `mock` parameter to all virtual endpoints that can trigger execution:

```python
# backend/server/api/routes/virtual.py

class VirtualStartRequest(BaseModel):
    workflow: Dict[str, Any]
    virtual_db: Optional[str] = None
    target_step_id: str
    target_module_name: str
    mock: bool = True  # Default to mock mode

class VirtualRespondRequest(BaseModel):
    workflow: Dict[str, Any]
    virtual_db: str
    virtual_run_id: str
    target_step_id: str
    target_module_name: str
    interaction_id: str
    response: Dict[str, Any]
    mock: bool = True  # Default to mock mode

class VirtualResumeConfirmRequest(BaseModel):
    workflow: Dict[str, Any]
    virtual_db: str
    target_step_id: str
    target_module_name: str
    mock: bool = True  # Default to mock mode
```

#### Execution Context

Pass mock mode through execution context:

```python
# backend/server/workflow/context.py (or wherever context is defined)

@dataclass
class WorkflowExecutionContext:
    # ... existing fields ...
    mock_mode: bool = False
```

The executor passes this to modules, which check it in their `execute()` method.

### Optional Custom Mock Data

Workflow JSON can optionally specify custom mock data per module instance:

```json
{
  "steps": [
    {
      "step_id": "generate_ideas",
      "modules": [
        {
          "module_type": "api.llm",
          "name": "generate_ideas",
          "inputs": { ... },
          "mock_output": {
            "response": [
              {"title": "Custom Mock Idea 1", "description": "Lorem ipsum..."},
              {"title": "Custom Mock Idea 2", "description": "Dolor sit amet..."}
            ]
          }
        }
      ]
    }
  ]
}
```

When `mock_output` is specified in the module config:
1. Executor checks for `mock_output` in module config first
2. If present and `mock_mode` is True, uses that instead of calling module
3. Otherwise falls back to module's `get_mock_output()`

This allows workflow authors to test specific scenarios without modifying
module code.

### Frontend Changes

#### VirtualRuntime

```typescript
// ui/editor/src/runtime/VirtualRuntime.ts

async runToModule(
  workflow: WorkflowDefinition,
  target: ModuleLocation,
  selections: ModuleSelection[],
  options?: { 
    openPanel?: "preview" | "state" | "none";
    mock?: boolean;  // New option, defaults to true
  }
): Promise<RunResult>
```

#### virtual-api.ts

```typescript
// ui/editor/src/runtime/virtual-api.ts

export async function virtualStart(params: {
  workflow: WorkflowDefinition;
  virtual_db: string | null;
  target_step_id: string;
  target_module_name: string;
  mock?: boolean;  // Add to all relevant functions
}): Promise<VirtualWorkflowResponse>
```

#### VirtualRuntimePanel

```typescript
// ui/editor/src/runtime/VirtualRuntimePanel.tsx

function VirtualRuntimePanel({ runtime, workflow, target, ... }) {
  const [isMockMode, setIsMockMode] = useState(true);
  
  const handleReloadWithRealData = async () => {
    setIsMockMode(false);
    // Reset runtime and re-run with mock=false
    runtime.reset();
    await runtime.actions.runToModule(workflow, target, [], { mock: false });
  };
  
  const handleReloadWithMockData = async () => {
    setIsMockMode(true);
    runtime.reset();
    await runtime.actions.runToModule(workflow, target, [], { mock: true });
  };
  
  return (
    <Sheet>
      <SheetHeader>
        <div className="flex items-center justify-between">
          <SheetTitle>Preview</SheetTitle>
          <div className="flex items-center gap-2">
            <Badge variant={isMockMode ? "secondary" : "default"}>
              {isMockMode ? "Mock Data" : "Real Data"}
            </Badge>
            {isMockMode ? (
              <Button 
                variant="outline" 
                size="sm"
                onClick={handleReloadWithRealData}
              >
                Reload with Real Data
              </Button>
            ) : (
              <Button 
                variant="outline" 
                size="sm"
                onClick={handleReloadWithMockData}
              >
                Back to Mock Data
              </Button>
            )}
          </div>
        </div>
      </SheetHeader>
      {/* ... existing content ... */}
    </Sheet>
  );
}
```

### Removing Static Preview

Once mock preview is working:

1. Remove `UxSchemaEditor` "Live Preview" section that uses `SchemaRenderer`
   directly with inline data
2. Remove `SchemaPreview` component from `MediaGenerateNode`
3. Remove `onLoadPreviewData` callback pattern from module nodes
4. All previews go through VirtualRuntime with mock mode

The UX schema editor component remains for configuring display schemas, but
preview functionality moves to the unified system.

## Schema-Aware Mock Generation

### Lorem Ipsum Generator

```python
# backend/server/utils/mock_generator.py

import random

LOREM_WORDS = [
    "lorem", "ipsum", "dolor", "sit", "amet", "consectetur", "adipiscing",
    "elit", "sed", "do", "eiusmod", "tempor", "incididunt", "ut", "labore",
    "et", "dolore", "magna", "aliqua"
]

def generate_mock_from_schema(schema: Dict[str, Any], depth: int = 0) -> Any:
    """
    Generate mock data that conforms to a JSON schema.
    Uses lorem ipsum style data - obviously fake but structurally correct.
    
    Args:
        schema: JSON Schema definition
        depth: Current nesting depth (for generating unique values)
    
    Returns:
        Mock data matching the schema structure
    """
    schema_type = schema.get('type')
    
    if schema_type == 'string':
        if 'enum' in schema:
            return schema['enum'][0]
        return _generate_lorem_string(schema, depth)
    
    elif schema_type == 'number':
        return 42.5 + depth
    
    elif schema_type == 'integer':
        return 42 + depth
    
    elif schema_type == 'boolean':
        return True
    
    elif schema_type == 'array':
        items_schema = schema.get('items', {'type': 'string'})
        min_items = schema.get('minItems', 2)
        max_items = schema.get('maxItems', 3)
        count = min(max_items, max(min_items, 2))
        return [
            generate_mock_from_schema(items_schema, depth + i)
            for i in range(count)
        ]
    
    elif schema_type == 'object':
        obj = {}
        properties = schema.get('properties', {})
        
        for idx, (prop_name, prop_schema) in enumerate(properties.items()):
            obj[prop_name] = generate_mock_from_schema(prop_schema, depth + idx)
        
        return obj
    
    # Fallback
    return f"mock_value_{depth}"

def _generate_lorem_string(schema: Dict[str, Any], depth: int) -> str:
    """Generate lorem ipsum string with index for uniqueness."""
    title = schema.get('title', '').lower()
    
    # Generate contextual but still obviously fake content
    if 'title' in title or 'name' in title:
        return f"Lorem Item {depth + 1}"
    if 'description' in title:
        words = LOREM_WORDS[depth % len(LOREM_WORDS):(depth % len(LOREM_WORDS)) + 5]
        return " ".join(words).capitalize() + "."
    if 'url' in title:
        return f"https://example.com/mock/{depth + 1}"
    if 'id' in title:
        return f"mock-id-{depth + 1:03d}"
    
    # Default: short lorem phrase
    start = depth % len(LOREM_WORDS)
    words = LOREM_WORDS[start:start + 3] or LOREM_WORDS[:3]
    return " ".join(words).capitalize()
```

## Implementation Plan

### Phase 1: Backend Mock Infrastructure

1. Add abstract `get_mock_output(inputs)` to `ModuleBase`
2. Add `mock_mode` to `WorkflowExecutionContext`
3. Implement `get_mock_output()` for `api.llm` (with schema-aware generation)
4. Implement `get_mock_output()` for `api.fetch`
5. Implement `get_mock_output()` for `db.query`
6. Update each module's `execute()` to check mock mode and call `get_mock_output()`
7. Add `mock` parameter to virtual endpoints
8. Create `generate_mock_from_schema()` utility

### Phase 2: Interactive Module Mocks

1. Add placeholder image to `backend/static/mock/`
2. Implement `_mock_sub_action()` for `media.generate`
3. Ensure mock sub-action creates VirtualDB records
4. Test interactive modules with mock upstream data
5. Verify metadata endpoints still work (not mocked)

### Phase 3: Frontend Integration

1. Add `mock` option to `VirtualRuntime.runToModule()`
2. Update `virtual-api.ts` to pass mock parameter
3. Add "Reload with Real Data" / "Back to Mock Data" buttons
4. Add mock/real indicator badge
5. Track mock mode state in VirtualRuntime

### Phase 4: Optional Custom Mock Data

1. Add `mock_output` field to module schema in workflow JSON
2. Update executor to check for custom mock output
3. Document custom mock data feature

### Phase 5: Remove Static Preview

1. Remove static preview from `UxSchemaEditor`
2. Remove `SchemaPreview` from `MediaGenerateNode`
3. Remove `onLoadPreviewData` pattern from all module nodes
4. Update all module nodes to use unified preview button

### Phase 6: Testing & Polish

1. Test all module types with mock mode
2. Test complex workflows (multiple LLM calls, nested selections)
3. Test "Reload with Real Data" toggle
4. Test custom mock data in workflow JSON
5. Update any affected tests

## File Changes Summary

### New Files

- `backend/server/utils/mock_generator.py` - Schema-aware mock data generation
- `backend/static/mock/placeholder.png` - Placeholder image for mock media

### Modified Files

**Backend:**
- `backend/server/engine/module_interface.py` - Add abstract `get_mock_output()`
- `backend/server/workflow/context.py` - Add `mock_mode` field
- `backend/server/api/routes/virtual.py` - Add `mock` parameter to requests
- `backend/server/modules/api/llm_call.py` - Add mock mode handling
- `backend/server/modules/api/fetch.py` - Add mock mode handling
- `backend/server/modules/db/query.py` - Add mock mode handling
- `backend/server/modules/media/generate.py` - Add mock sub-action

**Frontend:**
- `ui/editor/src/runtime/VirtualRuntime.ts` - Add mock option
- `ui/editor/src/runtime/virtual-api.ts` - Pass mock parameter
- `ui/editor/src/runtime/VirtualRuntimePanel.tsx` - Add toggle button
- `ui/editor/src/components/ux-schema-editor/UxSchemaEditor.tsx` - Remove static
  preview section
- `ui/editor/src/modules/media/generate/MediaGenerateNode.tsx` - Remove
  `SchemaPreview`, `onLoadPreviewData`
- `ui/editor/src/modules/user/select/UserSelectNode.tsx` - Remove
  `onLoadPreviewData` if present

## Open Questions

None - all questions from R1 have been addressed by operator feedback.
