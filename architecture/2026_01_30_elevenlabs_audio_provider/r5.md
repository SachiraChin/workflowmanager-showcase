# ElevenLabs Integration - Revision 5: Final Design

## Summary

Restructure CC workflow Step 7 (Music) from 4 modules to 2 modules, with
actual music generation via ElevenLabs.

## Changes from r4

Based on operator feedback:
1. `force_instrumental` is an ElevenLabs API parameter - don't append to prompt
2. LLM generates context-appropriate styles based on pet/aesthetic/mood data,
   NOT fixed categories like "Cozy Acoustic" or "Playful Bounce"

## Proposed Flow (2 Modules)

```
[Module 1: api.llm]
    Generate 3 context-appropriate music prompts
    - Styles determined by: pet type, aesthetic, mood, emotional arc
    - Example for "joyful golden retriever": upbeat acoustic, playful piano
    - Example for "pet memorial": reflective strings, gentle piano, soft ambient
         ↓
[Module 2: media.generate]
    - 3 tabs (one per generated style)
    - Editable: prompt text, BPM, instruments, mood, duration, format
    - Generate button → ElevenLabs txt2audio
    - Waveform audio player (react-audio-visualize)
    - Optional selection (can proceed without generating)
```

## Technical Specification

### 1. LLM Output Schema

**File:** `schemas/cc_music_prompts_schema.json`

```json
{
  "type": "object",
  "properties": {
    "prompts": {
      "type": "array",
      "minItems": 3,
      "maxItems": 3,
      "items": {
        "type": "object",
        "properties": {
          "name": {
            "type": "string",
            "description": "Evocative 2-3 word name for this style"
          },
          "description": {
            "type": "string",
            "description": "15-25 word summary of the sound and feel"
          },
          "prompt": {
            "type": "string",
            "description": "ElevenLabs-ready prompt (100-200 chars)"
          },
          "suggested_bpm": {
            "type": "integer",
            "minimum": 50,
            "maximum": 120
          },
          "instruments": {
            "type": "array",
            "items": { "type": "string" },
            "minItems": 2,
            "maxItems": 4,
            "description": "Instruments with texture descriptors"
          },
          "mood": {
            "type": "array",
            "items": { "type": "string" },
            "minItems": 2,
            "maxItems": 3,
            "description": "Mood descriptors"
          },
          "energy": {
            "type": "string",
            "description": "Energy arc or movement feel"
          }
        },
        "required": [
          "name", "description", "prompt", "suggested_bpm",
          "instruments", "mood", "energy"
        ]
      }
    }
  },
  "required": ["prompts"]
}
```

### 2. Provider-Side Prompt Assembly

The ElevenLabs provider assembles the final prompt internally.
Note: `duration_ms` and `force_instrumental` are API parameters, NOT prompt text.

**File:** `backend/providers/media/elevenlabs/provider.py`

```python
def _assemble_prompt(self, base_prompt: str, params: dict) -> str:
    """
    Assemble final ElevenLabs prompt with user parameters.

    Note: duration_ms and force_instrumental are API params, not prompt text.

    Example output:
    "Warm cozy acoustic with gentle fingerpicked guitar over soft pads.
    Target BPM: 72. Instruments: felt piano, warm acoustic guitar.
    Mood: gentle, cozy, loving."
    """
    parts = [base_prompt.rstrip('.')]

    if params.get('bpm'):
        parts.append(f"Target BPM: {params['bpm']}")

    if params.get('instruments'):
        instruments = params['instruments']
        if isinstance(instruments, list):
            instruments = ', '.join(instruments)
        parts.append(f"Instruments: {instruments}")

    if params.get('mood'):
        mood = params['mood']
        if isinstance(mood, list):
            mood = ', '.join(mood)
        parts.append(f"Mood: {mood}")

    return '. '.join(parts) + '.'
```

### 3. Display Schema for media.generate

**File:** `schemas/cc_audio_generation_display_schema.json`

```json
{
  "type": "object",
  "_ux.display": "passthrough",
  "properties": {
    "prompts": {
      "type": "array",
      "_ux": {
        "render_as": "tabs",
        "tab_key": "name"
      },
      "items": {
        "type": "object",
        "_ux": {
          "render_as": "tab.media[input_schema,audio_generation]",
          "provider": "elevenlabs",
          "prompt_id_field": "name",
          "input_schema": {
            "type": "object",
            "_ux": {
              "layout": "grid",
              "layout_columns": 3,
              "layout_columns_sm": 2
            },
            "properties": {
              "_text": {
                "type": "string",
                "title": "Prompt",
                "destination_field": "prompt",
                "_ux": {
                  "input_type": "textarea",
                  "col_span": "full",
                  "rows": 3,
                  "source_field": "prompt"
                }
              },
              "duration_ms": {
                "type": "integer",
                "title": "Duration (seconds)",
                "minimum": 10000,
                "maximum": 120000,
                "step": 5000,
                "_ux": {
                  "input_type": "slider",
                  "format": "duration_seconds",
                  "source_data": "{{ state.duration_selection.sound_track_duration * 1000 }}"
                }
              },
              "bpm": {
                "type": "integer",
                "title": "BPM",
                "minimum": 50,
                "maximum": 120,
                "step": 1,
                "_ux": {
                  "input_type": "number",
                  "source_field": "suggested_bpm"
                }
              },
              "instruments": {
                "type": "array",
                "title": "Instruments",
                "_ux": {
                  "input_type": "tag_input",
                  "source_field": "instruments"
                }
              },
              "mood": {
                "type": "array",
                "title": "Mood",
                "_ux": {
                  "input_type": "tag_input",
                  "source_field": "mood"
                }
              },
              "force_instrumental": {
                "type": "boolean",
                "title": "Instrumental Only",
                "default": true,
                "_ux": {
                  "input_type": "checkbox"
                }
              },
              "output_format": {
                "type": "string",
                "title": "Quality",
                "enum": ["mp3_44100_64", "mp3_44100_128"],
                "default": "mp3_44100_128",
                "enum_labels": {
                  "mp3_44100_64": "64kbps",
                  "mp3_44100_128": "128kbps"
                },
                "_ux": {
                  "input_type": "select"
                }
              }
            }
          }
        },
        "properties": {
          "name": {
            "type": "string",
            "_ux": {
              "display": true,
              "render_as": "card-title"
            }
          },
          "description": {
            "type": "string",
            "_ux": {
              "display": true,
              "render_as": "card-subtitle",
              "highlight": true
            }
          },
          "prompt": {
            "type": "string",
            "_ux.display": false
          },
          "suggested_bpm": {
            "type": "integer",
            "_ux.display": false
          },
          "instruments": {
            "type": "array",
            "_ux": {
              "display": true,
              "display_label": "Instruments",
              "display_format": "{{ value | join(', ') }}"
            }
          },
          "mood": {
            "type": "array",
            "_ux": {
              "display": true,
              "display_label": "Mood",
              "display_format": "{{ value | join(', ') }}"
            }
          },
          "energy": {
            "type": "string",
            "_ux": {
              "display": true,
              "display_label": "Energy"
            }
          }
        }
      }
    }
  }
}
```

### 4. Step JSON Structure

**File:** `step.json`

```json
{
  "step_id": "music",
  "name": "Step {step_number}: Music Generation",
  "description": "Generate and select background music for your pet video",
  "modules": [
    {
      "module_id": "api.llm",
      "inputs": {
        "resolver_schema": {
          "type": "object",
          "properties": {
            "input": { "resolver": "server" },
            "system": { "resolver": "server" },
            "metadata": { "resolver": "server" }
          }
        },
        "ai_config": {},
        "input": {
          "$ref": "prompts/cc_music_prompts_user.txt",
          "type": "jinja2"
        },
        "output_schema": {
          "$ref": "schemas/cc_music_prompts_schema.json",
          "type": "json"
        },
        "metadata": {
          "step_id": "{{ step.step_id }}"
        },
        "system": [
          {
            "$ref": "prompts/cc_role_pet_music_director.txt",
            "type": "text",
            "cache_ttl": 10800
          },
          {
            "$ref": "prompts/cc_music_prompts_system.txt",
            "type": "text",
            "cache_ttl": 10800
          }
        ],
        "provider": "openai"
      },
      "outputs_to_state": {
        "response": "music_prompts"
      },
      "name": "generate_music_prompts"
    },
    {
      "module_id": "media.generate",
      "inputs": {
        "resolver_schema": {
          "type": "object",
          "properties": {
            "prompts": { "resolver": "server" }
          }
        },
        "prompts": "{{ state.music_prompts.prompts }}",
        "schema": {
          "$ref": "schemas/cc_audio_generation_display_schema.json",
          "type": "json"
        },
        "title": "Generate and Select Music"
      },
      "outputs_to_state": {
        "selected_content_id": "selected_audio_id",
        "selected_content": "selected_audio_data",
        "generations": "audio_generations"
      },
      "sub_actions": [
        {
          "id": "generate_music",
          "label": "Generate Music",
          "action_type": "txt2audio",
          "loading_label": "Generating music..."
        }
      ],
      "retryable": {
        "default_option": "continue",
        "options": [
          {
            "id": "continue",
            "mode": "continue",
            "label": "Accept and continue"
          },
          {
            "id": "retry",
            "mode": "retry",
            "label": "Regenerate music prompts",
            "shortcut": "r",
            "target_module": "generate_music_prompts",
            "feedback": {
              "enabled": true,
              "prompt": "What should be different?",
              "default_message": "Please generate different music prompts."
            }
          }
        ]
      },
      "name": "generate_and_select_music"
    }
  ]
}
```

### 5. LLM System Prompt

**File:** `prompts/cc_music_prompts_system.txt`

```markdown
# MUSIC PROMPTS FOR ELEVENLABS

Generate 3 music prompts for a pet video. Each prompt should be tailored to
the specific pet type, story theme, and emotional context provided.

## CONTEXT-DRIVEN STYLE SELECTION

Your 3 prompts must be musically distinct while ALL being appropriate for
the given context. Base your style choices on:

- **Pet type**: Dogs often suit warmer, more energetic styles; cats may suit
  more subtle, mysterious tones
- **Story theme**: Memorial/loss themes need reflective, gentle music;
  playful themes can be more upbeat
- **Emotional arc**: Match the music energy to the video's emotional journey
- **Mood**: Let the specified mood guide instrument and texture choices

Examples of context-appropriate variety:
- Joyful theme → options might include: upbeat acoustic, playful piano, light
  bounce
- Memorial theme → options might include: reflective strings, gentle piano,
  soft ambient
- Cozy/loving theme → options might include: warm acoustic, soft lofi, tender
  strings

## OUTPUT STRUCTURE (for each of 3 prompts)

```json
{
  "name": "Evocative 2-3 word name",
  "description": "15-25 word summary of sound and feel",
  "prompt": "100-200 char ElevenLabs-ready prompt text",
  "suggested_bpm": 72,
  "instruments": ["felt piano", "warm pad", "soft acoustic guitar"],
  "mood": ["warm", "gentle", "loving"],
  "energy": "steady and cozy"
}
```

## PROMPT WRITING GUIDELINES

- Keep prompts 100-200 characters
- Use natural, flowing language
- Focus on texture and feeling
- Include instrument textures (felt piano, warm guitar, soft strings)
- Do NOT include BPM or duration in the prompt (added separately by system)
- Do NOT include "instrumental only" (handled by API parameter)

## FIELD GUIDELINES

**suggested_bpm**: Tune to the emotional context:
- Cozy/peaceful: 55-70
- Playful/joyful: 70-90
- Emotional/reflective: 60-75
- Energetic/fun: 85-105

**instruments**: 2-4 instruments WITH texture descriptors:
- Good: "felt piano with soft decay", "warm acoustic guitar"
- Bad: "piano", "guitar"

**mood**: 2-3 descriptors that match the story's emotional tone

**energy**: How the track feels (e.g., "steady and cozy", "gently flowing",
"softly building")
```

### 6. LLM User Prompt

**File:** `prompts/cc_music_prompts_user.txt`

```jinja2
Generate 3 music prompts for this pet video:

## Pet & Story Context

**Pet Type:** {{ state.pet_type_selection.label }}
**Story Theme:** {{ state.aesthetic_selection.label }} - {{ state.aesthetic_selection.description }}
**Mood:** {{ state.aesthetic_selection.visual_tone.mood }}

## Scene Details

**Title:** {{ state.selected_scene.title }}
**Narrative:** {{ state.selected_scene.narrative }}
**Emotional Arc:** {{ state.selected_scene.emotional_arc }}

## Duration

**Music Length:** {{ state.duration_selection.sound_track_duration }} seconds

Generate 3 musically distinct prompts that are ALL appropriate for this
{{ state.pet_type_selection.label | lower }} video with its
{{ state.aesthetic_selection.visual_tone.mood | lower }} feel and
{{ state.aesthetic_selection.label | lower }} theme.
```

## Backend Changes

### 1. MediaActor - Add txt2audio Support

**File:** `backend/worker/actors/media.py`

Add to action_type dispatch (around line 186):

```python
elif action_type == "txt2audio":
    # Pass prompt and params to provider - provider handles assembly
    result = method(prompt, method_params, progress_callback=progress_callback)
```

Update content type (around line 213):

```python
if action_type == "img2vid":
    content_type = "video"
elif action_type == "txt2audio":
    content_type = "audio"
else:
    content_type = "image"
```

Add audios_path to storage paths and download_media call.

## WebUI Changes

### 1. AudioGeneration Component

**File:** `ui/webui/src/interactions/types/media-generation/AudioGeneration.tsx`

Key features:
- Similar structure to ImageGeneration
- Uses `react-audio-visualize` for waveform display
- Audio player with play/pause, seek, volume
- Track list for multiple generations
- Selection optional

### 2. Install Dependency

```bash
cd ui/webui && npm install react-audio-visualize
```

## Files Summary

### Create:
- `workflows/cc/steps/7_music/schemas/cc_music_prompts_schema.json`
- `workflows/cc/steps/7_music/schemas/cc_audio_generation_display_schema.json`
- `workflows/cc/steps/7_music/prompts/cc_music_prompts_system.txt`
- `workflows/cc/steps/7_music/prompts/cc_music_prompts_user.txt`
- `ui/webui/src/interactions/types/media-generation/AudioGeneration.tsx`

### Modify:
- `workflows/cc/steps/7_music/step.json` (restructure)
- `backend/worker/actors/media.py` (add txt2audio)
- `ui/webui/src/interactions/types/media-generation/MediaGenerationHost.tsx`
- `ui/webui/src/interactions/types/media-generation/types.ts`

### Delete (after implementation complete):
- `workflows/cc/steps/7_music/schemas/cc_music_options_schema.json`
- `workflows/cc/steps/7_music/schemas/cc_music_options_display_schema.json`
- `workflows/cc/steps/7_music/schemas/cc_elevenlabs_schema.json`
- `workflows/cc/steps/7_music/prompts/cc_music_system.txt`
- `workflows/cc/steps/7_music/prompts/cc_music_user.txt`
- `workflows/cc/steps/7_music/prompts/cc_elevenlabs_system.txt`
- `workflows/cc/steps/7_music/prompts/cc_elevenlabs_user.txt`

## Implementation Order

1. Backend: Add `txt2audio` to MediaActor with prompt assembly
2. Workflow: Create new schemas
3. Workflow: Create new prompts
4. Workflow: Restructure `step.json`
5. WebUI: Install `react-audio-visualize`
6. WebUI: Create `AudioGeneration.tsx`
7. WebUI: Integrate in `MediaGenerationHost.tsx`
8. Test end-to-end
9. Delete old files

<!--
-->
