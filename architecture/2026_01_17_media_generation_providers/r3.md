# Media Generation Providers Architecture - R3

## Summary

Design for integrating image/video generation APIs (MidAPI/Midjourney and Leonardo AI) into the workflow system.

---

## Design Decisions

### 1. Return Pattern: `Tuple[List[str], dict]`

All generation methods return `(result_urls, raw_response)`. IDs for post-processing are in raw_response.

### 2. End-to-End Methods

Methods handle: submit → poll → return results.

### 3. Input as `Dict[str, Any]`

Flexibility for provider-specific params.

### 4. Error Handling

Base exceptions with `provider_error` field for original error data:

```python
class MediaGenerationError(Exception):
    """Base exception for all media generation errors."""
    def __init__(self, message: str, provider: str, provider_error: Any = None):
        self.provider = provider
        self.provider_error = provider_error
        super().__init__(message)

class GenerationFailedError(MediaGenerationError): pass
class GenerationTimeoutError(MediaGenerationError): pass
class AuthenticationError(MediaGenerationError): pass
class InsufficientCreditsError(MediaGenerationError): pass
class InvalidParameterError(MediaGenerationError): pass
class ProviderError(MediaGenerationError): pass
```

### 5. Logging

- `DEBUG`: Full request/response payloads
- `INFO`: Task started, task completed
- `WARNING`: Retries, rate limits
- `ERROR`: Failures, timeouts

---

## File Structure

Looking at existing patterns:
- LLM providers are in `server/modules/api/providers/` with a unified `llm_call.py` module
- Repositories are in `server/db/`

**Concern**: LLM providers (OpenAI, Anthropic) are used across workflow tasks. Media providers (MidAPI, Leonardo) are for specific steps. Should they be in the same place?

**Proposed structure** - Separate `media` folder under modules:

```
server/modules/
├── api/                      # LLM API (existing)
│   ├── llm_call.py          # Unified LLM module
│   └── providers/
│       ├── openai/
│       └── anthropic/
│
└── media/                    # Media generation (new)
    ├── __init__.py
    ├── exceptions.py         # MediaGenerationError, etc.
    ├── media_generation.py   # Workflow module (like llm_call.py)
    └── providers/
        ├── __init__.py
        ├── midapi.py         # MidAPI client
        └── leonardo.py       # Leonardo client

server/db/
└── content_repository.py     # Storage for generated content (new)
```

**Why separate from `api/providers`**:
- Different purpose: LLM is text generation, media is image/video
- Different usage pattern: LLM used broadly, media for specific steps
- Clearer organization
- Media module can have its own unified entry point (`media_generation.py`)

---

## Storage Architecture

### Where does storage happen?

**Options considered:**

| Option | Pros | Cons |
|--------|------|------|
| Inside provider client | Simple, all in one place | Tight coupling, provider shouldn't know about DB |
| Inside router | Router is just dispatch | Still mixing concerns |
| Inside workflow module | Module knows context | Module becomes complex |
| Separate repository | Clean separation, follows existing pattern | Extra layer |

**Decision**: Follow existing pattern - **Repository in `server/db/`**

The workflow module (`media_generation.py`) orchestrates:
1. Calls provider client to generate content
2. Calls repository to store metadata and content records
3. Returns results to workflow

```python
# server/modules/media/media_generation.py (workflow module)
class MediaGenerationModule(ExecutableModule):
    def execute(self, inputs, context):
        provider = inputs["provider"]
        operation = inputs["operation"]  # "txt2img", "img2vid", etc.
        params = inputs["params"]

        # 1. Get client and generate
        client = self._get_client(provider)
        urls, raw_response = getattr(client, operation)(params)

        # 2. Store in database
        metadata_id = context.db.content_repo.store_generation(
            workflow_run_id=context.workflow_run_id,
            provider=provider,
            operation=operation,
            request_params=params,
            response_data=raw_response,
            provider_task_id=self._extract_task_id(provider, raw_response),
        )

        for index, url in enumerate(urls):
            context.db.content_repo.store_content(
                metadata_id=metadata_id,
                workflow_run_id=context.workflow_run_id,
                index=index,
                provider_content_id=self._extract_content_id(provider, raw_response, index),
                content_type="image",  # or "video"
                provider_url=url,
            )

        # 3. Return to workflow
        return {
            "urls": urls,
            "metadata_id": metadata_id,
            "raw": raw_response
        }
```

**Clients stay pure** - just API interaction, no DB knowledge.

---

## Database Schema

### ID Naming Convention

Following codebase pattern (`workflow_run_id`, `event_id`, etc.):
- `content_generation_metadata_id`
- `generated_content_id`

### Tables

```
content_generation_metadata
├── content_generation_metadata_id: str (UUID) <-- minor change, it would be "cgm_{uuid7}" (change prefix as needed)
├── workflow_run_id: str (FK)
├── provider: enum("midapi", "leonardo")
├── operation: enum("txt2img", "img2img", "txt2vid", "img2vid", "upscale", "vary", "extend")
├── created_at: datetime
├── completed_at: datetime (nullable)
├── status: enum("pending", "complete", "failed")
├── request_params: jsonb      -- Input parameters sent to API
├── response_data: jsonb       -- Full raw response from API (yes, store everything)
├── provider_task_id: str      -- taskId (MidAPI) or generationId (Leonardo)
├── error_message: str (nullable)
└── credits_used: int (nullable)

generated_content
├── generated_content_id: str (UUID) <-- minor change, it would be "gc_{uuid7}" (change prefix as needed)
├── workflow_run_id: str (FK)
├── content_generation_metadata_id: str (FK)
├── index: int                 -- Position in result array (0, 1, 2, 3)
├── provider_content_id: str (nullable)  -- image_id for Leonardo, null for MidAPI
├── content_type: enum("image", "video")
├── provider_url: str          -- Original URL from provider
├── local_path: str (nullable) -- Local storage path after download
├── downloaded_at: datetime (nullable)
└── file_size_bytes: int (nullable)
```

### Response Storage

**Q: Are we storing response fully?**

**A: Yes**, in `response_data` jsonb field. Reasons:
- Leonardo has lots of useful metadata (seed, model used, etc.)
- MidAPI has timing info, param details
- Useful for debugging, auditing, future analysis
- jsonb doesn't require extraction - store as-is, query as needed

The `generated_content` table extracts only what's needed for operations (URLs, IDs for post-processing).

---

## MidAPI Client

### Pseudo Code

```python
# server/modules/media/providers/midapi.py

class MidAPIClient:
    BASE_URL = "https://api.midapi.ai"
    ENV_KEY = "MIDAPI_API_KEY"

    def __init__(self, api_key=None, timeout=300, poll_interval=5.0):
        self._api_key = api_key
        self._timeout = timeout
        self._poll_interval = poll_interval

    def _get_api_key(self) -> str:
        key = self._api_key or os.getenv(self.ENV_KEY)
        if not key:
            raise AuthenticationError("API key not provided", "midapi")
        return key

    def _request(self, method, endpoint, params=None, json_data=None) -> dict:
        response = requests.request(
            method,
            f"{self.BASE_URL}{endpoint}",
            headers={"Authorization": f"Bearer {self._get_api_key()}"},
            params=params,
            json=json_data,
            timeout=30
        )
        data = response.json()

        if data["code"] == 401:
            raise AuthenticationError(data["msg"], "midapi", data)
        if data["code"] == 402:
            raise InsufficientCreditsError(data["msg"], "midapi", data)
        if data["code"] != 200:
            raise ProviderError(data["msg"], "midapi", data)

        return data

    def _poll_until_complete(self, task_id) -> dict:
        start = time.time()
        while time.time() - start < self._timeout:
            data = self._request("GET", "/api/v1/mj/record-info", params={"taskId": task_id})
            task = data["data"]

            if task["successFlag"] == 1:
                return task
            if task["successFlag"] in (2, 3):
                raise GenerationFailedError(
                    task.get("errorMessage", "Generation failed"), "midapi", task
                )

            time.sleep(self._poll_interval)

        raise GenerationTimeoutError(f"Task {task_id} timed out", "midapi")

    def _extract_urls(self, task_data) -> List[str]:
        urls = []
        result_info = task_data.get("resultInfoJson", {})
        for item in result_info.get("resultUrls", []):
            if isinstance(item, dict):
                urls.append(item["resultUrl"])
            else:
                urls.append(item)
        return urls

    # === Generation Methods ===

    def txt2img(self, params: dict) -> Tuple[List[str], dict]:
        """
        params:
            prompt (str, required)
            speed: "relaxed" | "fast" | "turbo"
            aspect_ratio: "1:1" | "16:9" | "9:16" | etc
            version: "7" | "6.1" | "6" | "5.2" | "5.1" | "niji6"
            variety: int 0-100
            stylization: int 0-1000
            weirdness: int 0-3000
            water_mark: str
            enable_translation: bool
            callback_url: str
        """
        if "prompt" not in params:
            raise InvalidParameterError("prompt is required", "midapi")

        payload = {"taskType": "mj_txt2img", "prompt": params["prompt"][:2000]}

        # Map optional params
        if "speed" in params:
            payload["speed"] = params["speed"]
        if "aspect_ratio" in params:
            payload["aspectRatio"] = params["aspect_ratio"]
        if "version" in params:
            payload["version"] = params["version"]
        if "variety" in params:
            payload["variety"] = params["variety"]
        if "stylization" in params:
            payload["stylization"] = params["stylization"]
        if "weirdness" in params:
            payload["weirdness"] = params["weirdness"]
        if "water_mark" in params:
            payload["waterMark"] = params["water_mark"]
        if params.get("enable_translation"):
            payload["enableTranslation"] = True
        if "callback_url" in params:
            payload["callBackUrl"] = params["callback_url"]

        data = self._request("POST", "/api/v1/mj/generate", json_data=payload)
        task_id = data["data"]["taskId"]

        result = self._poll_until_complete(task_id)
        return self._extract_urls(result), result

    def img2img(self, params: dict) -> Tuple[List[str], dict]:
        """
        params:
            prompt (str, required)
            image_url (str, required)
            + optional params from txt2img
        """
        if "prompt" not in params:
            raise InvalidParameterError("prompt is required", "midapi")
        if "image_url" not in params:
            raise InvalidParameterError("image_url is required", "midapi")

        payload = {
            "taskType": "mj_img2img",
            "prompt": params["prompt"][:2000],
            "fileUrl": params["image_url"]
        }
        # Map optional params (same as txt2img)
        ...

        data = self._request("POST", "/api/v1/mj/generate", json_data=payload)
        result = self._poll_until_complete(data["data"]["taskId"])
        return self._extract_urls(result), result

    def style_reference(self, params: dict) -> Tuple[List[str], dict]:
        """
        Generate using images as STYLE reference.

        params:
            prompt (str, required)
            image_urls (List[str], required)
            + optional params (no speed)
        """
        payload = {
            "taskType": "mj_style_reference",
            "prompt": params["prompt"][:2000],
            "fileUrls": params["image_urls"]
        }
        ...

    def omni_reference(self, params: dict) -> Tuple[List[str], dict]:
        """
        Generate using images as OMNI reference (style + content).

        params:
            prompt (str, required)
            image_urls (List[str], required)
            ow (int): Reference strength 1-1000
        """
        payload = {
            "taskType": "mj_omni_reference",
            "prompt": params["prompt"][:2000],
            "fileUrls": params["image_urls"]
        }
        if "ow" in params:
            payload["ow"] = params["ow"]
        ...

    def txt2vid(self, params: dict) -> Tuple[List[str], dict]:
        """
        params:
            prompt (str, required)
            hd: bool
            batch_size: 1 | 2 | 4
            motion: "high" | "low"
            aspect_ratio: str
        """
        task_type = "mj_video_hd" if params.get("hd") else "mj_video"
        payload = {"taskType": task_type, "prompt": params["prompt"][:2000]}

        if "batch_size" in params:
            payload["videoBatchSize"] = str(params["batch_size"])
        if "motion" in params:
            payload["motion"] = params["motion"]
        if "aspect_ratio" in params:
            payload["aspectRatio"] = params["aspect_ratio"]
        ...

    def img2vid(self, params: dict) -> Tuple[List[str], dict]:
        """
        params:
            prompt (str, required)
            image_url (str, required)
            + txt2vid params
        """
        payload = {
            "taskType": "mj_video_hd" if params.get("hd") else "mj_video",
            "prompt": params["prompt"][:2000],
            "fileUrl": params["image_url"]
        }
        ...

    # === Post-Processing ===

    def upscale(self, task_id: str, index: int = 0, **kwargs) -> Tuple[List[str], dict]:
        """
        task_id: From raw["taskId"]
        index: 0-3
        """
        if not 0 <= index <= 3:
            raise InvalidParameterError("index must be 0-3", "midapi")

        payload = {"taskId": task_id, "imageIndex": index}
        if "water_mark" in kwargs:
            payload["waterMark"] = kwargs["water_mark"]
        if "callback_url" in kwargs:
            payload["callBackUrl"] = kwargs["callback_url"]

        data = self._request("POST", "/api/v1/mj/generateUpscale", json_data=payload)
        result = self._poll_until_complete(data["data"]["taskId"])
        return self._extract_urls(result), result

    def vary(self, task_id: str, index: int = 0, **kwargs) -> Tuple[List[str], dict]:
        """Same as upscale but creates variations."""
        # POST /api/v1/mj/generateVary
        ...

    def extend_video(self, task_id: str, index: int, params: dict) -> Tuple[List[str], dict]:
        """
        task_id: From video generation
        index: Which video from batch
        params:
            prompt (str, required)
            auto: bool
        """
        task_type = "mj_video_extend_auto" if params.get("auto") else "mj_video_extend_manual"
        payload = {
            "taskId": task_id,
            "index": index,
            "prompt": params["prompt"][:2000],
            "taskType": task_type
        }
        # POST /api/v1/mj/generateVideoExtend
        ...

    # === Utility ===

    def get_credits(self) -> int:
        data = self._request("GET", "/api/v1/common/credit")
        return int(data["data"])

    def get_download_url(self, url: str) -> str:
        """Get temp download URL (valid 20 min)."""
        data = self._request("POST", "/api/v1/common/download-url", json_data={"url": url})
        return data["data"]

    def get_task_status(self, task_id: str) -> dict:
        """Get status without polling."""
        data = self._request("GET", "/api/v1/mj/record-info", params={"taskId": task_id})
        return data["data"]
```

---

## Leonardo Client

### Pseudo Code

```python
# server/modules/media/providers/leonardo.py

class LeonardoClient:
    BASE_URL = "https://cloud.leonardo.ai/api/rest/v1"
    ENV_KEY = "LEONARDO_API_KEY"

    def __init__(self, api_key=None, timeout=300, poll_interval=5.0):
        self._api_key = api_key
        self._timeout = timeout
        self._poll_interval = poll_interval

    def _get_api_key(self) -> str:
        key = self._api_key or os.getenv(self.ENV_KEY)
        if not key:
            raise AuthenticationError("API key not provided", "leonardo")
        return key

    def _request(self, method, endpoint, params=None, json_data=None) -> dict:
        response = requests.request(
            method,
            f"{self.BASE_URL}{endpoint}",
            headers={
                "Authorization": f"Bearer {self._get_api_key()}",
                "Content-Type": "application/json"
            },
            params=params,
            json=json_data,
            timeout=30
        )

        if response.status_code == 401:
            raise AuthenticationError("Invalid API key", "leonardo")
        if response.status_code == 402:
            raise InsufficientCreditsError("Insufficient credits", "leonardo")
        if not response.ok:
            raise ProviderError(f"API error: {response.status_code}", "leonardo", response.text)

        return response.json()

    def _poll_generation(self, generation_id: str) -> dict:
        start = time.time()
        while time.time() - start < self._timeout:
            data = self._request("GET", f"/generations/{generation_id}")
            gen = data.get("generations_by_pk", {})
            status = gen.get("status")

            if status == "COMPLETE":
                return gen
            if status == "FAILED":
                raise GenerationFailedError("Generation failed", "leonardo", gen)

            time.sleep(self._poll_interval)

        raise GenerationTimeoutError(f"Generation {generation_id} timed out", "leonardo")

    def _poll_variation(self, variation_id: str) -> dict:
        start = time.time()
        while time.time() - start < self._timeout:
            data = self._request("GET", f"/variations/{variation_id}")
            variations = data.get("generated_image_variation_generic", [])

            if variations:
                var = variations[0]
                if var.get("status") == "COMPLETE":
                    return var
                if var.get("status") == "FAILED":
                    raise GenerationFailedError("Variation failed", "leonardo", var)

            time.sleep(self._poll_interval)

        raise GenerationTimeoutError(f"Variation {variation_id} timed out", "leonardo")

    def _extract_image_urls(self, generation: dict) -> List[str]:
        return [img["url"] for img in generation.get("generated_images", [])]

    def _extract_video_urls(self, generation: dict) -> List[str]:
        urls = []
        for img in generation.get("generated_images", []):
            if img.get("motionMP4URL"):
                urls.append(img["motionMP4URL"])
            elif img.get("url"):
                urls.append(img["url"])
        return urls

    # === Generation Methods ===

    def txt2img(self, params: dict) -> Tuple[List[str], dict]:
        """
        params:
            prompt (str, required)
            width: int 32-1536 (multiple of 8)
            height: int 32-1536 (multiple of 8)
            num_images: int 1-8
            model_id: str (UUID)
            guidance_scale: int 1-20
            num_inference_steps: int 10-60
            seed: int
            negative_prompt: str
            preset_style: str
            alchemy: bool
            photo_real: bool
            ultra: bool
            public: bool
            elements: List[{"akUUID": str, "weight": float}]
        """
        if "prompt" not in params:
            raise InvalidParameterError("prompt is required", "leonardo")

        payload = {"prompt": params["prompt"]}

        # Map optional params
        if "width" in params:
            payload["width"] = params["width"]
        if "height" in params:
            payload["height"] = params["height"]
        if "num_images" in params:
            payload["num_images"] = params["num_images"]
        if "model_id" in params:
            payload["modelId"] = params["model_id"]
        if "guidance_scale" in params:
            payload["guidance_scale"] = params["guidance_scale"]
        if "num_inference_steps" in params:
            payload["num_inference_steps"] = params["num_inference_steps"]
        if "seed" in params:
            payload["seed"] = params["seed"]
        if "negative_prompt" in params:
            payload["negative_prompt"] = params["negative_prompt"]
        if "preset_style" in params:
            payload["presetStyle"] = params["preset_style"]
        if "alchemy" in params:
            payload["alchemy"] = params["alchemy"]
        if "photo_real" in params:
            payload["photoReal"] = params["photo_real"]
        if "ultra" in params:
            payload["ultra"] = params["ultra"]
        if "public" in params:
            payload["public"] = params["public"]
        if "elements" in params:
            payload["elements"] = params["elements"]

        data = self._request("POST", "/generations", json_data=payload)
        gen_id = data["sdGenerationJob"]["generationId"]

        result = self._poll_generation(gen_id)
        return self._extract_image_urls(result), result

    def img2img(self, params: dict) -> Tuple[List[str], dict]:
        """
        params:
            prompt (str, required)
            init_image_id (str, required)
            init_strength: float 0.1-0.9
            + txt2img params
        """
        if "init_image_id" not in params:
            raise InvalidParameterError("init_image_id is required", "leonardo")

        payload = {"prompt": params["prompt"]}
        payload["init_image_id"] = params["init_image_id"]
        if "init_strength" in params:
            payload["init_strength"] = params["init_strength"]
        # + other params
        ...

    def img2vid(self, params: dict) -> Tuple[List[str], dict]:
        """
        params:
            prompt (str, required)
            image_id (str, required)
            image_type: "GENERATED" | "UPLOADED"
            model: "MOTION2" | "VEO3" | etc
            resolution: "RESOLUTION_480" | "RESOLUTION_720" | "RESOLUTION_1080"
            duration: int
            frame_interpolation: bool
            seed: int
            negative_prompt: str
        """
        if "prompt" not in params:
            raise InvalidParameterError("prompt is required", "leonardo")
        if "image_id" not in params:
            raise InvalidParameterError("image_id is required", "leonardo")

        payload = {
            "prompt": params["prompt"],
            "imageId": params["image_id"],
            "imageType": params.get("image_type", "GENERATED")
        }

        if "model" in params:
            payload["model"] = params["model"]
        if "resolution" in params:
            payload["resolution"] = params["resolution"]
        if "duration" in params:
            payload["duration"] = params["duration"]
        if "frame_interpolation" in params:
            payload["frameInterpolation"] = params["frame_interpolation"]
        if "seed" in params:
            payload["seed"] = params["seed"]
        if "negative_prompt" in params:
            payload["negativePrompt"] = params["negative_prompt"]

        data = self._request("POST", "/generations-image-to-video", json_data=payload)
        gen_id = data["motionVideoGenerationJob"]["generationId"]

        result = self._poll_generation(gen_id)
        return self._extract_video_urls(result), result

    # === Post-Processing ===

    def upscale(self, image_id: str, **kwargs) -> Tuple[List[str], dict]:
        """
        image_id: From raw["generated_images"][index]["id"]
        """
        payload = {"id": image_id}

        data = self._request("POST", "/variations/upscale", json_data=payload)
        var_id = data["sdUpscaleJob"]["id"]

        result = self._poll_variation(var_id)
        return [result["url"]], result

    # === Utility ===

    def get_credits(self) -> int:
        data = self._request("GET", "/me")
        user = data.get("user_details", [{}])[0]
        return user.get("apiSubscriptionTokens", 0) + user.get("apiPaidTokens", 0)

    def get_generation_status(self, generation_id: str) -> dict:
        """Get status without polling."""
        data = self._request("GET", f"/generations/{generation_id}")
        return data.get("generations_by_pk", {})
```

---

## Key Differences Summary

| Aspect | MidAPI | Leonardo |
|--------|--------|----------|
| Generation endpoint | Single `/generate` with `taskType` | Separate endpoints |
| Status polling | `/record-info?taskId=` | `/generations/{id}` |
| Status values | `0,1,2,3` (int) | `PENDING,COMPLETE,FAILED` |
| Upscale input | `taskId` + `index` | `imageId` directly |
| Results location | `resultInfoJson.resultUrls[]` | `generated_images[].url` |
| txt2vid | Supported | Not supported (use img2vid) |

---

## Questions for Review

1. **File structure**: Does `server/modules/media/` make sense as separate from `server/modules/api/`?
<!-- this is good -->

2. **Repository**: Should I show pseudo code for `content_repository.py`?
<!-- i am okay with repo, but i'm unclear with router and root module, are we going to use both router and root module or only module. 
also, behavior of this woldn't be like normal module, it will act more line submodule, which we dont have anything similar right now.
we will need to address that right after we are done with this. actually thinking about it, we probably need to address it now.

So, behavior of this is to work with existing prompt generation modules to run content of generated prompts. for example, if user select
mj and leonardo i  step 2, i want user to be able to generate images based on generated prompts and iterate over them. then when user goes
step 4, i need user to be able to videos based on selected image from step 2, (btw that selected image has to be auto selected for step 4,
and generate analysis automatically.

So, these media "modules" are not modules that will be included directly under a step, they are in a way, enhancements to a existing module.
I think we have to think around this and come up with a solution on how to implement these. In eye of user, it will be "Generate Images/Video" 
button with other button, probably highlighted, once button is clicked, lets say 4 images will get generated and shown under the prompt.
User can either adjust params (which actually provided by core aesthetics, and currently fixed values, but with this module, they can adjust 
it, or if needed, adjust prompt itself), and click the button again to generate images. Every time user click the button, we will generate more
images and shown as a grid below prompt in desecending order.

Lets add this as major discussion point before finalize provider behavior. be critical with this, give me ideas, dont be a yesman.
-->
